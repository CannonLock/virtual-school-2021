{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG Virtual School 2021 \u00b6 Transform your research with vast amounts of computing! The OSG Virtual School 2021 will prepare you to use high throughput computing (HTC) \u2014 whether on a local system or using OSG \u2014 to run large-scale research simulation, analysis, and other applications for cutting-edge science. Through lecture, demonstration, hands-on exercises, and a personalized learning plan that includes one-on-one and small-group consulting with experienced OSG staff, you will learn to harness HTC systems and apply them to your own computational research. The School is ideal for graduate students in any research area for whom large-scale computing is a vital part of the research process; we also consider applications from advanced undergraduates, post-doctoral students, faculty, and staff! This virtual offering has been tuned for remote participation, with the goal of helping attendees get their own work running within the 2-week period, and is not merely a translation of the in-person program (last offered in 2019 ). The Virtual School will be held the weeks of August 2\u20136 and 9\u201313. The event is free and materials will be posted publicly. A selective application process is required for full participation, which includes access to HTC experts and to HTC systems via the Center for High Throughput Computing and OSG Connect service (the latter otherwise available to researchers working on a U.S.-based academic, government, or non-profit research project). Also, the public is invited to register and attend the five lectures during the first week; for more information, see the Public Lectures page . While no travel is involved and the schedule allows for some flexibility, invited participants will be asked to commit at least 20 hours each week. The full schedule for 2021 is available to view. Note: We hope to return to an in-person OSG User School in 2022! Contact Us \u00b6 The OSG Virtual School is part of the OSG Outreach Area \u2014 please visit that site to learn about past OSG Schools. If you have any questions about the event, feel free to email us: user-school@opensciencegrid.org OSGUserSchool","title":"Home"},{"location":"#osg-virtual-school-2021","text":"Transform your research with vast amounts of computing! The OSG Virtual School 2021 will prepare you to use high throughput computing (HTC) \u2014 whether on a local system or using OSG \u2014 to run large-scale research simulation, analysis, and other applications for cutting-edge science. Through lecture, demonstration, hands-on exercises, and a personalized learning plan that includes one-on-one and small-group consulting with experienced OSG staff, you will learn to harness HTC systems and apply them to your own computational research. The School is ideal for graduate students in any research area for whom large-scale computing is a vital part of the research process; we also consider applications from advanced undergraduates, post-doctoral students, faculty, and staff! This virtual offering has been tuned for remote participation, with the goal of helping attendees get their own work running within the 2-week period, and is not merely a translation of the in-person program (last offered in 2019 ). The Virtual School will be held the weeks of August 2\u20136 and 9\u201313. The event is free and materials will be posted publicly. A selective application process is required for full participation, which includes access to HTC experts and to HTC systems via the Center for High Throughput Computing and OSG Connect service (the latter otherwise available to researchers working on a U.S.-based academic, government, or non-profit research project). Also, the public is invited to register and attend the five lectures during the first week; for more information, see the Public Lectures page . While no travel is involved and the schedule allows for some flexibility, invited participants will be asked to commit at least 20 hours each week. The full schedule for 2021 is available to view. Note: We hope to return to an in-person OSG User School in 2022!","title":"OSG Virtual School 2021"},{"location":"#contact-us","text":"The OSG Virtual School is part of the OSG Outreach Area \u2014 please visit that site to learn about past OSG Schools. If you have any questions about the event, feel free to email us: user-school@opensciencegrid.org OSGUserSchool","title":"Contact Us"},{"location":"participant-tips/","text":"OSGVS21 Participant Tips \u00b6 This page contains information about the OSG Virtual School 2021 that is mainly targeted at invited participants. Format \u00b6 As shown on the schedule page , there are several different kinds of events during the School: Lectures and demonstrations: Every day during the first week. The ones starting at 10 a.m. CDT are for invited participants only, and the ones starting at 2:30 p.m. CDT are for invited participants and registered guests from the public. Participants should attend at least one offering of each lecture (including demos, if any). Mentor one-on-one (1-1) meetings and mentor group meetings: Meet with your mentor individually to set and track progress on goals, or with small groups to discuss common interests. Extra topics: On Monday and Tuesday of the second week, there will be extra, optional lessons on more specialized topics. All participants are welcome to join, but use your time wisely. Not sure what topics to attend? Work with your mentor to decide. Showcase: On Wednesday, August 11, there will be talks from three researchers who use HTC. They will talk about their science, their computing, and how the process of learning about and using High Throughput Computing transformed their work. This event is optional, but many people have enjoyed it before. There will be time at the end to talk to the presenters. Lightning talks and wrap-up: On the last day, participants are invited to give very short talks about their work and what was accomplished during the School. Then there will be a brief wrap-up talk. Staff available times: Between 9 a.m. and 5 p.m. CDT, when there are no whole-group activities planned, staff will be available on Slack and by email to help with exercises, questions, your own projects, whatever! If your schedule allows, these are good times to work on School activities. Mentor \u00b6 Each participant is assigned a mentor from the instructional staff. Your mentor is your primary point of contact during the School and beyond, but you will work with others, too. They will help set goals, monitor progress, find resources/solutions, etc. When in doubt, contact your mentor\u2028(or Tim or Lauren or any staff member). Communication Tools \u00b6 The School will use a few technologies to communicate with participants. Zoom: lectures, Showcase, lightning talks and wrap-up; probably individual and small group meetings, too. Slack: All the time! Use the #general channel or one of the topic channel; also, DMs with staff and each other are fine. Website: Slides, exercises, lectures (eventually), schedule, etc. Feedback \u00b6 Tell us what we could do better! At any time, to any staff member, for any reason. We are here for you. We plan to send out one evaluation survey at the end of each week. They will not be too long, so please take the few minutes to complete each survey.","title":"Participant Tips"},{"location":"participant-tips/#osgvs21-participant-tips","text":"This page contains information about the OSG Virtual School 2021 that is mainly targeted at invited participants.","title":"OSGVS21 Participant Tips"},{"location":"participant-tips/#format","text":"As shown on the schedule page , there are several different kinds of events during the School: Lectures and demonstrations: Every day during the first week. The ones starting at 10 a.m. CDT are for invited participants only, and the ones starting at 2:30 p.m. CDT are for invited participants and registered guests from the public. Participants should attend at least one offering of each lecture (including demos, if any). Mentor one-on-one (1-1) meetings and mentor group meetings: Meet with your mentor individually to set and track progress on goals, or with small groups to discuss common interests. Extra topics: On Monday and Tuesday of the second week, there will be extra, optional lessons on more specialized topics. All participants are welcome to join, but use your time wisely. Not sure what topics to attend? Work with your mentor to decide. Showcase: On Wednesday, August 11, there will be talks from three researchers who use HTC. They will talk about their science, their computing, and how the process of learning about and using High Throughput Computing transformed their work. This event is optional, but many people have enjoyed it before. There will be time at the end to talk to the presenters. Lightning talks and wrap-up: On the last day, participants are invited to give very short talks about their work and what was accomplished during the School. Then there will be a brief wrap-up talk. Staff available times: Between 9 a.m. and 5 p.m. CDT, when there are no whole-group activities planned, staff will be available on Slack and by email to help with exercises, questions, your own projects, whatever! If your schedule allows, these are good times to work on School activities.","title":"Format"},{"location":"participant-tips/#mentor","text":"Each participant is assigned a mentor from the instructional staff. Your mentor is your primary point of contact during the School and beyond, but you will work with others, too. They will help set goals, monitor progress, find resources/solutions, etc. When in doubt, contact your mentor\u2028(or Tim or Lauren or any staff member).","title":"Mentor"},{"location":"participant-tips/#communication-tools","text":"The School will use a few technologies to communicate with participants. Zoom: lectures, Showcase, lightning talks and wrap-up; probably individual and small group meetings, too. Slack: All the time! Use the #general channel or one of the topic channel; also, DMs with staff and each other are fine. Website: Slides, exercises, lectures (eventually), schedule, etc.","title":"Communication Tools"},{"location":"participant-tips/#feedback","text":"Tell us what we could do better! At any time, to any staff member, for any reason. We are here for you. We plan to send out one evaluation survey at the end of each week. They will not be too long, so please take the few minutes to complete each survey.","title":"Feedback"},{"location":"public-lectures/","text":"OSGVS21 Public Lectures \u00b6 The OSG User School is our premier training program for researchers, facilitators, and others who are interested in learning more about how to use High Throughput Computing to advance research. This year\u2019s program is online again and the application process is complete. BUT we have some exciting news about sharing the OSG School with the entire community! For the first time, the OSG Virtual School 2021 will open its core lectures live to the public. The public lectures are excellent learning opportunities for students and researchers who could not be full participants of the School. The sessions are also well suited for those interested in learning more about High Throughput Computing, OSG, HTCondor, or leveraging any of these in your computing research. Topics include: The main concepts of High Throughput Computing (HTC) The basics of using HTCondor Getting started on OSG, a national distributed HTC infrastructure for Open Science How to prepare software for use in HTC systems Handling data (input and output) in HTC systems The public lectures are held daily, August 2\u20136, from 2:30\u20134:00 pm US Central Time. The detailed schedule is available on the Indico site. In addition, the lectures will be recorded and posted on the School website eventually, along with the complete set of exercises. While full participants also have priority access to experts, personalized learning plans, and small-group learning sessions, by opening lectures up to the public, a great deal of the School content will be available to everyone. Registration \u00b6 Follow this link to register for free: Register Or, contact us at user-school@opensciencegrid.org .","title":"Public Lectures"},{"location":"public-lectures/#osgvs21-public-lectures","text":"The OSG User School is our premier training program for researchers, facilitators, and others who are interested in learning more about how to use High Throughput Computing to advance research. This year\u2019s program is online again and the application process is complete. BUT we have some exciting news about sharing the OSG School with the entire community! For the first time, the OSG Virtual School 2021 will open its core lectures live to the public. The public lectures are excellent learning opportunities for students and researchers who could not be full participants of the School. The sessions are also well suited for those interested in learning more about High Throughput Computing, OSG, HTCondor, or leveraging any of these in your computing research. Topics include: The main concepts of High Throughput Computing (HTC) The basics of using HTCondor Getting started on OSG, a national distributed HTC infrastructure for Open Science How to prepare software for use in HTC systems Handling data (input and output) in HTC systems The public lectures are held daily, August 2\u20136, from 2:30\u20134:00 pm US Central Time. The detailed schedule is available on the Indico site. In addition, the lectures will be recorded and posted on the School website eventually, along with the complete set of exercises. While full participants also have priority access to experts, personalized learning plans, and small-group learning sessions, by opening lectures up to the public, a great deal of the School content will be available to everyone.","title":"OSGVS21 Public Lectures"},{"location":"public-lectures/#registration","text":"Follow this link to register for free: Register Or, contact us at user-school@opensciencegrid.org .","title":"Registration"},{"location":"schedule/","text":"OSGVS21 Schedule \u00b6 This page contains schedule details for full, invited participants of the OSG Virtual School 2021, who were selected following a prior application process. SOME presentations will presented publicly. For information about access to public lectures and demonstrations, visit the Indico scheduling site ; note that public attendance requires free registration on that site. Week 1: August 2\u20136 \u00b6 Note: Lectures and demonstrations this week are repeated each day. The first instance is for invited School participants only, and the second instance will also be open to the general public who have registered in advance. Invited participants may attend one or both times. During the times marked as \u201cStaff available\u201d, School staff will be monitoring email and the OSG School Slack for questions. So these are good times to work on School-related activities \u2014 such as exercises or applying things to your own work. .md-typeset table:not([class]) td, .md-typeset table:not([class]) th { padding-top: 0.2em; padding-bottom: 0.2em; } Monday, August 2 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available; (scheduled separately:) mentor 1-1 meetings and mentor group meetings 10:00\u201311:00 Lecture: Introduction to High Throughput Computing (participants only) 11:00\u201314:30 Staff available; (scheduled separately:) mentor 1-1 meetings and mentor group meetings 14:30\u201315:30 Lecture: Introduction to High Throughput Computing (public) 15:30\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings and mentor group meetings Tuesday, August 3 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Introduction to HTCondor (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Introduction to HTCondor (public) 16:00\u201317:00 Staff available Wednesday, August 4 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Introduction to OSG (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Introduction to OSG (public) 16:00\u201317:00 Staff available Thursday, August 5 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Software for HTC (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Software for HTC (public) 16:00\u201317:00 Staff available Friday, August 6 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Data for HTC (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Data for HTC (public) 16:00\u201317:00 Staff available Week 2: August 9\u201313 \u00b6 The focus of this week is on applying what you have learned (and are learning!) to your own work. Plus, there are opportunities to learn more and a wrap-up session at the end. Note: The \u201cExtra Topic\u201d sessions will not repeat \u2014 each one is a separate topic. Monday, August 9 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available; (scheduled separately:) mentor 1-1 meetings 10:00\u201311:00 Extra Topic 1: Self-checkpointing for long-running jobs 11:00\u201315:00 Staff available; (scheduled separately:) mentor 1-1 meetings 15:00\u201316:00 Extra Topic 2: Containers and GPUs 16:00\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings Tuesday, August 10 \u00b6 Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:00 Extra Topic 3: Introduction to HTC Facilitation 11:00\u201315:00 Staff available 15:00\u201316:00 Extra Topic 4: Workflows with DAGMan 16:00\u201317:00 Staff available Wednesday, August 11 \u00b6 The OSG School Showcase consists of three researchers talking about their science, their computing, and how the process of learning about and using High Throughput Computing transformed their work. For many participants of past Schools, this was a highlight! Please try to arrange your schedule to attend this one-time-only event. There will be time at the end to talk to the presenters. Times (US Central) Activities 9:00\u201311:00 Staff available 11:00\u201312:30 OSG School Showcase 12:30\u201317:00 Staff available Thursday, August 12 \u00b6 Times (US Central) Activities 9:00\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings Friday, August 13 \u00b6 For our last opportunity to be together, there will be a short wrap-up talk, plus a series of lightning talks from the School participants! Lightning talks are optional but encouraged; they are simply a way to present a little bit about your work and what you accomplished during the School. Details, including how to sign up, will be available during the School. Times (US Central) Activities 9:00\u201311:00 Staff available; (scheduled separately:) mentor 1-1 meetings 11:00\u201312:30 Wrap-up and Lightning Talks 12:30\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings","title":"Schedule"},{"location":"schedule/#osgvs21-schedule","text":"This page contains schedule details for full, invited participants of the OSG Virtual School 2021, who were selected following a prior application process. SOME presentations will presented publicly. For information about access to public lectures and demonstrations, visit the Indico scheduling site ; note that public attendance requires free registration on that site.","title":"OSGVS21 Schedule"},{"location":"schedule/#week-1-august-26","text":"Note: Lectures and demonstrations this week are repeated each day. The first instance is for invited School participants only, and the second instance will also be open to the general public who have registered in advance. Invited participants may attend one or both times. During the times marked as \u201cStaff available\u201d, School staff will be monitoring email and the OSG School Slack for questions. So these are good times to work on School-related activities \u2014 such as exercises or applying things to your own work. .md-typeset table:not([class]) td, .md-typeset table:not([class]) th { padding-top: 0.2em; padding-bottom: 0.2em; }","title":"Week 1: August 2&ndash;6"},{"location":"schedule/#monday-august-2","text":"Times (US Central) Activities 9:00\u201310:00 Staff available; (scheduled separately:) mentor 1-1 meetings and mentor group meetings 10:00\u201311:00 Lecture: Introduction to High Throughput Computing (participants only) 11:00\u201314:30 Staff available; (scheduled separately:) mentor 1-1 meetings and mentor group meetings 14:30\u201315:30 Lecture: Introduction to High Throughput Computing (public) 15:30\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings and mentor group meetings","title":"Monday, August 2"},{"location":"schedule/#tuesday-august-3","text":"Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Introduction to HTCondor (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Introduction to HTCondor (public) 16:00\u201317:00 Staff available","title":"Tuesday, August 3"},{"location":"schedule/#wednesday-august-4","text":"Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Introduction to OSG (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Introduction to OSG (public) 16:00\u201317:00 Staff available","title":"Wednesday, August 4"},{"location":"schedule/#thursday-august-5","text":"Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Software for HTC (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Software for HTC (public) 16:00\u201317:00 Staff available","title":"Thursday, August 5"},{"location":"schedule/#friday-august-6","text":"Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:30 Lecture and demonstration: Data for HTC (participants only) 11:30\u201314:30 Staff available 14:30\u201316:00 Lecture and demonstration: Data for HTC (public) 16:00\u201317:00 Staff available","title":"Friday, August 6"},{"location":"schedule/#week-2-august-913","text":"The focus of this week is on applying what you have learned (and are learning!) to your own work. Plus, there are opportunities to learn more and a wrap-up session at the end. Note: The \u201cExtra Topic\u201d sessions will not repeat \u2014 each one is a separate topic.","title":"Week 2: August 9&ndash;13"},{"location":"schedule/#monday-august-9","text":"Times (US Central) Activities 9:00\u201310:00 Staff available; (scheduled separately:) mentor 1-1 meetings 10:00\u201311:00 Extra Topic 1: Self-checkpointing for long-running jobs 11:00\u201315:00 Staff available; (scheduled separately:) mentor 1-1 meetings 15:00\u201316:00 Extra Topic 2: Containers and GPUs 16:00\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings","title":"Monday, August 9"},{"location":"schedule/#tuesday-august-10","text":"Times (US Central) Activities 9:00\u201310:00 Staff available 10:00\u201311:00 Extra Topic 3: Introduction to HTC Facilitation 11:00\u201315:00 Staff available 15:00\u201316:00 Extra Topic 4: Workflows with DAGMan 16:00\u201317:00 Staff available","title":"Tuesday, August 10"},{"location":"schedule/#wednesday-august-11","text":"The OSG School Showcase consists of three researchers talking about their science, their computing, and how the process of learning about and using High Throughput Computing transformed their work. For many participants of past Schools, this was a highlight! Please try to arrange your schedule to attend this one-time-only event. There will be time at the end to talk to the presenters. Times (US Central) Activities 9:00\u201311:00 Staff available 11:00\u201312:30 OSG School Showcase 12:30\u201317:00 Staff available","title":"Wednesday, August 11"},{"location":"schedule/#thursday-august-12","text":"Times (US Central) Activities 9:00\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings","title":"Thursday, August 12"},{"location":"schedule/#friday-august-13","text":"For our last opportunity to be together, there will be a short wrap-up talk, plus a series of lightning talks from the School participants! Lightning talks are optional but encouraged; they are simply a way to present a little bit about your work and what you accomplished during the School. Details, including how to sign up, will be available during the School. Times (US Central) Activities 9:00\u201311:00 Staff available; (scheduled separately:) mentor 1-1 meetings 11:00\u201312:30 Wrap-up and Lightning Talks 12:30\u201317:00 Staff available; (scheduled separately:) mentor 1-1 meetings","title":"Friday, August 13"},{"location":"staff/","text":"OSGVS21 Staff \u00b6 img { margin: 5px 0; } tr { vertical-align: top; } Tim Cartwright Tim founded the OSG School in 2010 and has led it since. When not involved in training, he mostly works for OSG in management, where he helps the project as a whole in a variety of capacities. Tim started working with OSG in 2005, and has background in cognitive science, linguistics, training, and software. Lauren Michael Lauren serves as co-Director of the OSG School program. She leads the Research Computing Facilitation teams for the OSG and at UW-Madison, where she has worked since 2013, and leverages research background in Biophysics and Life Sciences Communication. Christina Koch Christina is a Research Computing Facilitator at the University of Wisconsin\u2013Madison, where she helps researchers transition their big computational problems to large-scale computing resources. She facilitates use of both local resources at UW\u2013Madison and access to the national Open Science Grid. Derek Weitzel Derek is a research assistant professor at the University of Nebraska\u2013Lincoln. He is a member of the OSG Technology and Operations teams and has worked with the OSG since graduate school in 2009. Emelie Fuchs Emelie is an HPC Applications Specialist and OSG Research Facilitator at the University of Nebraska\u2013Lincoln Holland Computing Center. Ian Ross Ian works as a developer in the Center for High Throughput Computing, coupling high-throughput workflows with systems to enable text data-mining across millions of scientific documents. He has worked in the group since 2014, after completing his Ph.D. in high energy particle physics. Janet Stathas Janet is a Project Manager at the Morgridge Institute for Research and works primarily on the Partnership to Advance Throughput Computing (PATh) project. For the School she provides administrative and event planning support to the rest of the team. Jason Patton Jason is a Software Integration Scientist for the CHTC. He assists researchers and system administrators to get their applications working with the HTCondor software. He has worked for the CHTC at the University of Wisconsin\u2013Madison since 2016. Mat Rynge Mats is a senior computer scientist in the Scitech group. He is currently working on infrastructure and support for the Open Science Grid, as an Extended Collaborative Support Services (ECSS) consultant for XSEDE, and on cyberinfrastructure for NSF Large Facilities. Recent accomplishments include the Pegasus 5.0 release and a best technical paper award at the PEARC19 conference. Ryan Tanaka Ryan is a research programmer in the Science Automation Technologies group at the USC Information Sciences Institute. He currently works on developing the Pegasus Workflow Management System.","title":"Staff"},{"location":"staff/#osgvs21-staff","text":"img { margin: 5px 0; } tr { vertical-align: top; } Tim Cartwright Tim founded the OSG School in 2010 and has led it since. When not involved in training, he mostly works for OSG in management, where he helps the project as a whole in a variety of capacities. Tim started working with OSG in 2005, and has background in cognitive science, linguistics, training, and software. Lauren Michael Lauren serves as co-Director of the OSG School program. She leads the Research Computing Facilitation teams for the OSG and at UW-Madison, where she has worked since 2013, and leverages research background in Biophysics and Life Sciences Communication. Christina Koch Christina is a Research Computing Facilitator at the University of Wisconsin\u2013Madison, where she helps researchers transition their big computational problems to large-scale computing resources. She facilitates use of both local resources at UW\u2013Madison and access to the national Open Science Grid. Derek Weitzel Derek is a research assistant professor at the University of Nebraska\u2013Lincoln. He is a member of the OSG Technology and Operations teams and has worked with the OSG since graduate school in 2009. Emelie Fuchs Emelie is an HPC Applications Specialist and OSG Research Facilitator at the University of Nebraska\u2013Lincoln Holland Computing Center. Ian Ross Ian works as a developer in the Center for High Throughput Computing, coupling high-throughput workflows with systems to enable text data-mining across millions of scientific documents. He has worked in the group since 2014, after completing his Ph.D. in high energy particle physics. Janet Stathas Janet is a Project Manager at the Morgridge Institute for Research and works primarily on the Partnership to Advance Throughput Computing (PATh) project. For the School she provides administrative and event planning support to the rest of the team. Jason Patton Jason is a Software Integration Scientist for the CHTC. He assists researchers and system administrators to get their applications working with the HTCondor software. He has worked for the CHTC at the University of Wisconsin\u2013Madison since 2016. Mat Rynge Mats is a senior computer scientist in the Scitech group. He is currently working on infrastructure and support for the Open Science Grid, as an Extended Collaborative Support Services (ECSS) consultant for XSEDE, and on cyberinfrastructure for NSF Large Facilities. Recent accomplishments include the Pegasus 5.0 release and a best technical paper award at the PEARC19 conference. Ryan Tanaka Ryan is a research programmer in the Science Automation Technologies group at the USC Information Sciences Institute. He currently works on developing the Pegasus Workflow Management System.","title":"OSGVS21 Staff"},{"location":"materials/","text":"OSG Virtual School Materials \u00b6 School Overview and Intro to HTC \u00b6 View the slides ( PDF , PowerPoint ) Watch the lecture recording ( Intro to the Virtual School, HTC, and OSG ) Intro to HTCondor Job Execution \u00b6 View the slides ( PDF , PowerPoint ) Intro Exercises 1: Running and Viewing Simple Jobs (Strongly Recommended) \u00b6 Exercise 1.1: Log in to the local submit machine and look around Exercise 1.2: Experiment with HTCondor commands Exercise 1.3: Run jobs! Exercise 1.4: Read and interpret log files Exercise 1.5: Determining Resource Needs Exercise 1.6: Remove jobs from the queue Bonus Exercise 1.7: Compile and run some C code Intro Exercises 2: Running Many HTC Jobs (Strongly Recommended) \u00b6 Exercise 2.1: Work with input and output files Exercise 2.2: Use queue N , $(Cluster) , and $(Process) Exercise 2.3: Use queue from with custom variables Bonus Exercise 2.4: Use queue matching with a custom variable Bonus Exercises: Job Attributes and Handling \u00b6 Bonus Exercise 3.1: Explore condor_q Bonus Exercise 3.2: Explore condor_status Bonus Exercise 3.3: A job that needs retries OSG \u00b6 Links to the slides and video will be posted here. All exercises strongly recommended! Exercise 1: Refresher \u2013 Submitting Multiple Jobs Exercise 2: Log in to the OSG Submit Server Exercise 3: Running jobs in the OSG Exercise 4: Hardware Differences in the OSG Exercise 5: Software Differences in the OSG Software \u00b6 Slides will be posted here Software Exercises 1: Basic Software and Wrapper Script Use (Strongly Recommended) \u00b6 Exercise 1.1: Work With Downloaded Software Exercise 1.2: Use a Wrapper Script To Run Software Exercise 1.3: Using Arguments With Wrapper Scripts Software Exercises 2: Specific Software Examples (Pick One) \u00b6 Exercise 2.1: Compiling and Running a Simple Code Exercise 2.2: Compiling a Research Software Exercise 2.3: Compiling Python and Running Jobs Exercise 2.4: Compiling Matlab and Running Jobs Exercise 2.5: Using Conda Environments (Beta) Software Exercises 3: Using Containers in Jobs (Strongly Recommended) \u00b6 Exercise 3.1: Using Software in a Singularity Container Bonus Exercises: Python and Containers \u00b6 Exercise 4.1: Additional Python Exercise 4.2: Using Software in a Docker Container Exercise 4.3: Building Your Own Docker Container (Beta) Data \u00b6 Slides will be posted here Data Exercises 1: HTCondor File Transfer (Strongly Recommended) \u00b6 Exercise 1.1: Understanding a job's data needs Exercise 1.2: Using data compression with HTCondor file transfer Exercise 1.3: Splitting input Data Exercises 2: Using Stash (Strongly Recommended) \u00b6 Exercise 2.1: Using a web proxy for shared input Exercise 2.2: Stash for shared input Exercise 2.3: Stash for shared output Bonus Exercises: Shared File Systems \u00b6 Exercise 3.1: Shared filesystems for large input Exercise 3.2: Shared filesystems for large output Extra Topics \u00b6 Self-checkpointing for long-running jobs \u00b6 Slides will be posted here. Containers and GPUs \u00b6 View the slides ( PDF , PowerPoint ) Exercise 1.1: Containers Overview Exercise 1.2: Running a CPU job Exercise 1.3: Running a GPU job Introduction to Research Computing Facilitation \u00b6 Slides will be posted here. Workflows with DAGMan \u00b6 Slides will be posted here. Exercise 1.1: Coordinating set of jobs: A simple DAG Exercise 1.2: A brief detour through the Mandelbrot set Exercise 1.3: A more complex DAG Exercise 1.4: Handling jobs that fail with DAGMan Bonus Exercise 4.5: HTCondor challenges","title":"Overview"},{"location":"materials/#osg-virtual-school-materials","text":"","title":"OSG Virtual School Materials"},{"location":"materials/#school-overview-and-intro-to-htc","text":"View the slides ( PDF , PowerPoint ) Watch the lecture recording ( Intro to the Virtual School, HTC, and OSG )","title":"School Overview and Intro to HTC"},{"location":"materials/#intro-to-htcondor-job-execution","text":"View the slides ( PDF , PowerPoint )","title":"Intro to HTCondor Job Execution"},{"location":"materials/#intro-exercises-1-running-and-viewing-simple-jobs-strongly-recommended","text":"Exercise 1.1: Log in to the local submit machine and look around Exercise 1.2: Experiment with HTCondor commands Exercise 1.3: Run jobs! Exercise 1.4: Read and interpret log files Exercise 1.5: Determining Resource Needs Exercise 1.6: Remove jobs from the queue Bonus Exercise 1.7: Compile and run some C code","title":"Intro Exercises 1: Running and Viewing Simple Jobs (Strongly Recommended)"},{"location":"materials/#intro-exercises-2-running-many-htc-jobs-strongly-recommended","text":"Exercise 2.1: Work with input and output files Exercise 2.2: Use queue N , $(Cluster) , and $(Process) Exercise 2.3: Use queue from with custom variables Bonus Exercise 2.4: Use queue matching with a custom variable","title":"Intro Exercises 2: Running Many HTC Jobs (Strongly Recommended)"},{"location":"materials/#bonus-exercises-job-attributes-and-handling","text":"Bonus Exercise 3.1: Explore condor_q Bonus Exercise 3.2: Explore condor_status Bonus Exercise 3.3: A job that needs retries","title":"Bonus Exercises: Job Attributes and Handling"},{"location":"materials/#osg","text":"Links to the slides and video will be posted here. All exercises strongly recommended! Exercise 1: Refresher \u2013 Submitting Multiple Jobs Exercise 2: Log in to the OSG Submit Server Exercise 3: Running jobs in the OSG Exercise 4: Hardware Differences in the OSG Exercise 5: Software Differences in the OSG","title":"OSG"},{"location":"materials/#software","text":"Slides will be posted here","title":"Software"},{"location":"materials/#software-exercises-1-basic-software-and-wrapper-script-use-strongly-recommended","text":"Exercise 1.1: Work With Downloaded Software Exercise 1.2: Use a Wrapper Script To Run Software Exercise 1.3: Using Arguments With Wrapper Scripts","title":"Software Exercises 1: Basic Software and Wrapper Script Use (Strongly Recommended)"},{"location":"materials/#software-exercises-2-specific-software-examples-pick-one","text":"Exercise 2.1: Compiling and Running a Simple Code Exercise 2.2: Compiling a Research Software Exercise 2.3: Compiling Python and Running Jobs Exercise 2.4: Compiling Matlab and Running Jobs Exercise 2.5: Using Conda Environments (Beta)","title":"Software Exercises 2: Specific Software Examples (Pick One)"},{"location":"materials/#software-exercises-3-using-containers-in-jobs-strongly-recommended","text":"Exercise 3.1: Using Software in a Singularity Container","title":"Software Exercises 3: Using Containers in Jobs (Strongly Recommended)"},{"location":"materials/#bonus-exercises-python-and-containers","text":"Exercise 4.1: Additional Python Exercise 4.2: Using Software in a Docker Container Exercise 4.3: Building Your Own Docker Container (Beta)","title":"Bonus Exercises: Python and Containers"},{"location":"materials/#data","text":"Slides will be posted here","title":"Data"},{"location":"materials/#data-exercises-1-htcondor-file-transfer-strongly-recommended","text":"Exercise 1.1: Understanding a job's data needs Exercise 1.2: Using data compression with HTCondor file transfer Exercise 1.3: Splitting input","title":"Data Exercises 1: HTCondor File Transfer (Strongly Recommended)"},{"location":"materials/#data-exercises-2-using-stash-strongly-recommended","text":"Exercise 2.1: Using a web proxy for shared input Exercise 2.2: Stash for shared input Exercise 2.3: Stash for shared output","title":"Data Exercises 2: Using Stash (Strongly Recommended)"},{"location":"materials/#bonus-exercises-shared-file-systems","text":"Exercise 3.1: Shared filesystems for large input Exercise 3.2: Shared filesystems for large output","title":"Bonus Exercises: Shared File Systems"},{"location":"materials/#extra-topics","text":"","title":"Extra Topics"},{"location":"materials/#self-checkpointing-for-long-running-jobs","text":"Slides will be posted here.","title":"Self-checkpointing for long-running jobs"},{"location":"materials/#containers-and-gpus","text":"View the slides ( PDF , PowerPoint ) Exercise 1.1: Containers Overview Exercise 1.2: Running a CPU job Exercise 1.3: Running a GPU job","title":"Containers and GPUs"},{"location":"materials/#introduction-to-research-computing-facilitation","text":"Slides will be posted here.","title":"Introduction to Research Computing Facilitation"},{"location":"materials/#workflows-with-dagman","text":"Slides will be posted here. Exercise 1.1: Coordinating set of jobs: A simple DAG Exercise 1.2: A brief detour through the Mandelbrot set Exercise 1.3: A more complex DAG Exercise 1.4: Handling jobs that fail with DAGMan Bonus Exercise 4.5: HTCondor challenges","title":"Workflows with DAGMan"},{"location":"materials/data/part1-ex1-data-needs/","text":"Data Exercise 1.1: Understanding Data Requirements \u00b6 This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application BLAST . Setup \u00b6 Log in to login04.osgconnect.net Create a directory for this exercise named blast-data and change into it Copy the Input Files \u00b6 To run BLAST, we need the executable, input file, and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Copy the BLAST executables: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/ncbi-blast-2.12.0+-x64-linux.tar.gz user@login04 $ tar -xzvf ncbi-blast-2.12.0+-x64-linux.tar.gz Download these files to your current directory: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/pdbaa.tar.gz user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/mouse.fa Untar the pdbaa database: user@login04 $ tar -xzvf pdbaa.tar.gz Understanding BLAST \u00b6 Remember that blastx is executed in a command like the following: user@login04 $ ./blastx -db <DATABASE ROOTNAME> -query <INPUT FILE> -out <RESULTS FILE> In the above, the <INPUT FILE> is the name of a file containing a number of genetic sequences (e.g. mouse.fa ), and the database that these are compared against is made up of several files that begin with the same <DATABASE ROOTNAME> , (e.g. pdbaa/pdbaa ). The output from this analysis will be printed to <RESULTS FILE> that is also indicated in the command. Calculating Data Needs \u00b6 Using the files that you prepared in blast-data , we will calculate how much disk space is needed if we were to run a hypothetical BLAST job with a wrapper script, where the job: Transfers all of its input files (including the executable) as tarballs Untars the input files tarballs on the execute host Runs blastx using the untarred input files Here are some commands that will be useful for calculating your job's storage needs: List the size of a specific file: user@login04 $ ls -lh <FILE NAME> List the sizes of all files in the current directory: user@login04 $ ls -lh Sum the size of all files in a specific directory: user@login04 $ du -sh <DIRECTORY> Input requirements \u00b6 Total up the amount of data in all of the files necessary to run the blastx wrapper job, including the executable itself. Write down this number. Also take note of how much total data is in the pdbaa directory. Compressed Files Remember, blastx reads the un-compressed pdbaa files. Output requirements \u00b6 The output that we care about from blastx is saved in the file whose name is indicated after the -out argument to blastx . Also, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well. Up next! \u00b6 Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. Next Exercise","title":"Exercise 1.1"},{"location":"materials/data/part1-ex1-data-needs/#data-exercise-11-understanding-data-requirements","text":"This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application BLAST .","title":"Data Exercise 1.1: Understanding Data Requirements"},{"location":"materials/data/part1-ex1-data-needs/#setup","text":"Log in to login04.osgconnect.net Create a directory for this exercise named blast-data and change into it","title":"Setup"},{"location":"materials/data/part1-ex1-data-needs/#copy-the-input-files","text":"To run BLAST, we need the executable, input file, and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Copy the BLAST executables: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/ncbi-blast-2.12.0+-x64-linux.tar.gz user@login04 $ tar -xzvf ncbi-blast-2.12.0+-x64-linux.tar.gz Download these files to your current directory: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/pdbaa.tar.gz user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/mouse.fa Untar the pdbaa database: user@login04 $ tar -xzvf pdbaa.tar.gz","title":"Copy the Input Files"},{"location":"materials/data/part1-ex1-data-needs/#understanding-blast","text":"Remember that blastx is executed in a command like the following: user@login04 $ ./blastx -db <DATABASE ROOTNAME> -query <INPUT FILE> -out <RESULTS FILE> In the above, the <INPUT FILE> is the name of a file containing a number of genetic sequences (e.g. mouse.fa ), and the database that these are compared against is made up of several files that begin with the same <DATABASE ROOTNAME> , (e.g. pdbaa/pdbaa ). The output from this analysis will be printed to <RESULTS FILE> that is also indicated in the command.","title":"Understanding BLAST"},{"location":"materials/data/part1-ex1-data-needs/#calculating-data-needs","text":"Using the files that you prepared in blast-data , we will calculate how much disk space is needed if we were to run a hypothetical BLAST job with a wrapper script, where the job: Transfers all of its input files (including the executable) as tarballs Untars the input files tarballs on the execute host Runs blastx using the untarred input files Here are some commands that will be useful for calculating your job's storage needs: List the size of a specific file: user@login04 $ ls -lh <FILE NAME> List the sizes of all files in the current directory: user@login04 $ ls -lh Sum the size of all files in a specific directory: user@login04 $ du -sh <DIRECTORY>","title":"Calculating Data Needs"},{"location":"materials/data/part1-ex1-data-needs/#input-requirements","text":"Total up the amount of data in all of the files necessary to run the blastx wrapper job, including the executable itself. Write down this number. Also take note of how much total data is in the pdbaa directory. Compressed Files Remember, blastx reads the un-compressed pdbaa files.","title":"Input requirements"},{"location":"materials/data/part1-ex1-data-needs/#output-requirements","text":"The output that we care about from blastx is saved in the file whose name is indicated after the -out argument to blastx . Also, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well.","title":"Output requirements"},{"location":"materials/data/part1-ex1-data-needs/#up-next","text":"Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. Next Exercise","title":"Up next!"},{"location":"materials/data/part1-ex2-file-transfer/","text":"Data Exercise 1.2: File Compression and Testing Resource Requirements \u00b6 The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, and to begin examining the memory and disk space used by your jobs in order to plan larger batches, which we'll tackle in later exercises today. Setup \u00b6 The executable we'll use in this exercise and later today is the same blastx executable from previous exercises. Log in to login04.osgconnect.net Change into the blast-data folder that you created in the previous exercise. Review: HTCondor File Transfer \u00b6 Recall that OSG does NOT have a shared filesystem! Instead, HTCondor transfers your executable and input files (specified with the executable and transfer_input_files submit file directives, respectively) to a working directory on the execute node, regardless of how these files were arranged on the submit node. In this exercise we'll use the same blastx example job that we used previously, but modify the submit file and test how much memory and disk space it uses on the execute node. Start with a test submit file \u00b6 We've started a submit file for you, below, which you'll add to in the remaining steps. executable = transfer_input_files = output = test.out error = test.err log = test.log request_memory = request_disk = request_cpus = 1 requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue Implement file compression \u00b6 In our first blast job from the Software exercises ( 1.1 ), the database files in the pdbaa directory were all transferred, as is, but we could instead transfer them as a single, compressed file using tar . For this version of the job, let's compress our blast database files to send them to the submit node as a single tar.gz file (otherwise known as a tarball), by following the below steps: Change into the pdbaa directory and compress the database files into a single file called pdbaa_files.tar.gz using the tar command. Note that this file will be different from the pdbaa.tar.gz file that you used earlier, because it will only contain the pdbaa files, and not the pdbaa directory, itself.) Remember, a typical command for creating a tar file is: user@login04 $ tar -cvzf <COMPRESSED FILENAME> <LIST OF FILES OR DIRECTORIES> Replacing <COMPRESSED FILENAME> with the name of the tarball that you would like to create and <LIST OF FILES OR DIRECTORIES> with a space-separated list of files and/or directories that you want inside pdbaa_files.tar.gz. Move the resulting tarball to the blast-data directory. Create a wrapper script that will first decompress the pdbaa_files.tar.gz file, and then run blast. Because this file will now be our executable in the submit file, we'll also end up transferring the blastx executable with transfer_input_files . In the blast-data directory, create a new file, called blast_wrapper.sh , with the following contents: #!/bin/bash tar -xzvf pdbaa_files.tar.gz ./blastx -db pdbaa -query mouse.fa -out mouse.fa.result rm pdbaa.* Extra Files! The last line removes the resulting database files that came from pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output since they're \"new\" files that HTCondor didn't transfer over as input. List the executable and input files \u00b6 Make sure to update the submit file with the following: Add the new executable (the wrapper script you created above) In transfer_input_files , list the blastx binary, the pdbaa_files.tar.gz file, and the input query file. Commas, commas everywhere! Remember that transfer_input_files accepts a comma separated list of files, and that you need to list the full location of the blastx executable ( blastx ). There will be no arguments, since the arguments to the blastx command are now captured in the wrapper script. Predict memory and disk requests from your data \u00b6 Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about: How much memory blastx would use if it loaded all of the database files and the query input file into memory. How much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node). Whether you'd like to request some extra memory or disk space, just in case Look at the log file for your blastx job from Software exercise ( 1.1 ), and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests (you may still want to request slightly more than the job actually used). Run the test job \u00b6 Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the pdbaa database files) were copied back at the end of the job. Run a du -sh on the directory with this job's input. How does it compare to the directory from Software exercise ( 1.1 ), and why? Conclusions \u00b6 In this exercise, you: Used your data requirements knowledge from the previous exercise to write a job. Executed the job on a remote worker node and took note of the data usage. When you've completed the above, continue with the next exercise .","title":"Exercise 1.2"},{"location":"materials/data/part1-ex2-file-transfer/#data-exercise-12-file-compression-and-testing-resource-requirements","text":"The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, and to begin examining the memory and disk space used by your jobs in order to plan larger batches, which we'll tackle in later exercises today.","title":"Data Exercise 1.2: File Compression and Testing Resource Requirements"},{"location":"materials/data/part1-ex2-file-transfer/#setup","text":"The executable we'll use in this exercise and later today is the same blastx executable from previous exercises. Log in to login04.osgconnect.net Change into the blast-data folder that you created in the previous exercise.","title":"Setup"},{"location":"materials/data/part1-ex2-file-transfer/#review-htcondor-file-transfer","text":"Recall that OSG does NOT have a shared filesystem! Instead, HTCondor transfers your executable and input files (specified with the executable and transfer_input_files submit file directives, respectively) to a working directory on the execute node, regardless of how these files were arranged on the submit node. In this exercise we'll use the same blastx example job that we used previously, but modify the submit file and test how much memory and disk space it uses on the execute node.","title":"Review: HTCondor File Transfer"},{"location":"materials/data/part1-ex2-file-transfer/#start-with-a-test-submit-file","text":"We've started a submit file for you, below, which you'll add to in the remaining steps. executable = transfer_input_files = output = test.out error = test.err log = test.log request_memory = request_disk = request_cpus = 1 requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue","title":"Start with a test submit file"},{"location":"materials/data/part1-ex2-file-transfer/#implement-file-compression","text":"In our first blast job from the Software exercises ( 1.1 ), the database files in the pdbaa directory were all transferred, as is, but we could instead transfer them as a single, compressed file using tar . For this version of the job, let's compress our blast database files to send them to the submit node as a single tar.gz file (otherwise known as a tarball), by following the below steps: Change into the pdbaa directory and compress the database files into a single file called pdbaa_files.tar.gz using the tar command. Note that this file will be different from the pdbaa.tar.gz file that you used earlier, because it will only contain the pdbaa files, and not the pdbaa directory, itself.) Remember, a typical command for creating a tar file is: user@login04 $ tar -cvzf <COMPRESSED FILENAME> <LIST OF FILES OR DIRECTORIES> Replacing <COMPRESSED FILENAME> with the name of the tarball that you would like to create and <LIST OF FILES OR DIRECTORIES> with a space-separated list of files and/or directories that you want inside pdbaa_files.tar.gz. Move the resulting tarball to the blast-data directory. Create a wrapper script that will first decompress the pdbaa_files.tar.gz file, and then run blast. Because this file will now be our executable in the submit file, we'll also end up transferring the blastx executable with transfer_input_files . In the blast-data directory, create a new file, called blast_wrapper.sh , with the following contents: #!/bin/bash tar -xzvf pdbaa_files.tar.gz ./blastx -db pdbaa -query mouse.fa -out mouse.fa.result rm pdbaa.* Extra Files! The last line removes the resulting database files that came from pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output since they're \"new\" files that HTCondor didn't transfer over as input.","title":"Implement file compression"},{"location":"materials/data/part1-ex2-file-transfer/#list-the-executable-and-input-files","text":"Make sure to update the submit file with the following: Add the new executable (the wrapper script you created above) In transfer_input_files , list the blastx binary, the pdbaa_files.tar.gz file, and the input query file. Commas, commas everywhere! Remember that transfer_input_files accepts a comma separated list of files, and that you need to list the full location of the blastx executable ( blastx ). There will be no arguments, since the arguments to the blastx command are now captured in the wrapper script.","title":"List the executable and input files"},{"location":"materials/data/part1-ex2-file-transfer/#predict-memory-and-disk-requests-from-your-data","text":"Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about: How much memory blastx would use if it loaded all of the database files and the query input file into memory. How much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node). Whether you'd like to request some extra memory or disk space, just in case Look at the log file for your blastx job from Software exercise ( 1.1 ), and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests (you may still want to request slightly more than the job actually used).","title":"Predict memory and disk requests from your data"},{"location":"materials/data/part1-ex2-file-transfer/#run-the-test-job","text":"Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the pdbaa database files) were copied back at the end of the job. Run a du -sh on the directory with this job's input. How does it compare to the directory from Software exercise ( 1.1 ), and why?","title":"Run the test job"},{"location":"materials/data/part1-ex2-file-transfer/#conclusions","text":"In this exercise, you: Used your data requirements knowledge from the previous exercise to write a job. Executed the job on a remote worker node and took note of the data usage. When you've completed the above, continue with the next exercise .","title":"Conclusions"},{"location":"materials/data/part1-ex3-blast-split/","text":"Data Exercise 1.3: Splitting Large Input for Better Throughput \u00b6 The objective of this exercise is to prepare for blasting a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files. Setup \u00b6 Log in to login04.osgconnect.net Create a directory for this exercise named blast-split and change into it. Copy over the following files from the previous exercise : Your submit file blastx pdbaa_files.tar.gz blast_wrapper.sh Remember to modify the submit file for the new locations of the above files. Obtain the large input \u00b6 We've previously used blastx to analyze a relatively small input file of test data, mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/mouse_rna.tar.gz After un-tar'ing ( tar xzf mouse_rna.tar.gz ) the file, you should be able to confirm that it's size is roughly 100 MB. Not only is this near the size cutoff for HTCondor file transfer, it would take hours to complete a single blastx analysis for it and the resulting output file would be huge. Split the input file \u00b6 For blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database. Because genetic sequence data is used heavily across the life sciences, there are also tools for splitting up the data into smaller files. One of these is called genome tools , and you can download a package of precompiled binaries (just like BLAST) using the following command: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/gt-1.5.10-Linux_x86_64-64bit-complete.tar.gz Un-tar the gt package ( tar -xzvf ... ), then run its sequence file splitter as follows, with the target file size of 1MB: user@login04 $ ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 1 mouse_rna.fa You'll notice that the result is a set of 100 files, all about the size of 1 MB, and numbered 1 through 100. Run a Jobs on Split Input \u00b6 Now, you'll submit jobs on the split input files, where each job will use a different piece of the large original input file. Modify the submit file \u00b6 First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps: Copy the submit file from the previous exercise to a new file called blast_split.sub and modify the \"queue\" line of the submit file to the following: queue inputfile matching mouse_rna.fa.* Replace the mouse.fa instances in the submit file with $(inputfile) , and rename the output, log, and error files to use the same inputfile variable: output = $(inputfile).out error = $(inputfile).err log = $(inputfile).log Add an arguments line to the submit file so it will pass the name of the input file to the wrapper script arguments = $(inputfile) Add the $(inputfile) to the end of your list of transfer_input_files : transfer_input_files = ... , $(inputfile) Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each. Modify the wrapper file \u00b6 Replace instances of the input file name in the blast_wrapper.sh script so that it will insert the first argument in place of the input filename, like so: ./blastx -db pdbaa -query $1 -out $1.result Note Bash shell scripts will use the first argument in place of $1 , the second argument as $2 , etc. Submit the jobs \u00b6 This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired output , error , and result files come back at the end of the job. In our tests, the jobs ran for ~15 minutes. Jobs on jobs! Be careful to not submit the job again. Why? Our queue statement says ... matching mouse_rna.fa.* , and look at the current directory. There are new files named mouse_rna.fa.X.log and other files. Submitting again, the queue statement would see these new files, and try to run blast on them! If you want to remove all of the extra files, you can try: user@login04 $ rm *.error *.log *.out *.result Update the resource requests \u00b6 After the job finishes successfully, examine the log file for memory and disk usage, and update the requests in the submit file. In Exercise 2.1 you'll submit many jobs at once and use a different method for handling the pdbaa_files.tar.gz file, which is a bit too large to use regular file transfer when submitting many jobs.","title":"Exercise 1.3"},{"location":"materials/data/part1-ex3-blast-split/#data-exercise-13-splitting-large-input-for-better-throughput","text":"The objective of this exercise is to prepare for blasting a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files.","title":"Data Exercise 1.3: Splitting Large Input for Better Throughput"},{"location":"materials/data/part1-ex3-blast-split/#setup","text":"Log in to login04.osgconnect.net Create a directory for this exercise named blast-split and change into it. Copy over the following files from the previous exercise : Your submit file blastx pdbaa_files.tar.gz blast_wrapper.sh Remember to modify the submit file for the new locations of the above files.","title":"Setup"},{"location":"materials/data/part1-ex3-blast-split/#obtain-the-large-input","text":"We've previously used blastx to analyze a relatively small input file of test data, mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/mouse_rna.tar.gz After un-tar'ing ( tar xzf mouse_rna.tar.gz ) the file, you should be able to confirm that it's size is roughly 100 MB. Not only is this near the size cutoff for HTCondor file transfer, it would take hours to complete a single blastx analysis for it and the resulting output file would be huge.","title":"Obtain the large input"},{"location":"materials/data/part1-ex3-blast-split/#split-the-input-file","text":"For blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database. Because genetic sequence data is used heavily across the life sciences, there are also tools for splitting up the data into smaller files. One of these is called genome tools , and you can download a package of precompiled binaries (just like BLAST) using the following command: user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/gt-1.5.10-Linux_x86_64-64bit-complete.tar.gz Un-tar the gt package ( tar -xzvf ... ), then run its sequence file splitter as follows, with the target file size of 1MB: user@login04 $ ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 1 mouse_rna.fa You'll notice that the result is a set of 100 files, all about the size of 1 MB, and numbered 1 through 100.","title":"Split the input file"},{"location":"materials/data/part1-ex3-blast-split/#run-a-jobs-on-split-input","text":"Now, you'll submit jobs on the split input files, where each job will use a different piece of the large original input file.","title":"Run a Jobs on Split Input"},{"location":"materials/data/part1-ex3-blast-split/#modify-the-submit-file","text":"First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps: Copy the submit file from the previous exercise to a new file called blast_split.sub and modify the \"queue\" line of the submit file to the following: queue inputfile matching mouse_rna.fa.* Replace the mouse.fa instances in the submit file with $(inputfile) , and rename the output, log, and error files to use the same inputfile variable: output = $(inputfile).out error = $(inputfile).err log = $(inputfile).log Add an arguments line to the submit file so it will pass the name of the input file to the wrapper script arguments = $(inputfile) Add the $(inputfile) to the end of your list of transfer_input_files : transfer_input_files = ... , $(inputfile) Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each.","title":"Modify the submit file"},{"location":"materials/data/part1-ex3-blast-split/#modify-the-wrapper-file","text":"Replace instances of the input file name in the blast_wrapper.sh script so that it will insert the first argument in place of the input filename, like so: ./blastx -db pdbaa -query $1 -out $1.result Note Bash shell scripts will use the first argument in place of $1 , the second argument as $2 , etc.","title":"Modify the wrapper file"},{"location":"materials/data/part1-ex3-blast-split/#submit-the-jobs","text":"This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired output , error , and result files come back at the end of the job. In our tests, the jobs ran for ~15 minutes. Jobs on jobs! Be careful to not submit the job again. Why? Our queue statement says ... matching mouse_rna.fa.* , and look at the current directory. There are new files named mouse_rna.fa.X.log and other files. Submitting again, the queue statement would see these new files, and try to run blast on them! If you want to remove all of the extra files, you can try: user@login04 $ rm *.error *.log *.out *.result","title":"Submit the jobs"},{"location":"materials/data/part1-ex3-blast-split/#update-the-resource-requests","text":"After the job finishes successfully, examine the log file for memory and disk usage, and update the requests in the submit file. In Exercise 2.1 you'll submit many jobs at once and use a different method for handling the pdbaa_files.tar.gz file, which is a bit too large to use regular file transfer when submitting many jobs.","title":"Update the resource requests"},{"location":"materials/data/part2-ex1-blast-proxy/","text":"Data Exercise 2.1: Using a Web Proxy for Large Shared Input \u00b6 Continuing the series of exercises blasting mouse genetic sequences, the objective of this exercise is to use a web proxy to stage the large database, which will be downloaded into each of many jobs that use the split input files from the last exercise (Exercise 1.3). Setup \u00b6 Make sure you are logged into login04.osgconnect.net Make sure you are in the same directory as the previous exercise, Exercise 1.3 directory named blast-split . Place the Large File on the Proxy \u00b6 First, you'll need to put the pdbaa_files.tar.gz file onto the Stash web directory. Use the following command: user@login04 $ cp pdbaa_files.tar.gz /public/<USERNAME> Replacing <USERNAME> with your username Test a download of the file \u00b6 Once the file is placed in your /public directory, it can be downloaded from a corresponding URL such as http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz , where <USERNAME> is your username on login04.osgconnect.net . Using the above convention (and from a different directory on login04.osgconnect.net , any directory), you can test the download of your pdbaa_files.tar.gz file with a command like the following: user@login04 $ wget http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz Again, replacing <USERNAME> with your own username. You may realize that you've been using wget to download files from a web proxy for many of the previous exercises at the school! Run a New Test Job \u00b6 Now, you'll repeat the last exercise (with a single input query file) but have HTCondor download the pdbaa_files.tar.gz file from the web proxy, instead of having the file transferred from the submit server. Modify the submit file and wrapper script \u00b6 In the wrapper script, we have to add some special lines so that we can pull from the HTTP proxy and see where they are coming from. Normally, we would let HTCondor do the HTTP transfer, but we want to see the download and to see where it came from. In blast_wrapper.sh , we will have to add commands to pull the data file: #!/bin/bash # Set the http_proxy environment which wget uses export http_proxy = $OSG_SQUID_LOCATION # Copy the pdbaa_files.tar.gz to the worker node # Add the -S argument, so we can see if it was a cache HIT or MISS wget -S http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz tar xvzf pdbaa_files.tar.gz ./blastx -db pdbaa -query $1 -out $1 .result rm pdbaa* Be sure to replace <USERNAME> with your own username. The new line will download the pdbaa_files.tar.gz from the HTTP proxy, using the closest cache (because wget will look at the environment variable http_proxy for the newest cache). Also notice the final line of the wrapper script has been modified to delete the pdbaa data files as well as the pdbaa_files.tar.gz file so that it will not be transferred back to the submit server when the job finishes. In your submit file, you will need to remove the pdbaa_files.tar.gz file from the transfer_input_files , because we are now transferring the tarball via the wget command in our wrapper script. Submit the test job \u00b6 You may wish to first remove the log, result, output, and error files from the previous tests, which will be overwritten when the new test job completes. user@login04 $ rm *.error *.out *.result *.log Submit a single test job! (If your submit file uses a queue .. matching statement, a simple way to submit a single job is to temporarily change it to queue inputfile matching mouse_rna.fa.1 . When the job starts, the wrapper will download the pdbaa_files.tar.gz file from the web proxy. If the jobs takes longer than two minutes, you can assume that it will complete successfully, and then continue with the rest of the exercise. After the job completes examine the error file generated by the submission. At the top of the file, you will find something like: --2021-07-23 10 :35:51-- http://stash.osgconnect.net/public/dweitzel/pdbaa_files.tar.gz Resolving iitgrid.iit.edu ( iitgrid.iit.edu ) ... 216 .47.155.220 Connecting to iitgrid.iit.edu ( iitgrid.iit.edu ) | 216 .47.155.220 | :3128... connected. Proxy request sent, awaiting response... HTTP/1.1 200 OK Server: nginx/1.16.1 Content-Type: application/octet-stream Content-Length: 22105180 Last-Modified: Fri, 23 Jul 2021 14 :27:49 GMT ETag: \"60fad1e5-1514c5c\" Accept-Ranges: bytes Age: 0 Date: Fri, 23 Jul 2021 15 :35:51 GMT X-Cache: HIT from iitgrid.iit.edu Via: 1 .1 iitgrid.iit.edu ( squid/frontier-squid-4.10-2.1 ) Connection: close Length: 22105180 ( 21M ) [ application/octet-stream ] Saving to: 'pdbaa_files.tar.gz.1' ... Notice the X-Cache line. It says it was a cache HIT from the proxy iitgrid.iit.edu . Yay! You successfully used a proxy to cache data near your worker node! Notice, the name of the cache may be different, and it may be a MISS the first time you request the data from the cache. Get HTCondor to do the wget for you! \u00b6 The transfer_input_files command in the submit file can also be an HTTP address. Instead of using wget in your blast_wrapper.sh file, remove it and add the HTTP address to the transfer_input_files line in your blast_split.sub executable = blast_wrapper.sh transfer_input_files = blastx, $(inputfile), http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz output = $(inputfile).out ... Run all 100 Jobs! \u00b6 If all of the previous tests have gone okay, you can prepare to run all 100 jobs that will use the split input files. To make sure you're not going to generate too much data, use the size of files from the previous test to calculate how much total data you're going to add to the blast-split directory for 100 jobs. Make sure you remove pdbaa_files.tar.gz from the transfer_input_files in the split submit file. Submit all 100 jobs! They may take a while to all complete, but it will still be faster than the many hours it would have taken to blast the single, large mouse_rna.fa file without splitting it up. In the meantime, as long as the first several jobs are running for longer than two minutes, you can move on to the next exercise","title":"Exercise 2.1"},{"location":"materials/data/part2-ex1-blast-proxy/#data-exercise-21-using-a-web-proxy-for-large-shared-input","text":"Continuing the series of exercises blasting mouse genetic sequences, the objective of this exercise is to use a web proxy to stage the large database, which will be downloaded into each of many jobs that use the split input files from the last exercise (Exercise 1.3).","title":"Data Exercise 2.1: Using a Web Proxy for Large Shared Input"},{"location":"materials/data/part2-ex1-blast-proxy/#setup","text":"Make sure you are logged into login04.osgconnect.net Make sure you are in the same directory as the previous exercise, Exercise 1.3 directory named blast-split .","title":"Setup"},{"location":"materials/data/part2-ex1-blast-proxy/#place-the-large-file-on-the-proxy","text":"First, you'll need to put the pdbaa_files.tar.gz file onto the Stash web directory. Use the following command: user@login04 $ cp pdbaa_files.tar.gz /public/<USERNAME> Replacing <USERNAME> with your username","title":"Place the Large File on the Proxy"},{"location":"materials/data/part2-ex1-blast-proxy/#test-a-download-of-the-file","text":"Once the file is placed in your /public directory, it can be downloaded from a corresponding URL such as http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz , where <USERNAME> is your username on login04.osgconnect.net . Using the above convention (and from a different directory on login04.osgconnect.net , any directory), you can test the download of your pdbaa_files.tar.gz file with a command like the following: user@login04 $ wget http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz Again, replacing <USERNAME> with your own username. You may realize that you've been using wget to download files from a web proxy for many of the previous exercises at the school!","title":"Test a download of the file"},{"location":"materials/data/part2-ex1-blast-proxy/#run-a-new-test-job","text":"Now, you'll repeat the last exercise (with a single input query file) but have HTCondor download the pdbaa_files.tar.gz file from the web proxy, instead of having the file transferred from the submit server.","title":"Run a New Test Job"},{"location":"materials/data/part2-ex1-blast-proxy/#modify-the-submit-file-and-wrapper-script","text":"In the wrapper script, we have to add some special lines so that we can pull from the HTTP proxy and see where they are coming from. Normally, we would let HTCondor do the HTTP transfer, but we want to see the download and to see where it came from. In blast_wrapper.sh , we will have to add commands to pull the data file: #!/bin/bash # Set the http_proxy environment which wget uses export http_proxy = $OSG_SQUID_LOCATION # Copy the pdbaa_files.tar.gz to the worker node # Add the -S argument, so we can see if it was a cache HIT or MISS wget -S http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz tar xvzf pdbaa_files.tar.gz ./blastx -db pdbaa -query $1 -out $1 .result rm pdbaa* Be sure to replace <USERNAME> with your own username. The new line will download the pdbaa_files.tar.gz from the HTTP proxy, using the closest cache (because wget will look at the environment variable http_proxy for the newest cache). Also notice the final line of the wrapper script has been modified to delete the pdbaa data files as well as the pdbaa_files.tar.gz file so that it will not be transferred back to the submit server when the job finishes. In your submit file, you will need to remove the pdbaa_files.tar.gz file from the transfer_input_files , because we are now transferring the tarball via the wget command in our wrapper script.","title":"Modify the submit file and wrapper script"},{"location":"materials/data/part2-ex1-blast-proxy/#submit-the-test-job","text":"You may wish to first remove the log, result, output, and error files from the previous tests, which will be overwritten when the new test job completes. user@login04 $ rm *.error *.out *.result *.log Submit a single test job! (If your submit file uses a queue .. matching statement, a simple way to submit a single job is to temporarily change it to queue inputfile matching mouse_rna.fa.1 . When the job starts, the wrapper will download the pdbaa_files.tar.gz file from the web proxy. If the jobs takes longer than two minutes, you can assume that it will complete successfully, and then continue with the rest of the exercise. After the job completes examine the error file generated by the submission. At the top of the file, you will find something like: --2021-07-23 10 :35:51-- http://stash.osgconnect.net/public/dweitzel/pdbaa_files.tar.gz Resolving iitgrid.iit.edu ( iitgrid.iit.edu ) ... 216 .47.155.220 Connecting to iitgrid.iit.edu ( iitgrid.iit.edu ) | 216 .47.155.220 | :3128... connected. Proxy request sent, awaiting response... HTTP/1.1 200 OK Server: nginx/1.16.1 Content-Type: application/octet-stream Content-Length: 22105180 Last-Modified: Fri, 23 Jul 2021 14 :27:49 GMT ETag: \"60fad1e5-1514c5c\" Accept-Ranges: bytes Age: 0 Date: Fri, 23 Jul 2021 15 :35:51 GMT X-Cache: HIT from iitgrid.iit.edu Via: 1 .1 iitgrid.iit.edu ( squid/frontier-squid-4.10-2.1 ) Connection: close Length: 22105180 ( 21M ) [ application/octet-stream ] Saving to: 'pdbaa_files.tar.gz.1' ... Notice the X-Cache line. It says it was a cache HIT from the proxy iitgrid.iit.edu . Yay! You successfully used a proxy to cache data near your worker node! Notice, the name of the cache may be different, and it may be a MISS the first time you request the data from the cache.","title":"Submit the test job"},{"location":"materials/data/part2-ex1-blast-proxy/#get-htcondor-to-do-the-wget-for-you","text":"The transfer_input_files command in the submit file can also be an HTTP address. Instead of using wget in your blast_wrapper.sh file, remove it and add the HTTP address to the transfer_input_files line in your blast_split.sub executable = blast_wrapper.sh transfer_input_files = blastx, $(inputfile), http://stash.osgconnect.net/public/<USERNAME>/pdbaa_files.tar.gz output = $(inputfile).out ...","title":"Get HTCondor to do the wget for you!"},{"location":"materials/data/part2-ex1-blast-proxy/#run-all-100-jobs","text":"If all of the previous tests have gone okay, you can prepare to run all 100 jobs that will use the split input files. To make sure you're not going to generate too much data, use the size of files from the previous test to calculate how much total data you're going to add to the blast-split directory for 100 jobs. Make sure you remove pdbaa_files.tar.gz from the transfer_input_files in the split submit file. Submit all 100 jobs! They may take a while to all complete, but it will still be faster than the many hours it would have taken to blast the single, large mouse_rna.fa file without splitting it up. In the meantime, as long as the first several jobs are running for longer than two minutes, you can move on to the next exercise","title":"Run all 100 Jobs!"},{"location":"materials/data/part2-ex2-stash-shared/","text":"Data Exercise 2.2: Using Stash for Large Shared Data \u00b6 This exercise will use a BLAST workflow to demonstrate the functionality of StashCache for transferring input files to jobs on OSG. Because our individual blast jobs from Exercise 2.1 would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our pdbaa_files.tar.gz file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from Exercise 2.1, but instead of using the web proxy for the pdbaa database, we will place it in Stash via the OSG Connect server. Stash is connected to a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed. We will be using the command stashcp to copy files from Stash to the execute hosts. It has a cp like syntax. Setup \u00b6 Make sure you're logged in to login04.osgconnect.net Copy the following files from the previous Exercise 2.1 to a new directory in /home/<username> called stash-shared : blast_wrapper.sh blastx mouse_rna.fa.1 mouse_rna.fa.2 mouse_rna.fa.3 Your most recent submit file (probably named blast_split.sub ) Place the Database in Stash \u00b6 Copy to your public space on OSG Connect \u00b6 StashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your BLAST database into this public directory. You have already placed files in your /public directory in the previous exercise in order for it to be accessible to the HTTP proxies. The same directory is accessible to StashCache. As the public directory name indicates, your files placed in the public directory will be accessible to anyone's jobs if they know how to use stashcp , though no one else will be able to edit the files, since only you can place or change files in your public space. For your own work in the future, make sure that you never put any sensitive data in such locations. Check the file on OSG Connect \u00b6 Next, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node: user@login04 $ ls /public/<USERNAME> Now, load the stashcache module, which will allow you to test a copy of the file from StashCache into your /home directory on login04.osgconnect.net : user@login04 $ module load stashcache user@login04 $ stashcp /osgconnect/public/<USERNAME>/pdbaa_files.tar.gz ./ Replacing all instances of <USERNAME> with your username on login04.osgconnect.net . You should now see the pdbaa_files.tar.gz file in your current directory. Notice that we had to include the /osgconnect/public and your username in the file path for stashcp , which make sure you're copying from your public space. Modify the Submit File and Wrapper \u00b6 You will have to modify the wrapper and submit files to use Stash: At the top of the wrapper script (after #!/bin/bash ), add the lines that load the stashcache module and to copy the pdbaa_files.tar.gz file into the current directory of the job: module load stashcache stashcp /osgconnect/public/<USERNAME>/pdbaa_files.tar.gz ./ Replacing <USERNAME> with your own username. Since HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing rm command, if you're confident) from your wrapper script to make sure the pdbaa_files.tar.gz file is also deleted and not copied back as perceived output. rm pdbaa_files.tar.gz Delete the wget and export http_proxy lines from the job wrapper script blast_wrapper.sh Add the following line to the submit file and update the \"requirements\" statement to require servers with OSG Connect modules (for accessing the stashcp module), somewhere before the word queue . +WantsStashCache = true requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= true) Remove the HTTP address in the transfer_input_files . Confirm that your queue statement is correct for the current directory. It should be something like: queue inputfile matching mouse_rna.fa.* And that mouse_rna.fa.* files exist in the current directory (you should have copied a few them from the previous exercise directory). Submit the Job \u00b6 Now submit and monitor the job! If your 100 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise! Note: Keeping Stash 'Clean' \u00b6 Just as for data on a web proxy, it is VERY important to remove old files from Stash when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from public on login04.osgconnect.net when you don't need them there anymore, but only after all jobs have finished. The next time you use Stash after the school, remember to first check for old files that you can delete. Next exercise \u00b6 Once completed, move onto the next exercise: Using Stash for unique large input","title":"Exercise 2.2"},{"location":"materials/data/part2-ex2-stash-shared/#data-exercise-22-using-stash-for-large-shared-data","text":"This exercise will use a BLAST workflow to demonstrate the functionality of StashCache for transferring input files to jobs on OSG. Because our individual blast jobs from Exercise 2.1 would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our pdbaa_files.tar.gz file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from Exercise 2.1, but instead of using the web proxy for the pdbaa database, we will place it in Stash via the OSG Connect server. Stash is connected to a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed. We will be using the command stashcp to copy files from Stash to the execute hosts. It has a cp like syntax.","title":"Data Exercise 2.2: Using Stash for Large Shared Data"},{"location":"materials/data/part2-ex2-stash-shared/#setup","text":"Make sure you're logged in to login04.osgconnect.net Copy the following files from the previous Exercise 2.1 to a new directory in /home/<username> called stash-shared : blast_wrapper.sh blastx mouse_rna.fa.1 mouse_rna.fa.2 mouse_rna.fa.3 Your most recent submit file (probably named blast_split.sub )","title":"Setup"},{"location":"materials/data/part2-ex2-stash-shared/#place-the-database-in-stash","text":"","title":"Place the Database in Stash"},{"location":"materials/data/part2-ex2-stash-shared/#copy-to-your-public-space-on-osg-connect","text":"StashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your BLAST database into this public directory. You have already placed files in your /public directory in the previous exercise in order for it to be accessible to the HTTP proxies. The same directory is accessible to StashCache. As the public directory name indicates, your files placed in the public directory will be accessible to anyone's jobs if they know how to use stashcp , though no one else will be able to edit the files, since only you can place or change files in your public space. For your own work in the future, make sure that you never put any sensitive data in such locations.","title":"Copy to your public space on OSG Connect"},{"location":"materials/data/part2-ex2-stash-shared/#check-the-file-on-osg-connect","text":"Next, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node: user@login04 $ ls /public/<USERNAME> Now, load the stashcache module, which will allow you to test a copy of the file from StashCache into your /home directory on login04.osgconnect.net : user@login04 $ module load stashcache user@login04 $ stashcp /osgconnect/public/<USERNAME>/pdbaa_files.tar.gz ./ Replacing all instances of <USERNAME> with your username on login04.osgconnect.net . You should now see the pdbaa_files.tar.gz file in your current directory. Notice that we had to include the /osgconnect/public and your username in the file path for stashcp , which make sure you're copying from your public space.","title":"Check the file on OSG Connect"},{"location":"materials/data/part2-ex2-stash-shared/#modify-the-submit-file-and-wrapper","text":"You will have to modify the wrapper and submit files to use Stash: At the top of the wrapper script (after #!/bin/bash ), add the lines that load the stashcache module and to copy the pdbaa_files.tar.gz file into the current directory of the job: module load stashcache stashcp /osgconnect/public/<USERNAME>/pdbaa_files.tar.gz ./ Replacing <USERNAME> with your own username. Since HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing rm command, if you're confident) from your wrapper script to make sure the pdbaa_files.tar.gz file is also deleted and not copied back as perceived output. rm pdbaa_files.tar.gz Delete the wget and export http_proxy lines from the job wrapper script blast_wrapper.sh Add the following line to the submit file and update the \"requirements\" statement to require servers with OSG Connect modules (for accessing the stashcp module), somewhere before the word queue . +WantsStashCache = true requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= true) Remove the HTTP address in the transfer_input_files . Confirm that your queue statement is correct for the current directory. It should be something like: queue inputfile matching mouse_rna.fa.* And that mouse_rna.fa.* files exist in the current directory (you should have copied a few them from the previous exercise directory).","title":"Modify the Submit File and Wrapper"},{"location":"materials/data/part2-ex2-stash-shared/#submit-the-job","text":"Now submit and monitor the job! If your 100 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise!","title":"Submit the Job"},{"location":"materials/data/part2-ex2-stash-shared/#note-keeping-stash-clean","text":"Just as for data on a web proxy, it is VERY important to remove old files from Stash when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from public on login04.osgconnect.net when you don't need them there anymore, but only after all jobs have finished. The next time you use Stash after the school, remember to first check for old files that you can delete.","title":"Note: Keeping Stash 'Clean'"},{"location":"materials/data/part2-ex2-stash-shared/#next-exercise","text":"Once completed, move onto the next exercise: Using Stash for unique large input","title":"Next exercise"},{"location":"materials/data/part2-ex3-stash-unique/","text":"Data Exercise 2.3: Using Stash for unique large input \u00b6 In this exercise, we will run a multimedia program that converts and manipulates video files. In particular, we want to convert large .mov files to smaller (10-100s of MB) mp4 files. Just like the Blast database in the previous exercise , these video files are too large to send to jobs using HTCondor's default file transfer mechanism, so we'll be using the Stash tool to send our data to jobs. This exercise should take 25-30 minutes. Data \u00b6 A copy of the movie files for this exercise have been placed in /public/osgvs21 , so that they'll be available to our jobs when they run out on OSG. Log into login04.osgconnect.net Create a directory for this exercise named stash-unique and change into it. We're going to need a list of these files later. Below is the final list of movie files. Because of the size, you do not need to download the files to your /public directory, and instead use the copies in the stash directory. For now, let's save a list of the videos to a file in this directory. Save it as movie_list.txt : ducks.mov teaching.mov test_open_terminal.mov Software \u00b6 We'll be using a multi-purpose media tool called ffmpeg to convert video formats. The basic command to convert a file looks like this: user@login04 $ ./ffmpeg -i input.mov output.mp4 In order to resize our files, we're going to manually set the video bitrate and resize the frames, so that the resulting file is smaller. user@login04 $ ./ffmpeg -i input.mp4 -b:v 400k -s 640x360 output.mp4 To get the ffmpeg binary do the following: We'll be downloading the ffmpeg pre-built static binary originally from this page: http://johnvansickle.com/ffmpeg/ . user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/ffmpeg-release-64bit-static.tar.xz Once the binary is downloaded, un-tar it, and then copy the main ffmpeg program into your current directory: user@login04 $ tar -xf ffmpeg-release-64bit-static.tar.xz user@login04 $ cp ffmpeg-4.0.1-64bit-static/ffmpeg ./ Script \u00b6 We want to write a script that runs on the worker node that uses ffmpeg to convert a .mov file to a smaller format. Our script will need to: Copy that movie file from Stash to the job's current working directory (as in the previous exercise Run the appropriate ffmpeg command Remove the original movie file so that it doesn't get transferred back to the submit server. This last step is particularly important, as otherwise you will have large files transferring into the submit server and filling up your local scratch directory space. Create a file called run_ffmpeg.sh , that does the steps described above. Use the name of the smallest .mov file in the ffmpeg command. An example of that script is below: #!/bin/bash module load stashcache stashcp /osgconnect/public/osgvs21/test_open_terminal.mov ./ ./ffmpeg -i test_open_terminal.mov -b:v 400k -s 640x360 test_open_terminal.mp4 rm test_open_terminal.mov Ultimately we'll want to submit several jobs (one for each .mov file), but to start with, we'll run one job to make sure that everything works. Submit File \u00b6 Create a submit file for this job, based on other submit files from the school ( This file, for example .) Things to consider: We'll be copying the video file into the job's working directory from StashCache, so make sure to request enough disk space for the input mov file and the output mp4 file. If you're aren't sure how much to request, ask a helper. Important Don't list the name of the .mov in transfer_input_files . Our job will be interacting with the input .mov files solely from within the script we wrote above. Note that we do need to transfer the ffmpeg program that we downloaded above. transfer_input_files = ffmpeg Add the same requirements as the previous exercise: +WantsStashCache = true requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= true) Initial Job \u00b6 With everything in place, submit the job. Once it finishes, we should check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and did not transfer the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then we can go ahead and convert all of the files we uploaded to Stash. How to view your videos Remember how we used the stash web server when we used it to distribute our blast database? You can use that web server to view the video files. Just copy the .mp4 video into your /public/<USERNAME> directory. The file will be available at http://stash.osgconnect.net/public/<USERNAME>/<MP4_NAME>.mp4 . Multiple jobs \u00b6 We wrote the name of the .mov file into our run_ffmpeg.sh executable script. To submit a set of jobs for all of our .mov files, what will we need to change in: The script? The submit file? Once you've thought about it, check your reasoning against the instructions below. Add an argument to your script \u00b6 Look at your run_ffmpeg.sh script. What values will change for every job? The input file will change with every job - and don't forget that the output file will too! Let's make them both into arguments. To add arguments to a bash script, we use the notation $1 for the first argument (our input file) and $2 for the second argument (our output file name). The final script should look like this: #!/bin/bash module load stashcache stashcp /osgconnect/public/osgvs21/$1 ./ ./ffmpeg -i $1 -b:v 400k -s 640x360 $2 rm $1 Note that we use the input file name multiple times in our script, so we'll have to use $1 multiple times as well. Modify your submit file \u00b6 We now need to tell each job what arguments to use. We will do this by adding an arguments line to our submit file. Because we'll only have the input file name, the \"output\" file name will be the input file name with the mp4 extension. That should look like this: arguments = $(mov) $(mov).mp4 To set these arguments, we will use the queue .. from syntax that we learned in the HTC Exercise 2.3 . In our submit file, we can then change our queue statement to: queue mov from movie_list.txt Once you've made these changes, try submitting all the jobs! Bonus \u00b6 If you wanted to set a different output file name, bitrate and/or size for each original movie, how could you modify: movie_list.txt Your submit file run_ffmpeg.sh to do so? Show hint Here's the changes you can make to the various files: movie_list.txt ducks.mov ducks.mp4 500k 1280x720 teaching.mov teaching.mp4 400k 320x180 test_open_terminal.mov terminal.mp4 600k 640x360 Submit file arguments = $(mov) $(mp4) $(bitrate) $(size) queue mov,mp4,bitrate,size from movie_list.txt run_ffmpeg.sh 1 2 3 4 5 6 #!/bin/bash module load stashcache stashcp /osgconnect/public/osgvsp20/ $1 ./ ./ffmpeg -i $1 -b:v $3 -s $4 $2 rm $1","title":"Exercise 2.3"},{"location":"materials/data/part2-ex3-stash-unique/#data-exercise-23-using-stash-for-unique-large-input","text":"In this exercise, we will run a multimedia program that converts and manipulates video files. In particular, we want to convert large .mov files to smaller (10-100s of MB) mp4 files. Just like the Blast database in the previous exercise , these video files are too large to send to jobs using HTCondor's default file transfer mechanism, so we'll be using the Stash tool to send our data to jobs. This exercise should take 25-30 minutes.","title":"Data Exercise 2.3: Using Stash for unique large input"},{"location":"materials/data/part2-ex3-stash-unique/#data","text":"A copy of the movie files for this exercise have been placed in /public/osgvs21 , so that they'll be available to our jobs when they run out on OSG. Log into login04.osgconnect.net Create a directory for this exercise named stash-unique and change into it. We're going to need a list of these files later. Below is the final list of movie files. Because of the size, you do not need to download the files to your /public directory, and instead use the copies in the stash directory. For now, let's save a list of the videos to a file in this directory. Save it as movie_list.txt : ducks.mov teaching.mov test_open_terminal.mov","title":"Data"},{"location":"materials/data/part2-ex3-stash-unique/#software","text":"We'll be using a multi-purpose media tool called ffmpeg to convert video formats. The basic command to convert a file looks like this: user@login04 $ ./ffmpeg -i input.mov output.mp4 In order to resize our files, we're going to manually set the video bitrate and resize the frames, so that the resulting file is smaller. user@login04 $ ./ffmpeg -i input.mp4 -b:v 400k -s 640x360 output.mp4 To get the ffmpeg binary do the following: We'll be downloading the ffmpeg pre-built static binary originally from this page: http://johnvansickle.com/ffmpeg/ . user@login04 $ wget http://stash.osgconnect.net/public/osgvs21/ffmpeg-release-64bit-static.tar.xz Once the binary is downloaded, un-tar it, and then copy the main ffmpeg program into your current directory: user@login04 $ tar -xf ffmpeg-release-64bit-static.tar.xz user@login04 $ cp ffmpeg-4.0.1-64bit-static/ffmpeg ./","title":"Software"},{"location":"materials/data/part2-ex3-stash-unique/#script","text":"We want to write a script that runs on the worker node that uses ffmpeg to convert a .mov file to a smaller format. Our script will need to: Copy that movie file from Stash to the job's current working directory (as in the previous exercise Run the appropriate ffmpeg command Remove the original movie file so that it doesn't get transferred back to the submit server. This last step is particularly important, as otherwise you will have large files transferring into the submit server and filling up your local scratch directory space. Create a file called run_ffmpeg.sh , that does the steps described above. Use the name of the smallest .mov file in the ffmpeg command. An example of that script is below: #!/bin/bash module load stashcache stashcp /osgconnect/public/osgvs21/test_open_terminal.mov ./ ./ffmpeg -i test_open_terminal.mov -b:v 400k -s 640x360 test_open_terminal.mp4 rm test_open_terminal.mov Ultimately we'll want to submit several jobs (one for each .mov file), but to start with, we'll run one job to make sure that everything works.","title":"Script"},{"location":"materials/data/part2-ex3-stash-unique/#submit-file","text":"Create a submit file for this job, based on other submit files from the school ( This file, for example .) Things to consider: We'll be copying the video file into the job's working directory from StashCache, so make sure to request enough disk space for the input mov file and the output mp4 file. If you're aren't sure how much to request, ask a helper. Important Don't list the name of the .mov in transfer_input_files . Our job will be interacting with the input .mov files solely from within the script we wrote above. Note that we do need to transfer the ffmpeg program that we downloaded above. transfer_input_files = ffmpeg Add the same requirements as the previous exercise: +WantsStashCache = true requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= true)","title":"Submit File"},{"location":"materials/data/part2-ex3-stash-unique/#initial-job","text":"With everything in place, submit the job. Once it finishes, we should check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and did not transfer the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then we can go ahead and convert all of the files we uploaded to Stash. How to view your videos Remember how we used the stash web server when we used it to distribute our blast database? You can use that web server to view the video files. Just copy the .mp4 video into your /public/<USERNAME> directory. The file will be available at http://stash.osgconnect.net/public/<USERNAME>/<MP4_NAME>.mp4 .","title":"Initial Job"},{"location":"materials/data/part2-ex3-stash-unique/#multiple-jobs","text":"We wrote the name of the .mov file into our run_ffmpeg.sh executable script. To submit a set of jobs for all of our .mov files, what will we need to change in: The script? The submit file? Once you've thought about it, check your reasoning against the instructions below.","title":"Multiple jobs"},{"location":"materials/data/part2-ex3-stash-unique/#add-an-argument-to-your-script","text":"Look at your run_ffmpeg.sh script. What values will change for every job? The input file will change with every job - and don't forget that the output file will too! Let's make them both into arguments. To add arguments to a bash script, we use the notation $1 for the first argument (our input file) and $2 for the second argument (our output file name). The final script should look like this: #!/bin/bash module load stashcache stashcp /osgconnect/public/osgvs21/$1 ./ ./ffmpeg -i $1 -b:v 400k -s 640x360 $2 rm $1 Note that we use the input file name multiple times in our script, so we'll have to use $1 multiple times as well.","title":"Add an argument to your script"},{"location":"materials/data/part2-ex3-stash-unique/#modify-your-submit-file","text":"We now need to tell each job what arguments to use. We will do this by adding an arguments line to our submit file. Because we'll only have the input file name, the \"output\" file name will be the input file name with the mp4 extension. That should look like this: arguments = $(mov) $(mov).mp4 To set these arguments, we will use the queue .. from syntax that we learned in the HTC Exercise 2.3 . In our submit file, we can then change our queue statement to: queue mov from movie_list.txt Once you've made these changes, try submitting all the jobs!","title":"Modify your submit file"},{"location":"materials/data/part2-ex3-stash-unique/#bonus","text":"If you wanted to set a different output file name, bitrate and/or size for each original movie, how could you modify: movie_list.txt Your submit file run_ffmpeg.sh to do so? Show hint Here's the changes you can make to the various files: movie_list.txt ducks.mov ducks.mp4 500k 1280x720 teaching.mov teaching.mp4 400k 320x180 test_open_terminal.mov terminal.mp4 600k 640x360 Submit file arguments = $(mov) $(mp4) $(bitrate) $(size) queue mov,mp4,bitrate,size from movie_list.txt run_ffmpeg.sh 1 2 3 4 5 6 #!/bin/bash module load stashcache stashcp /osgconnect/public/osgvsp20/ $1 ./ ./ffmpeg -i $1 -b:v $3 -s $4 $2 rm $1","title":"Bonus"},{"location":"materials/data/part3-ex1-input/","text":"Bonus Data Exercise 3.1: Large Input Data \u00b6 In this exercise, we will do a similar version of the previous exercise . This exercise should take 10-15 minutes. Background \u00b6 In the previous exercises, we used two \"web-based\" tools to stage and deliver our files to jobs: the web proxy and Stash . Another alternative for handling large files (both input and output), especially if they are unique to each job, is a local shared filesystem. This is a filesystem that all (or most) of the execute servers can access, so data stored there can be copied to the job from that system instead of as a transfer or download. For this example, we'll be submitting the same jobs as the previous exercise , but we will stage our data in a shared filesystem local to CHTC. The name of our shared filesystem is Staging and user directories are found as sub-directories of the path /staging . This is just one example of what it can look like to use a shared filesystem. If you are running jobs at your own institution, the shared filesystem and how to access it may be different. Accessing the Filesystem \u00b6 Running on learn.chtc.wisc.edu For these next 2 exercises, we will be using learn.chtc.wisc.edu . Because our shared filesystem is only available on the local CHTC HTCondor pool, you'll need to log into our local submit server, learn.chtc.wisc.edu . Once you've logged in, navigate to your Staging directory. It should be at the location /staging/<USERNAME> , where <USERNAME> is your username on learn.chtc.wisc.edu . Previous Files \u00b6 Data \u00b6 Like the previous example, we'll start by downloading our source movie files into the Staging directory. Run this command in your Staging directory , /staging/<USERNAME> . user@learn $ wget http://stash.osgconnect.net/public/osgvsp20/videos.tar.gz While the files are copying, feel free to open a second connection to learn.chtc.wisc.edu and follow the instructions below. Once the files have finished downloading, untar them. Software, Executable, Submit File \u00b6 Because these jobs will be similar to the previous exercise, we can copy the software ( ffmpeg ), our executable ( run_ffmpeg.sh ) and submit file from login04.osgconnect.net to learn.chtc.wisc.edu , or, feel free to replicate these by following the instructions in the previous exercise . These files should go into a sub-directory of your home directory, not your Staging directory . Ch-ch-ch-changes \u00b6 What changes will we need to make to our previous job submission in order to submit it in CHTC, using the Staging location? Read on. Script \u00b6 The major actions of our script will be the same: Copy the movie file to the job's current working directory, Run the appropriate ffmpeg command Remove the original movie file. The main difference is that the mov file will be copied from your Staging directory instead of being downloaded from Stash. Like before, your script should remove that file before the job completes so that it doesn't get transferred back to the submit server. Remove the lines in the run_ffmpeg.sh that mention module load . Remove the stashcp line Change the first command of your run_ffmpeg.sh script to only copy one .mov file: cp /staging/<USERNAME>/test_open_terminal.mov ./ You should use your username on learn.chtc.wisc.edu in the path above. If you have a version of the script that uses arguments instead of the filenames, that's okay. Submit File \u00b6 Remove any previous requirements and add a line to the file (before the final queue statement) that ensures your job will land on computers that have access to Staging: requirements = (Target.HasCHTCStaging == true) Initial Job \u00b6 As before, we should test our job submission with a single mov file before submitting jobs for all three. Alter your submit file (if necessary) to run a job that converts the test_open_terminal.mov file. Once the job finishes, check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and not the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then our script did what it should have. Multiple jobs \u00b6 Change your submit file as in the previous exercise in order to submit 3 jobs to convert all three files!","title":"Bonus Exercise 3.1"},{"location":"materials/data/part3-ex1-input/#bonus-data-exercise-31-large-input-data","text":"In this exercise, we will do a similar version of the previous exercise . This exercise should take 10-15 minutes.","title":"Bonus Data Exercise 3.1: Large Input Data"},{"location":"materials/data/part3-ex1-input/#background","text":"In the previous exercises, we used two \"web-based\" tools to stage and deliver our files to jobs: the web proxy and Stash . Another alternative for handling large files (both input and output), especially if they are unique to each job, is a local shared filesystem. This is a filesystem that all (or most) of the execute servers can access, so data stored there can be copied to the job from that system instead of as a transfer or download. For this example, we'll be submitting the same jobs as the previous exercise , but we will stage our data in a shared filesystem local to CHTC. The name of our shared filesystem is Staging and user directories are found as sub-directories of the path /staging . This is just one example of what it can look like to use a shared filesystem. If you are running jobs at your own institution, the shared filesystem and how to access it may be different.","title":"Background"},{"location":"materials/data/part3-ex1-input/#accessing-the-filesystem","text":"Running on learn.chtc.wisc.edu For these next 2 exercises, we will be using learn.chtc.wisc.edu . Because our shared filesystem is only available on the local CHTC HTCondor pool, you'll need to log into our local submit server, learn.chtc.wisc.edu . Once you've logged in, navigate to your Staging directory. It should be at the location /staging/<USERNAME> , where <USERNAME> is your username on learn.chtc.wisc.edu .","title":"Accessing the Filesystem"},{"location":"materials/data/part3-ex1-input/#previous-files","text":"","title":"Previous Files"},{"location":"materials/data/part3-ex1-input/#data","text":"Like the previous example, we'll start by downloading our source movie files into the Staging directory. Run this command in your Staging directory , /staging/<USERNAME> . user@learn $ wget http://stash.osgconnect.net/public/osgvsp20/videos.tar.gz While the files are copying, feel free to open a second connection to learn.chtc.wisc.edu and follow the instructions below. Once the files have finished downloading, untar them.","title":"Data"},{"location":"materials/data/part3-ex1-input/#software-executable-submit-file","text":"Because these jobs will be similar to the previous exercise, we can copy the software ( ffmpeg ), our executable ( run_ffmpeg.sh ) and submit file from login04.osgconnect.net to learn.chtc.wisc.edu , or, feel free to replicate these by following the instructions in the previous exercise . These files should go into a sub-directory of your home directory, not your Staging directory .","title":"Software, Executable, Submit File"},{"location":"materials/data/part3-ex1-input/#ch-ch-ch-changes","text":"What changes will we need to make to our previous job submission in order to submit it in CHTC, using the Staging location? Read on.","title":"Ch-ch-ch-changes"},{"location":"materials/data/part3-ex1-input/#script","text":"The major actions of our script will be the same: Copy the movie file to the job's current working directory, Run the appropriate ffmpeg command Remove the original movie file. The main difference is that the mov file will be copied from your Staging directory instead of being downloaded from Stash. Like before, your script should remove that file before the job completes so that it doesn't get transferred back to the submit server. Remove the lines in the run_ffmpeg.sh that mention module load . Remove the stashcp line Change the first command of your run_ffmpeg.sh script to only copy one .mov file: cp /staging/<USERNAME>/test_open_terminal.mov ./ You should use your username on learn.chtc.wisc.edu in the path above. If you have a version of the script that uses arguments instead of the filenames, that's okay.","title":"Script"},{"location":"materials/data/part3-ex1-input/#submit-file","text":"Remove any previous requirements and add a line to the file (before the final queue statement) that ensures your job will land on computers that have access to Staging: requirements = (Target.HasCHTCStaging == true)","title":"Submit File"},{"location":"materials/data/part3-ex1-input/#initial-job","text":"As before, we should test our job submission with a single mov file before submitting jobs for all three. Alter your submit file (if necessary) to run a job that converts the test_open_terminal.mov file. Once the job finishes, check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and not the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then our script did what it should have.","title":"Initial Job"},{"location":"materials/data/part3-ex1-input/#multiple-jobs","text":"Change your submit file as in the previous exercise in order to submit 3 jobs to convert all three files!","title":"Multiple jobs"},{"location":"materials/data/part3-ex2-output/","text":"Bonus Data Exercise 3.2: Large Output Data \u00b6 In this exercise, we will run a job that produces a very large output file, based on a few parameters. This exercise should take 15-20 minutes. Background \u00b6 This exercise will be the reverse of the previous exercise! Instead of large input/small output, we will be using a program that has no input except for a few arguments on the command line, but produces a file that is several GB in size. As before, we will need to write a shell script that runs the program and handles the data. The Program \u00b6 If you haven't already, log in to learn.chtc.wisc.edu . Download the software package and untar it. user@learn $ wget http://stash.osgconnect.net/public/osgvsp20/motif-flanks.tar.gz user@learn $ tar -xzf motif-flanks.tar.gz Use the cd command to enter the unpacked motif-flanks directory. Take a look at the README file and then do the following: Compile the code. Run the program without any arguments. Based on the README , what is the largest amount of data we might expect? This program generates all permutations of nucleotide sequences surrounding a given DNA motif. We can choose the length of permutation we want both before and after a motif of our choice. To use this program on the command line and save the output to a FASTA file, we can use the command: user@learn $ ./motif-flanks 2 AGTTCATGCCT 2 > sequences.fa According to the usage information and README, the two numerical arguments can add up to 13, at most, and the middle sequence can be any DNA sequence up to 20 characters. The largest output we can expect is around 4 GB. Test Job \u00b6 Having output of up to 4 GB means two things: we will want to run a smaller test before we run the program at its peak, and the output data will need to go into a shared location like Staging, instead of returning to the submit server. First, we'll create a shell script to serve as the job's executable. What commands do you need to put in the script? What do you need to do with the sequences.fa file before the job exits? Our script needs to run the motif-flanks command as shown above, redirecting the output to a file called sequences.fa . Then, after that command completes, the sequences.fa file should be moved to your Staging directory, as it is too large to return to the submit server as usual. Write the script and then check it against the script below. Yours might look slightly different. #!/bin/sh ./motif-flanks 4 GATTTTCGATC 4 > sequences.fa mv sequences.fa /staging/<USERNAME> Replacing <USERNAME> with your own username. Note Note that the two arguments in the script (4 and 4) are much smaller than the total possible for the software (two values that add up to 13). This is because we want to run a smaller test before submitting a job with the largest possible combination of arguments. Next, create a submit file for this job, based on other submit files from the school. Some important considerations: We're writing our file to the job's working directory, so make sure to request several GB of disk space. Add a line to the file that ensures your job will land on computers that have access to Staging (see the file from the last exercise ). The executable will be the script you wrote above. Once you have a submit file that does all these things, submit the test job. Once the job has completed, do the following: Check the directory where you submitted the job. Has the sequences.fa file returned there, accidentally? Check your Staging directory. Did the sequences.fa file get copied there successfully? Check file size. How big is the sequences.fa file? You can use the ls -lh command with the filename to find out. If your job successfully copied the sequences.fa file to Staging and did not return it to your submission directory on the submit server, congratulations! Everything is working as it should and you can now submit a full job. Final Job \u00b6 Having done a test, it should be straightforward to run a \"full scale\" job. Edit your run_motif.sh executable so that the motif-flanks command uses larger numerical arguments: ./motif-flanks 6 GATTTTCGATC 7 > sequences.fa Then submit your job. When it completes, check the size of the output file in Staging.","title":"Bonus Exercise 3.2"},{"location":"materials/data/part3-ex2-output/#bonus-data-exercise-32-large-output-data","text":"In this exercise, we will run a job that produces a very large output file, based on a few parameters. This exercise should take 15-20 minutes.","title":"Bonus Data Exercise 3.2: Large Output Data"},{"location":"materials/data/part3-ex2-output/#background","text":"This exercise will be the reverse of the previous exercise! Instead of large input/small output, we will be using a program that has no input except for a few arguments on the command line, but produces a file that is several GB in size. As before, we will need to write a shell script that runs the program and handles the data.","title":"Background"},{"location":"materials/data/part3-ex2-output/#the-program","text":"If you haven't already, log in to learn.chtc.wisc.edu . Download the software package and untar it. user@learn $ wget http://stash.osgconnect.net/public/osgvsp20/motif-flanks.tar.gz user@learn $ tar -xzf motif-flanks.tar.gz Use the cd command to enter the unpacked motif-flanks directory. Take a look at the README file and then do the following: Compile the code. Run the program without any arguments. Based on the README , what is the largest amount of data we might expect? This program generates all permutations of nucleotide sequences surrounding a given DNA motif. We can choose the length of permutation we want both before and after a motif of our choice. To use this program on the command line and save the output to a FASTA file, we can use the command: user@learn $ ./motif-flanks 2 AGTTCATGCCT 2 > sequences.fa According to the usage information and README, the two numerical arguments can add up to 13, at most, and the middle sequence can be any DNA sequence up to 20 characters. The largest output we can expect is around 4 GB.","title":"The Program"},{"location":"materials/data/part3-ex2-output/#test-job","text":"Having output of up to 4 GB means two things: we will want to run a smaller test before we run the program at its peak, and the output data will need to go into a shared location like Staging, instead of returning to the submit server. First, we'll create a shell script to serve as the job's executable. What commands do you need to put in the script? What do you need to do with the sequences.fa file before the job exits? Our script needs to run the motif-flanks command as shown above, redirecting the output to a file called sequences.fa . Then, after that command completes, the sequences.fa file should be moved to your Staging directory, as it is too large to return to the submit server as usual. Write the script and then check it against the script below. Yours might look slightly different. #!/bin/sh ./motif-flanks 4 GATTTTCGATC 4 > sequences.fa mv sequences.fa /staging/<USERNAME> Replacing <USERNAME> with your own username. Note Note that the two arguments in the script (4 and 4) are much smaller than the total possible for the software (two values that add up to 13). This is because we want to run a smaller test before submitting a job with the largest possible combination of arguments. Next, create a submit file for this job, based on other submit files from the school. Some important considerations: We're writing our file to the job's working directory, so make sure to request several GB of disk space. Add a line to the file that ensures your job will land on computers that have access to Staging (see the file from the last exercise ). The executable will be the script you wrote above. Once you have a submit file that does all these things, submit the test job. Once the job has completed, do the following: Check the directory where you submitted the job. Has the sequences.fa file returned there, accidentally? Check your Staging directory. Did the sequences.fa file get copied there successfully? Check file size. How big is the sequences.fa file? You can use the ls -lh command with the filename to find out. If your job successfully copied the sequences.fa file to Staging and did not return it to your submission directory on the submit server, congratulations! Everything is working as it should and you can now submit a full job.","title":"Test Job"},{"location":"materials/data/part3-ex2-output/#final-job","text":"Having done a test, it should be straightforward to run a \"full scale\" job. Edit your run_motif.sh executable so that the motif-flanks command uses larger numerical arguments: ./motif-flanks 6 GATTTTCGATC 7 > sequences.fa Then submit your job. When it completes, check the size of the output file in Staging.","title":"Final Job"},{"location":"materials/gpus/part1-ex1-containers-overview/","text":"GPU Exercise 1.1: Containers Overview \u00b6 In this tutorial, we explore GPUs and containers on OSG, using the popular Tensorflow sofware package. Tensorflow is a good example here as the software is too complex to bundle up and ship with your job. Containers solve this problem by defining a full OS image, containing not only the complex software package, but dependencies and environment configuration as well. https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. Setup \u00b6 Log in to login04.osgconnect.net Get a copy of the tutorial by running tutorial tensorflow-containers Change into the tutorial with cd tutorial-tensorflow-containers Defining container images \u00b6 Defining containers is fully described in the Docker and Singularity Containers section. Here we will just provide an overview of how you could take something like an existing Tensorflow image provided by OSG staff, and extend it by adding your own modules to it. Let's assume you like Tensorflow version 2.3. The definition of this image can be found in Github: Dockerfile . You don't really need to understand how an image was built in order to use it. As described in the containers documentation, make sure the HTCondor submit file has: Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" If you want to extend an existing image, you can just inherit from the parent image available on DockerHub here . For example, if you just need some additional Python packages, your new Dockerfile could look like: FROM opensciencegrid/tensorflow:2.3 RUN python3 -m pip install some_package_name You can then docker build and docker push it so that your new image is available on DockerHub. Note that OSG does not provide any infrastructure for these steps. You will have to complete them on your own computer or using the DockerHub build infrastructure. Adding a container to the OSG CVMFS distribution mechanism \u00b6 How to add a container image to the OSG CVMFS distribution mechanism is also described in Docker and Singularity Containers , but a quick scan of the cvmfs-singularity-sync and specifically the docker_images.txt file show us that the tensorflow images are listed as: opensciencegrid/tensorflow:* opensciencegrid/tensorflow-gpu:* Those two lines means that all tags from those two DockerHub repositories should be mapped to /cvmfs/singularity.opensciencegrid.org/ . On the login node, try running: ls /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ This is the image in its expanded form - something we can execute with Singularity! Testing the container on the submit host \u00b6 Before submitting jobs to the OSG, it is always a good idea to test your code so that you understand runtime requirements. The containers can be tested on the OSGConnect submit hosts with singularity shell , which will drop you into a container and let you exlore it interactively. To explore the Tensorflow 2.3 image, run: singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ Note how the command line prompt changes, providing you an indicator that you are inside the image. You can exit any time by running exit . Another important thing to note is that your $HOME directory is automatically mounted inside the interactive container - allowing you to access your codes and test it out. First, start with a simple python3 import test to make sure tensorflow is available: $ python3 Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tensorflow 2021-01-15 17:32:33.901607: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:32:33.901735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. >>> Tensorflow will warn you that no GPUs where found. This is expected as we do not have GPUs attached to our login nodes, and it is fine as Tensorflow works fine with regular CPUs (slower of course). Exit out of Python3 with CTRL+D and then we can run a Tensorflow testcode which can be found in this tutorial: $ python3 test.py 2021-01-15 17:37:43.152892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:43.153021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2021-01-15 17:37:44.899967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:44.900063: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303) 2021-01-15 17:37:44.900130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login05.osgconnect.net): /proc/driver/nvidia/version does not exist 2021-01-15 17:37:44.900821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-01-15 17:37:44.912483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz 2021-01-15 17:37:44.915548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-01-15 17:37:44.915645: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-01-15 17:37:44.921895: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) We will again see a bunch of warnings regarding GPUs not being available, but as we can see by the /job:localhost/replica:0/task:0/device:CPU:0 line, the code ran on one of the CPUs. When testing your own code like this, take note of how much memory, disk and runtime is required - it is needed in the next step. Once you are done with testing, use CTRL+D or run exit to exit out of the container. Note that you can not submit jobs from within the container.","title":"Exercise 1.1"},{"location":"materials/gpus/part1-ex1-containers-overview/#gpu-exercise-11-containers-overview","text":"In this tutorial, we explore GPUs and containers on OSG, using the popular Tensorflow sofware package. Tensorflow is a good example here as the software is too complex to bundle up and ship with your job. Containers solve this problem by defining a full OS image, containing not only the complex software package, but dependencies and environment configuration as well. https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.","title":"GPU Exercise 1.1: Containers Overview"},{"location":"materials/gpus/part1-ex1-containers-overview/#setup","text":"Log in to login04.osgconnect.net Get a copy of the tutorial by running tutorial tensorflow-containers Change into the tutorial with cd tutorial-tensorflow-containers","title":"Setup"},{"location":"materials/gpus/part1-ex1-containers-overview/#defining-container-images","text":"Defining containers is fully described in the Docker and Singularity Containers section. Here we will just provide an overview of how you could take something like an existing Tensorflow image provided by OSG staff, and extend it by adding your own modules to it. Let's assume you like Tensorflow version 2.3. The definition of this image can be found in Github: Dockerfile . You don't really need to understand how an image was built in order to use it. As described in the containers documentation, make sure the HTCondor submit file has: Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" If you want to extend an existing image, you can just inherit from the parent image available on DockerHub here . For example, if you just need some additional Python packages, your new Dockerfile could look like: FROM opensciencegrid/tensorflow:2.3 RUN python3 -m pip install some_package_name You can then docker build and docker push it so that your new image is available on DockerHub. Note that OSG does not provide any infrastructure for these steps. You will have to complete them on your own computer or using the DockerHub build infrastructure.","title":"Defining container images"},{"location":"materials/gpus/part1-ex1-containers-overview/#adding-a-container-to-the-osg-cvmfs-distribution-mechanism","text":"How to add a container image to the OSG CVMFS distribution mechanism is also described in Docker and Singularity Containers , but a quick scan of the cvmfs-singularity-sync and specifically the docker_images.txt file show us that the tensorflow images are listed as: opensciencegrid/tensorflow:* opensciencegrid/tensorflow-gpu:* Those two lines means that all tags from those two DockerHub repositories should be mapped to /cvmfs/singularity.opensciencegrid.org/ . On the login node, try running: ls /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ This is the image in its expanded form - something we can execute with Singularity!","title":"Adding a container to the OSG CVMFS distribution mechanism"},{"location":"materials/gpus/part1-ex1-containers-overview/#testing-the-container-on-the-submit-host","text":"Before submitting jobs to the OSG, it is always a good idea to test your code so that you understand runtime requirements. The containers can be tested on the OSGConnect submit hosts with singularity shell , which will drop you into a container and let you exlore it interactively. To explore the Tensorflow 2.3 image, run: singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ Note how the command line prompt changes, providing you an indicator that you are inside the image. You can exit any time by running exit . Another important thing to note is that your $HOME directory is automatically mounted inside the interactive container - allowing you to access your codes and test it out. First, start with a simple python3 import test to make sure tensorflow is available: $ python3 Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tensorflow 2021-01-15 17:32:33.901607: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:32:33.901735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. >>> Tensorflow will warn you that no GPUs where found. This is expected as we do not have GPUs attached to our login nodes, and it is fine as Tensorflow works fine with regular CPUs (slower of course). Exit out of Python3 with CTRL+D and then we can run a Tensorflow testcode which can be found in this tutorial: $ python3 test.py 2021-01-15 17:37:43.152892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:43.153021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2021-01-15 17:37:44.899967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:44.900063: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303) 2021-01-15 17:37:44.900130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login05.osgconnect.net): /proc/driver/nvidia/version does not exist 2021-01-15 17:37:44.900821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-01-15 17:37:44.912483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz 2021-01-15 17:37:44.915548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-01-15 17:37:44.915645: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-01-15 17:37:44.921895: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) We will again see a bunch of warnings regarding GPUs not being available, but as we can see by the /job:localhost/replica:0/task:0/device:CPU:0 line, the code ran on one of the CPUs. When testing your own code like this, take note of how much memory, disk and runtime is required - it is needed in the next step. Once you are done with testing, use CTRL+D or run exit to exit out of the container. Note that you can not submit jobs from within the container.","title":"Testing the container on the submit host"},{"location":"materials/gpus/part1-ex2-cpu-jobs/","text":"GPU Exercise 1.2: Running a CPU job \u00b6 If Tensorflow can run on GPUs, you might be wondering why we might want to run it on slower CPUs? One reason is that CPUs are plentiful while GPUs are still somewhat scarce. If you have a lot of shorter Tensorflow jobs, they might complete faster on available CPUs, rather than wait in the queue for the faster, less available, GPUs. The good news is that Tensorflow code should work in both enviroments automatically, so if your code runs too slow on CPUs, moving to GPUs should be easy. To submit our job, we need a submit file and a job wrapper script. The submit file is a basic OSGConnect flavored HTCondor file, specifying that we want the job to run in a container. cpu-job.submit contains: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 0 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 And job-wrapper.sh: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash set -e echo echo \"I'm running on\" $( hostname -f ) echo \"OSG site: $OSG_SITE_NAME \" echo python3 test.py 2 > & 1 The job can now be submitted with condor_submit cpu-job.submit . Once the job is done, check the files named after the job id for the outputs.","title":"Exercise 1.2"},{"location":"materials/gpus/part1-ex2-cpu-jobs/#gpu-exercise-12-running-a-cpu-job","text":"If Tensorflow can run on GPUs, you might be wondering why we might want to run it on slower CPUs? One reason is that CPUs are plentiful while GPUs are still somewhat scarce. If you have a lot of shorter Tensorflow jobs, they might complete faster on available CPUs, rather than wait in the queue for the faster, less available, GPUs. The good news is that Tensorflow code should work in both enviroments automatically, so if your code runs too slow on CPUs, moving to GPUs should be easy. To submit our job, we need a submit file and a job wrapper script. The submit file is a basic OSGConnect flavored HTCondor file, specifying that we want the job to run in a container. cpu-job.submit contains: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 0 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 And job-wrapper.sh: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash set -e echo echo \"I'm running on\" $( hostname -f ) echo \"OSG site: $OSG_SITE_NAME \" echo python3 test.py 2 > & 1 The job can now be submitted with condor_submit cpu-job.submit . Once the job is done, check the files named after the job id for the outputs.","title":"GPU Exercise 1.2: Running a CPU job"},{"location":"materials/gpus/part1-ex3-gpu-jobs/","text":"GPU Exercise 1.2: Running a GPU job \u00b6 When moving the job to be run on a GPU, all we have to do is update two lines in the submit file: set request_gpus to 1 and specify a GPU enabled container image for +SingularityImage . The updated submit file can be found in gpu-job.submit with the contents: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 1 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 Submit a job with condor_submit gpu-job.submit . Once the job is complete, check the .out file for a line stating the code was run under a GPU. Something similar to: 2021-02-02 23:25:19.022467: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 The GPU:0 parts shows that a GPU was found and used for the computation.","title":"Exercise 1.3"},{"location":"materials/gpus/part1-ex3-gpu-jobs/#gpu-exercise-12-running-a-gpu-job","text":"When moving the job to be run on a GPU, all we have to do is update two lines in the submit file: set request_gpus to 1 and specify a GPU enabled container image for +SingularityImage . The updated submit file can be found in gpu-job.submit with the contents: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 1 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 Submit a job with condor_submit gpu-job.submit . Once the job is complete, check the .out file for a line stating the code was run under a GPU. Something similar to: 2021-02-02 23:25:19.022467: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 The GPU:0 parts shows that a GPU was found and used for the computation.","title":"GPU Exercise 1.2: Running a GPU job"},{"location":"materials/htcondor/part1-ex1-login/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.1: Log In and Look Around \u00b6 The goal of this first exercise is simply to log in to the local submit server and look around a little bit, which will take only a few minutes. If you have trouble getting SSH access to the submit server, ask the instructors right away! Gaining access is critical for all remaining exercises. Logging In \u00b6 Today, you will use a submit server named learn.chtc.wisc.edu , which will allow you to submit jobs to our local HTCondor pool in CHTC. To log in, use a Secure Shell (SSH) client. From a Mac or Linux computer, run the Terminal app and use the ssh command, like so: # Change <USERNAME> to your username username@learn $ ssh <USERNAME>@learn.chtc.wisc.edu On Windows, we recommend a free client called PuTTY , but any SSH client should be fine. If you need help finding or using an SSH client, ask the instructors for help right away ! About Your Password \u00b6 Your mentor should have given you your username and password. If this is not true or you have lost the password, reach out to any staff member for help. While the passwd command will work (and will change your password temporarily), your initial password will be automatically reset for you on an hourly basis. So consider not changing your password and save the one provided in a secure place. Running Commands \u00b6 In the exercises, we will show commands that you are supposed to type or copy into the command line, like this: username@learn $ hostname learn.chtc.wisc.edu Note In the first line of the example above, the username@learn $ part is meant to show the Linux command-line prompt. You do not type this part! Further, your actual prompt probably is a bit different, and that is expected. So in the example above, the command that you type at your own prompt is just the eight characters hostname . The second line of the example, without the prompt, shows the output of the command; you do not type this part, either. Here are a few other commands that you can try (the examples below do not show the output from each command): username@learn $ whoami username@learn $ date username@learn $ uname -a A suggestion for the day: try typing into the command line as many of the commands as you can. Copy-and-paste is fine, of course, but you WILL learn more if you take the time to type each command yourself. Organizing Your Workspace \u00b6 You will be doing many different exercises over the next few days, many of them on this submit server. Each exercise may use many files, once finished. To avoid confusion, it may be useful to create a separate directory for each exercise. For instance, for the rest of this exercise, you may wish to create and use a directory named intro-1.1-login , or something like that. username@learn $ mkdir intro-1.1-login username@learn $ cd intro-1.1-login Showing the Version of HTCondor \u00b6 HTCondor is installed on this server. But what version? You can ask HTCondor itself: username@learn $ condor_version $ CondorVersion: 8 .9.8 Jun 29 2020 BuildID: 508520 PackageID: 8 .9.8-0.508520 $ $ CondorPlatform: x86_64_CentOS7 $ As you can see from the output, we are using HTCondor 8.9.8. FYI: Background information about HTCondor version numbers \u00b6 HTCondor always has two types of releases at one time: stable and development. HTCondor 8.6.x and 8.8.x are considered stable releases, indicated by even-numbered second digits (e.g., 6 or 8 in these cases). Within one stable series, all versions have the same features (for example 8.6.0 and 8.6.8 have the same set of features) and differ only in bug and security fixes. HTCondor 8.9.8 is the latest development release series of HTCondor. You know that these are a development release because the second digit (i.e., 9) is an odd number. CHTC is usually running the latest development series as the local CHTC Pool is somewhat of a final testing ground for new features. Other HTCondor pools and submit servers that you use outside of CHTC (including the OSG submit server you'll use later) may run different versions. In general, the user-facing HTCondor features in 8.6 forward are mostly the same, but you may see some differences in the format of output from condor_ commands or in more advanced or non-user features. Reference Materials \u00b6 Here are a few links to reference materials that might be interesting after the school (or perhaps during). HTCondor home page HTCondor manuals ; it is probably best to read the manual corresponding to the version of HTCondor that you use. That link points to the latest version of the manual, but you can switch versions using the toggle in the lower left corner of that page. Center for High Throughput Computing , our campus research computing center, and home to HTCondor and other development of distributed computing tools.","title":"Exercise 1.1"},{"location":"materials/htcondor/part1-ex1-login/#htc-exercise-11-log-in-and-look-around","text":"The goal of this first exercise is simply to log in to the local submit server and look around a little bit, which will take only a few minutes. If you have trouble getting SSH access to the submit server, ask the instructors right away! Gaining access is critical for all remaining exercises.","title":"HTC Exercise 1.1: Log In and Look Around"},{"location":"materials/htcondor/part1-ex1-login/#logging-in","text":"Today, you will use a submit server named learn.chtc.wisc.edu , which will allow you to submit jobs to our local HTCondor pool in CHTC. To log in, use a Secure Shell (SSH) client. From a Mac or Linux computer, run the Terminal app and use the ssh command, like so: # Change <USERNAME> to your username username@learn $ ssh <USERNAME>@learn.chtc.wisc.edu On Windows, we recommend a free client called PuTTY , but any SSH client should be fine. If you need help finding or using an SSH client, ask the instructors for help right away !","title":"Logging In"},{"location":"materials/htcondor/part1-ex1-login/#about-your-password","text":"Your mentor should have given you your username and password. If this is not true or you have lost the password, reach out to any staff member for help. While the passwd command will work (and will change your password temporarily), your initial password will be automatically reset for you on an hourly basis. So consider not changing your password and save the one provided in a secure place.","title":"About Your Password"},{"location":"materials/htcondor/part1-ex1-login/#running-commands","text":"In the exercises, we will show commands that you are supposed to type or copy into the command line, like this: username@learn $ hostname learn.chtc.wisc.edu Note In the first line of the example above, the username@learn $ part is meant to show the Linux command-line prompt. You do not type this part! Further, your actual prompt probably is a bit different, and that is expected. So in the example above, the command that you type at your own prompt is just the eight characters hostname . The second line of the example, without the prompt, shows the output of the command; you do not type this part, either. Here are a few other commands that you can try (the examples below do not show the output from each command): username@learn $ whoami username@learn $ date username@learn $ uname -a A suggestion for the day: try typing into the command line as many of the commands as you can. Copy-and-paste is fine, of course, but you WILL learn more if you take the time to type each command yourself.","title":"Running Commands"},{"location":"materials/htcondor/part1-ex1-login/#organizing-your-workspace","text":"You will be doing many different exercises over the next few days, many of them on this submit server. Each exercise may use many files, once finished. To avoid confusion, it may be useful to create a separate directory for each exercise. For instance, for the rest of this exercise, you may wish to create and use a directory named intro-1.1-login , or something like that. username@learn $ mkdir intro-1.1-login username@learn $ cd intro-1.1-login","title":"Organizing Your Workspace"},{"location":"materials/htcondor/part1-ex1-login/#showing-the-version-of-htcondor","text":"HTCondor is installed on this server. But what version? You can ask HTCondor itself: username@learn $ condor_version $ CondorVersion: 8 .9.8 Jun 29 2020 BuildID: 508520 PackageID: 8 .9.8-0.508520 $ $ CondorPlatform: x86_64_CentOS7 $ As you can see from the output, we are using HTCondor 8.9.8.","title":"Showing the Version of HTCondor"},{"location":"materials/htcondor/part1-ex1-login/#fyi-background-information-about-htcondor-version-numbers","text":"HTCondor always has two types of releases at one time: stable and development. HTCondor 8.6.x and 8.8.x are considered stable releases, indicated by even-numbered second digits (e.g., 6 or 8 in these cases). Within one stable series, all versions have the same features (for example 8.6.0 and 8.6.8 have the same set of features) and differ only in bug and security fixes. HTCondor 8.9.8 is the latest development release series of HTCondor. You know that these are a development release because the second digit (i.e., 9) is an odd number. CHTC is usually running the latest development series as the local CHTC Pool is somewhat of a final testing ground for new features. Other HTCondor pools and submit servers that you use outside of CHTC (including the OSG submit server you'll use later) may run different versions. In general, the user-facing HTCondor features in 8.6 forward are mostly the same, but you may see some differences in the format of output from condor_ commands or in more advanced or non-user features.","title":"FYI: Background information about HTCondor version numbers"},{"location":"materials/htcondor/part1-ex1-login/#reference-materials","text":"Here are a few links to reference materials that might be interesting after the school (or perhaps during). HTCondor home page HTCondor manuals ; it is probably best to read the manual corresponding to the version of HTCondor that you use. That link points to the latest version of the manual, but you can switch versions using the toggle in the lower left corner of that page. Center for High Throughput Computing , our campus research computing center, and home to HTCondor and other development of distributed computing tools.","title":"Reference Materials"},{"location":"materials/htcondor/part1-ex2-commands/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.2: Experiment With HTCondor Commands \u00b6 The goal of this exercise is to learn about two foundational HTCondor commands, condor_q and condor_status . They will be useful for monitoring your jobs and available slots (respectively) throughout the week. This exercise should take only a few minutes. Viewing Slots \u00b6 As discussed in the lecture, the condor_status command is used to view the current state of slots in an HTCondor pool. At its most basic, the command is very straightforward: username@learn $ condor_status This command, running on our (CHTC) pool, will produce a lot of output; there is one line per slot, and we typically have over 10,000 slots. TIP: You can widen your terminal window, which may help you to see all details of the output better. Here is some example output (what you see will be longer): slot1_31@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+01:14:34 slot1_32@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+03:57:00 slot1_33@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 1+00:05:17 slot1@e438.chtc.wisc.edu LINUX X86_64 Owner Idle 0.300 250 7+03:22:21 slot1_1@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 0.930 1024 0+02:42:08 slot1_2@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 3.530 1024 0+02:40:24 This output consists of 8 columns: Col Example Meaning Name slot1_1@e438.chtc.wisc.edu Full slot name (including the hostname) OpSys LINUX Operating system Arch X86_64 Slot architecture (e.g., Intel 64 bit) State Claimed State of the slot ( Unclaimed is available, Owner is being used by the machine owner, Claimed is matched to a job) Activity Busy Is there activity on the slot? LoadAv 0.930 Load average, a measure of CPU activity on the slot Mem 1024 Memory available to the slot, in MB ActvtyTime 0+02:42:08 Amount of time spent in current activity (days + hours:minutes:seconds) At the end of the slot listing, there is a summary. Here is an example: Machines Owner Claimed Unclaimed Matched Preempting Drain X86_64/LINUX 10831 0 10194 631 0 0 6 X86_64/WINDOWS 2 2 0 0 0 0 0 Total 10833 2 10194 631 0 0 6 There is one row of summary for each machine (i.e. \"slot\") architecture/operating system combination with columns for the number of slots in each state. The final row gives a summary of slot states for the whole pool. Questions: \u00b6 When you run condor_status , how many 64-bit Linux slots are available? (Hint: Unclaimed = available.) What percent of the total slots are currently claimed by a job? (Note: there is a rapid turnover of slots, which is what allows users with new submission to have jobs start quickly.) How have these numbers changed (if at all) when you run the command again? Viewing Whole Machines, Only \u00b6 Also try out the -compact for a slightly different view of whole machines (i.e. server hostnames), without the individual slots shown. username@learn $ condor_status -compact How has the column information changed? Viewing Jobs \u00b6 The condor_q command lists jobs that are on this submit machine and that are running or waiting to run. The _q part of the name is meant to suggest the word \u201cqueue\u201d, or list of job sets waiting to finish. Viewing Your Own Jobs \u00b6 The default behavior of the command lists only your jobs: username@learn $ condor_q The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one set (\"batch\") of submitted jobs per line. If you had a single job in the queue, it would look something like the below: -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 09:59:31 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS aapohl CMD: run_ffmpeg.sh 7/12 09:58 _ _ 1 1 18801.0 This output consists of 8 (or 9) columns: Col Example Meaning OWNER aapohl The user ID of the user who submitted the job BATCH_NAME run_ffmpeg.sh The executable or \"jobbatchname\" specified within the submit file(s) SUBMITTED 7/12 09:58 The date and time when the job was submitted DONE _ Number of jobs in this batch that have completed RUN _ Number of jobs in this batch that are currently running IDLE 1 Number of jobs in this batch that are idle, waiting for a match HOLD _ Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user TOTAL 1 Total number of jobs in this batch JOB_IDS 18801.0 Job ID or range of Job IDs in this batch At the end of the job listing, there is a summary. Here is a sample: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended It shows total counts of jobs in the different possible states. Questions: For the sample above, when was the job submitted? For the sample above, was the job running or not yet? How can you tell? Viewing Everyone\u2019s Jobs \u00b6 By default, the condor_q command shows your jobs only. To see everyone\u2019s jobs that are queued on the machine, add the -all option: username@learn $ condor_q -all How many jobs are queued in total (i.e., running or waiting to run)? How many jobs from this submit machine are running right now? Viewing Jobs without the Default \"batch\" Mode \u00b6 The condor_q output, by default, groups \"batches\" of jobs together (if they were submitted with the same submit file or \"jobbatchname\"). To see more information for EVERY job on a separate line of output, use the -nobatch option to condor_q : username@learn $ condor_q -all -nobatch How has the column information changed? (Below is an example of the top of the output.) -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 11:58:44 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 18203.0 s16_alirezakho 7/11 09:51 0+00:00:00 I 0 0.7 pascal 18204.0 s16_alirezakho 7/11 09:51 0+00:00:00 I 0 0.7 pascal 18801.0 aapohl 7/12 09:58 0+00:00:00 I 0 0.0 run_ffmpeg.sh 18997.0 s16_martincum 7/12 10:59 0+00:00:32 I 0 733.0 runR.pl 1_0 run_perm.R 1 0 10 19027.5 s16_martincum 7/12 11:06 0+00:09:20 I 0 2198.0 runR.pl 1_5 run_perm.R 1 5 1000 The -nobatch output shows a line for every job and consists of 8 columns: Col Example Meaning ID 18801.0 Job ID, which is the cluster , a dot character ( . ), and the process OWNER aapohl The user ID of the user who submitted the job SUBMITTED 7/12 09:58 The date and time when the job was submitted RUN_TIME 0+00:00:00 Total time spent running so far (days + hours:minutes:seconds) ST I Status of job: I is Idle (waiting to run), R is Running, H is Held, etc. PRI 0 Job priority (see next lecture) SIZE 0.0 Current run-time memory usage, in MB CMD run_ffmpeg.sh The executable command (with arguments) to be run In future exercises, you'll want to switch between condor_q and condor_q -nobatch to see different types of information about YOUR jobs. Extra Information \u00b6 Both condor_status and condor_q have many command-line options, some of which significantly change their output. You will explore a few of the most useful options in future exercises, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands: Use the (brief) built-in help for the commands, e.g.: condor_q -h Read the installed man(ual) pages for the commands, e.g.: man condor_q Find the command in the online manual ; note: the text online is the same as the man text, only formatted for the web","title":"Exercise 1.2"},{"location":"materials/htcondor/part1-ex2-commands/#htc-exercise-12-experiment-with-htcondor-commands","text":"The goal of this exercise is to learn about two foundational HTCondor commands, condor_q and condor_status . They will be useful for monitoring your jobs and available slots (respectively) throughout the week. This exercise should take only a few minutes.","title":"HTC Exercise 1.2: Experiment With HTCondor Commands"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-slots","text":"As discussed in the lecture, the condor_status command is used to view the current state of slots in an HTCondor pool. At its most basic, the command is very straightforward: username@learn $ condor_status This command, running on our (CHTC) pool, will produce a lot of output; there is one line per slot, and we typically have over 10,000 slots. TIP: You can widen your terminal window, which may help you to see all details of the output better. Here is some example output (what you see will be longer): slot1_31@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+01:14:34 slot1_32@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+03:57:00 slot1_33@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 1+00:05:17 slot1@e438.chtc.wisc.edu LINUX X86_64 Owner Idle 0.300 250 7+03:22:21 slot1_1@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 0.930 1024 0+02:42:08 slot1_2@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 3.530 1024 0+02:40:24 This output consists of 8 columns: Col Example Meaning Name slot1_1@e438.chtc.wisc.edu Full slot name (including the hostname) OpSys LINUX Operating system Arch X86_64 Slot architecture (e.g., Intel 64 bit) State Claimed State of the slot ( Unclaimed is available, Owner is being used by the machine owner, Claimed is matched to a job) Activity Busy Is there activity on the slot? LoadAv 0.930 Load average, a measure of CPU activity on the slot Mem 1024 Memory available to the slot, in MB ActvtyTime 0+02:42:08 Amount of time spent in current activity (days + hours:minutes:seconds) At the end of the slot listing, there is a summary. Here is an example: Machines Owner Claimed Unclaimed Matched Preempting Drain X86_64/LINUX 10831 0 10194 631 0 0 6 X86_64/WINDOWS 2 2 0 0 0 0 0 Total 10833 2 10194 631 0 0 6 There is one row of summary for each machine (i.e. \"slot\") architecture/operating system combination with columns for the number of slots in each state. The final row gives a summary of slot states for the whole pool.","title":"Viewing Slots"},{"location":"materials/htcondor/part1-ex2-commands/#questions","text":"When you run condor_status , how many 64-bit Linux slots are available? (Hint: Unclaimed = available.) What percent of the total slots are currently claimed by a job? (Note: there is a rapid turnover of slots, which is what allows users with new submission to have jobs start quickly.) How have these numbers changed (if at all) when you run the command again?","title":"Questions:"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-whole-machines-only","text":"Also try out the -compact for a slightly different view of whole machines (i.e. server hostnames), without the individual slots shown. username@learn $ condor_status -compact How has the column information changed?","title":"Viewing Whole Machines, Only"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-jobs","text":"The condor_q command lists jobs that are on this submit machine and that are running or waiting to run. The _q part of the name is meant to suggest the word \u201cqueue\u201d, or list of job sets waiting to finish.","title":"Viewing Jobs"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-your-own-jobs","text":"The default behavior of the command lists only your jobs: username@learn $ condor_q The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one set (\"batch\") of submitted jobs per line. If you had a single job in the queue, it would look something like the below: -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 09:59:31 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS aapohl CMD: run_ffmpeg.sh 7/12 09:58 _ _ 1 1 18801.0 This output consists of 8 (or 9) columns: Col Example Meaning OWNER aapohl The user ID of the user who submitted the job BATCH_NAME run_ffmpeg.sh The executable or \"jobbatchname\" specified within the submit file(s) SUBMITTED 7/12 09:58 The date and time when the job was submitted DONE _ Number of jobs in this batch that have completed RUN _ Number of jobs in this batch that are currently running IDLE 1 Number of jobs in this batch that are idle, waiting for a match HOLD _ Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user TOTAL 1 Total number of jobs in this batch JOB_IDS 18801.0 Job ID or range of Job IDs in this batch At the end of the job listing, there is a summary. Here is a sample: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended It shows total counts of jobs in the different possible states. Questions: For the sample above, when was the job submitted? For the sample above, was the job running or not yet? How can you tell?","title":"Viewing Your Own Jobs"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-everyones-jobs","text":"By default, the condor_q command shows your jobs only. To see everyone\u2019s jobs that are queued on the machine, add the -all option: username@learn $ condor_q -all How many jobs are queued in total (i.e., running or waiting to run)? How many jobs from this submit machine are running right now?","title":"Viewing Everyone\u2019s Jobs"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-jobs-without-the-default-batch-mode","text":"The condor_q output, by default, groups \"batches\" of jobs together (if they were submitted with the same submit file or \"jobbatchname\"). To see more information for EVERY job on a separate line of output, use the -nobatch option to condor_q : username@learn $ condor_q -all -nobatch How has the column information changed? (Below is an example of the top of the output.) -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/12/19 11:58:44 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 18203.0 s16_alirezakho 7/11 09:51 0+00:00:00 I 0 0.7 pascal 18204.0 s16_alirezakho 7/11 09:51 0+00:00:00 I 0 0.7 pascal 18801.0 aapohl 7/12 09:58 0+00:00:00 I 0 0.0 run_ffmpeg.sh 18997.0 s16_martincum 7/12 10:59 0+00:00:32 I 0 733.0 runR.pl 1_0 run_perm.R 1 0 10 19027.5 s16_martincum 7/12 11:06 0+00:09:20 I 0 2198.0 runR.pl 1_5 run_perm.R 1 5 1000 The -nobatch output shows a line for every job and consists of 8 columns: Col Example Meaning ID 18801.0 Job ID, which is the cluster , a dot character ( . ), and the process OWNER aapohl The user ID of the user who submitted the job SUBMITTED 7/12 09:58 The date and time when the job was submitted RUN_TIME 0+00:00:00 Total time spent running so far (days + hours:minutes:seconds) ST I Status of job: I is Idle (waiting to run), R is Running, H is Held, etc. PRI 0 Job priority (see next lecture) SIZE 0.0 Current run-time memory usage, in MB CMD run_ffmpeg.sh The executable command (with arguments) to be run In future exercises, you'll want to switch between condor_q and condor_q -nobatch to see different types of information about YOUR jobs.","title":"Viewing Jobs without the Default \"batch\" Mode"},{"location":"materials/htcondor/part1-ex2-commands/#extra-information","text":"Both condor_status and condor_q have many command-line options, some of which significantly change their output. You will explore a few of the most useful options in future exercises, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands: Use the (brief) built-in help for the commands, e.g.: condor_q -h Read the installed man(ual) pages for the commands, e.g.: man condor_q Find the command in the online manual ; note: the text online is the same as the man text, only formatted for the web","title":"Extra Information"},{"location":"materials/htcondor/part1-ex3-jobs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.3: Run Jobs! \u00b6 The goal of this exercise is to submit jobs to HTCondor and have them run on the local pool (CHTC). This is a huge step in learning to use an HTC system! This exercise will take longer than the first two, short ones. It is the essential part of this exercise time. If you are having any problems getting the jobs to run, please ask the instructors! It is very important that you know how to run jobs. Running Your First Job \u00b6 Nearly all of the time, when you want to run an HTCondor job, you first write an HTCondor submit file for it. In this section, you will run the same hostname command as in Exercise 1.1, but where this command will run within a job on one of the 'execute' servers in CHTC's local HTCondor pool. Here is a straightforward submit file for the hostname command: executable = /bin/hostname output = hostname.out error = hostname.err log = hostname.log request_cpus = 1 request_memory = 1GB request_disk = 1MB queue Write those lines of text in a file named hostname.sub . Note There is nothing magic about the name of an HTCondor submit file. It can be any filename you want. It's a good practice to always include the .sub extension, but it is not required. Ultimately, a submit file is a text file The lines of the submit file have the following meanings: executable The name of the program to run (relative to the directory from which you submit). output The filename where HTCondor will write the standard output from your job. error The filename where HTCondor will write the standard error from your job. This particular job is not likely to have any, but it is best to include this line for every job. log The filename where HTCondor will write information about your job run. While not required, it is a really good idea to have a log file for every job. request_* Tells HTCondor how many cpus and how much memory and disk we want, which is not much, because the 'hostname' executable is very small. queue Tells HTCondor to run your job with the settings above. Note that we are not using the arguments or transfer_input_files lines that were mentioned during lecture because the hostname program is all that needs to be transferred from the submit server, and we want to run it without any additional options. Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your job: username@learn $ condor_submit hostname.sub Submitting job(s). 1 job(s) submitted to cluster NNNN. The actual cluster number will be shown instead of NNNN . If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file or ask for help. Notice that condor_submit returns back to the shell prompt right away. It does not wait for your job to run. Instead, as soon as it has finished submitting your job into the queue, the submit command finishes. View your job in the queue \u00b6 Now, use condor_q and condor_q -nobatch to watch for your job in the queue! You may not even catch the job in the R running state, because the hostname command runs very quickly. When the job itself is finished, it will 'leave' the queue and no longer be listed in the condor_q output. After the job finishes, check for the hostname output in hostname.out , which is where job information printed to the terminal screen will be printed for the job. username@learn $ cat hostname.out e171.chtc.wisc.edu The hostname.err file should be empty, unless there were issues running the hostname executable after it was transferred to the slot. The hostname.log is more complex and will be the focus of a later exercise. Running a Job With Arguments \u00b6 Very often, when you run a command on the command line, it includes arguments (i.e. options) after the program name, as in the below examples: username@learn $ cat hostname.out username@learn $ sleep 60 username@learn $ dc -e '6 7 * p' In an HTCondor submit file, the program (or 'executable') name goes in the executable statement and all remaining arguments go into an arguments statement. For example, if the full command is: username@learn $ sleep 60 Then in the submit file, we would put the location of the \"sleep\" program (you can find it with which sleep ) as the job executable , and 60 as the job arguments : executable = /bin/sleep arguments = 60 For the command-line command: username@learn $ dc -e '6 7 * p' We would put the following into the submit file, putting the arguments statement in quotes, since it contains single quotes: executable = /usr/bin/dc arguments = \"-e '6 7 * p'\" Let\u2019s try a job submission with arguments. We will use the sleep command shown above, which simply does nothing for the specified number of seconds, then exits normally. It is convenient for simulating a job that takes a while to run. Create a new submit file (you name it this time) and save the following text in it. executable = /bin/sleep arguments = 60 output = sleep.out error = sleep.err log = sleep.log request_cpus = 1 request_memory = 1GB request_disk = 1MB queue Except for changing a few filenames, this submit file is nearly identical to the last one. But, see the extra arguments line? Submit this new job. Again, watch for it to run using condor_q and condor_q -nobatch ; check once every 15 seconds or so. Once the job starts running, it will take about 1 minute to run (because of the sleep command, right?), so you should be able to see it running for a bit. When the job finishes, it will disappear from the queue, but there will be no output in the output or error files, because sleep does not produce any output. Running a Script Job From the Submit Directory \u00b6 So far, we have been running programs (executables) that come with the standard Linux system. More frequently, you will want to run a program that exists within your directory or perhaps a shell script of commands that you'd like to run within a job. In this example, you will write a shell script and a submit file that runs the shell script within a job: Put the following contents into a file named test-script.sh : #!/bin/sh echo 'Date: ' ` date ` echo 'Host: ' ` hostname ` echo 'System: ' ` uname -spo ` echo \"Program: $0 \" echo \"Args: $* \" echo 'ls: ' ` ls ` # END Add executable permissions to the file (so that it can be run as a program): username@learn $ chmod +x test-script.sh Test your script from the command line: username@learn $ ./test-script.sh hello 42 Date: Mon Jul 17 10:02:20 CDT 2017 Host: learn.chtc.wisc.edu System: Linux x86_64 GNU/Linux Program: ./test-script.sh Args: hello 42 ls: hostname.sub montage hostname.err hostname.log hostname.out test-script.sh This step is really important! If you cannot run your executable from the command-line, HTCondor probably cannot run it on another machine, either. Further, debugging problems like this one is surprisingly difficult. So, if possible, test your executable and arguments as a command at the command-line first. Write the submit file (this should be getting easier by now): executable = test-script.sh arguments = foo bar baz output = script.out error = script.err log = script.log request_cpus = 1 request_memory = 1GB request_disk = 1MB queue In this example, the executable that was named in the submit file did not start with a / , so the location of the file is relative to the submit directory itself. In other words, in this format the executable must be in the same directory as the submit file. Note Blank lines between commands and spaces around the = do not matter to HTCondor. For example, this submit file is equivalent to the one above: executable = test-script.sh arguments = foo bar baz output = script.out error = script.err log = script.log request_cpus=1 request_memory=1GB request_disk=1MB queue Use whitespace to make things clear to you . What format do you prefer to read? Submit the job, wait for it to finish, and check the output (and error, which should be empty). What do you notice about the lines returned for \"Program\" and \"ls\"? Remember that only files pertaining to this job will be in the job working directory on the execute server. You're also seeing the effects of HTCondor's need to standardize some filenames when running your job, though they are named as you expect in the submission directory (per the submit file contents). Extra Challenge \u00b6 Note There are Extra Challenges throughout the school curriculum. You may be better off coming back to these after you've completed all other exercises for your current working session. Below is a Python script that does something similar to the shell script above. Run this Python script using HTCondor. #!/usr/bin/env python \"\"\"Extra Challenge for OSG User School Written by Tim Cartwright Submitted to CHTC by #YOUR_NAME# \"\"\" import getpass import os import platform import socket import sys import time arguments = None if len ( sys . argv ) > 1 : arguments = '\"' + ' ' . join ( sys . argv [ 1 :]) + '\"' print >> sys . stderr , __doc__ print 'Time :' , time . strftime ( '%Y-%m- %d ( %a ) %H:%M:%S %Z' ) print 'Host :' , getpass . getuser (), '@' , socket . gethostname () uname = platform . uname () print \"System :\" , uname [ 0 ], uname [ 2 ], uname [ 4 ] print \"Version :\" , platform . python_version () print \"Program :\" , sys . executable print 'Script :' , os . path . abspath ( __file__ ) print 'Args :' , arguments","title":"Exercise 1.3"},{"location":"materials/htcondor/part1-ex3-jobs/#htc-exercise-13-run-jobs","text":"The goal of this exercise is to submit jobs to HTCondor and have them run on the local pool (CHTC). This is a huge step in learning to use an HTC system! This exercise will take longer than the first two, short ones. It is the essential part of this exercise time. If you are having any problems getting the jobs to run, please ask the instructors! It is very important that you know how to run jobs.","title":"HTC Exercise 1.3: Run Jobs!"},{"location":"materials/htcondor/part1-ex3-jobs/#running-your-first-job","text":"Nearly all of the time, when you want to run an HTCondor job, you first write an HTCondor submit file for it. In this section, you will run the same hostname command as in Exercise 1.1, but where this command will run within a job on one of the 'execute' servers in CHTC's local HTCondor pool. Here is a straightforward submit file for the hostname command: executable = /bin/hostname output = hostname.out error = hostname.err log = hostname.log request_cpus = 1 request_memory = 1GB request_disk = 1MB queue Write those lines of text in a file named hostname.sub . Note There is nothing magic about the name of an HTCondor submit file. It can be any filename you want. It's a good practice to always include the .sub extension, but it is not required. Ultimately, a submit file is a text file The lines of the submit file have the following meanings: executable The name of the program to run (relative to the directory from which you submit). output The filename where HTCondor will write the standard output from your job. error The filename where HTCondor will write the standard error from your job. This particular job is not likely to have any, but it is best to include this line for every job. log The filename where HTCondor will write information about your job run. While not required, it is a really good idea to have a log file for every job. request_* Tells HTCondor how many cpus and how much memory and disk we want, which is not much, because the 'hostname' executable is very small. queue Tells HTCondor to run your job with the settings above. Note that we are not using the arguments or transfer_input_files lines that were mentioned during lecture because the hostname program is all that needs to be transferred from the submit server, and we want to run it without any additional options. Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your job: username@learn $ condor_submit hostname.sub Submitting job(s). 1 job(s) submitted to cluster NNNN. The actual cluster number will be shown instead of NNNN . If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file or ask for help. Notice that condor_submit returns back to the shell prompt right away. It does not wait for your job to run. Instead, as soon as it has finished submitting your job into the queue, the submit command finishes.","title":"Running Your First Job"},{"location":"materials/htcondor/part1-ex3-jobs/#view-your-job-in-the-queue","text":"Now, use condor_q and condor_q -nobatch to watch for your job in the queue! You may not even catch the job in the R running state, because the hostname command runs very quickly. When the job itself is finished, it will 'leave' the queue and no longer be listed in the condor_q output. After the job finishes, check for the hostname output in hostname.out , which is where job information printed to the terminal screen will be printed for the job. username@learn $ cat hostname.out e171.chtc.wisc.edu The hostname.err file should be empty, unless there were issues running the hostname executable after it was transferred to the slot. The hostname.log is more complex and will be the focus of a later exercise.","title":"View your job in the queue"},{"location":"materials/htcondor/part1-ex3-jobs/#running-a-job-with-arguments","text":"Very often, when you run a command on the command line, it includes arguments (i.e. options) after the program name, as in the below examples: username@learn $ cat hostname.out username@learn $ sleep 60 username@learn $ dc -e '6 7 * p' In an HTCondor submit file, the program (or 'executable') name goes in the executable statement and all remaining arguments go into an arguments statement. For example, if the full command is: username@learn $ sleep 60 Then in the submit file, we would put the location of the \"sleep\" program (you can find it with which sleep ) as the job executable , and 60 as the job arguments : executable = /bin/sleep arguments = 60 For the command-line command: username@learn $ dc -e '6 7 * p' We would put the following into the submit file, putting the arguments statement in quotes, since it contains single quotes: executable = /usr/bin/dc arguments = \"-e '6 7 * p'\" Let\u2019s try a job submission with arguments. We will use the sleep command shown above, which simply does nothing for the specified number of seconds, then exits normally. It is convenient for simulating a job that takes a while to run. Create a new submit file (you name it this time) and save the following text in it. executable = /bin/sleep arguments = 60 output = sleep.out error = sleep.err log = sleep.log request_cpus = 1 request_memory = 1GB request_disk = 1MB queue Except for changing a few filenames, this submit file is nearly identical to the last one. But, see the extra arguments line? Submit this new job. Again, watch for it to run using condor_q and condor_q -nobatch ; check once every 15 seconds or so. Once the job starts running, it will take about 1 minute to run (because of the sleep command, right?), so you should be able to see it running for a bit. When the job finishes, it will disappear from the queue, but there will be no output in the output or error files, because sleep does not produce any output.","title":"Running a Job With Arguments"},{"location":"materials/htcondor/part1-ex3-jobs/#running-a-script-job-from-the-submit-directory","text":"So far, we have been running programs (executables) that come with the standard Linux system. More frequently, you will want to run a program that exists within your directory or perhaps a shell script of commands that you'd like to run within a job. In this example, you will write a shell script and a submit file that runs the shell script within a job: Put the following contents into a file named test-script.sh : #!/bin/sh echo 'Date: ' ` date ` echo 'Host: ' ` hostname ` echo 'System: ' ` uname -spo ` echo \"Program: $0 \" echo \"Args: $* \" echo 'ls: ' ` ls ` # END Add executable permissions to the file (so that it can be run as a program): username@learn $ chmod +x test-script.sh Test your script from the command line: username@learn $ ./test-script.sh hello 42 Date: Mon Jul 17 10:02:20 CDT 2017 Host: learn.chtc.wisc.edu System: Linux x86_64 GNU/Linux Program: ./test-script.sh Args: hello 42 ls: hostname.sub montage hostname.err hostname.log hostname.out test-script.sh This step is really important! If you cannot run your executable from the command-line, HTCondor probably cannot run it on another machine, either. Further, debugging problems like this one is surprisingly difficult. So, if possible, test your executable and arguments as a command at the command-line first. Write the submit file (this should be getting easier by now): executable = test-script.sh arguments = foo bar baz output = script.out error = script.err log = script.log request_cpus = 1 request_memory = 1GB request_disk = 1MB queue In this example, the executable that was named in the submit file did not start with a / , so the location of the file is relative to the submit directory itself. In other words, in this format the executable must be in the same directory as the submit file. Note Blank lines between commands and spaces around the = do not matter to HTCondor. For example, this submit file is equivalent to the one above: executable = test-script.sh arguments = foo bar baz output = script.out error = script.err log = script.log request_cpus=1 request_memory=1GB request_disk=1MB queue Use whitespace to make things clear to you . What format do you prefer to read? Submit the job, wait for it to finish, and check the output (and error, which should be empty). What do you notice about the lines returned for \"Program\" and \"ls\"? Remember that only files pertaining to this job will be in the job working directory on the execute server. You're also seeing the effects of HTCondor's need to standardize some filenames when running your job, though they are named as you expect in the submission directory (per the submit file contents).","title":"Running a Script Job From the Submit Directory"},{"location":"materials/htcondor/part1-ex3-jobs/#extra-challenge","text":"Note There are Extra Challenges throughout the school curriculum. You may be better off coming back to these after you've completed all other exercises for your current working session. Below is a Python script that does something similar to the shell script above. Run this Python script using HTCondor. #!/usr/bin/env python \"\"\"Extra Challenge for OSG User School Written by Tim Cartwright Submitted to CHTC by #YOUR_NAME# \"\"\" import getpass import os import platform import socket import sys import time arguments = None if len ( sys . argv ) > 1 : arguments = '\"' + ' ' . join ( sys . argv [ 1 :]) + '\"' print >> sys . stderr , __doc__ print 'Time :' , time . strftime ( '%Y-%m- %d ( %a ) %H:%M:%S %Z' ) print 'Host :' , getpass . getuser (), '@' , socket . gethostname () uname = platform . uname () print \"System :\" , uname [ 0 ], uname [ 2 ], uname [ 4 ] print \"Version :\" , platform . python_version () print \"Program :\" , sys . executable print 'Script :' , os . path . abspath ( __file__ ) print 'Args :' , arguments","title":"Extra Challenge"},{"location":"materials/htcondor/part1-ex4-logs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.4: Read and Interpret Log Files \u00b6 The goal of this exercise is to learn how to understand the contents of a job log file, which is where HTCondor describes the steps taken to run your job. When things go wrong with your job, the log is the best place to look for first pointers (in addition to the .err file). This exercise is short, but you'll want to at least read over it before moving on (and come back later, if you can't run through it now). Reading a Log File \u00b6 For this exercise, we can examine a log file for any previous job that you have run. The example output below is based on the sleep 60 job. A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are all of the event headings from the sleep job log (detailed output in between headings has been omitted here): 000 (5739.000.000) 2020-07-10 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> 040 (5739.000.000) 2020-07-10 10:45:10 Started transferring input files 040 (5739.000.000) 2020-07-10 10:45:10 Finished transferring input files 001 (5739.000.000) 2020-07-10 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> 006 (5739.000.000) 2020-07-10 10:45:20 Image size of job updated: 72 040 (5739.000.000) 2020-07-10 10:45:20 Started transferring output files 040 (5739.000.000) 2020-07-10 10:45:20 Finished transferring output files 006 (5739.000.000) 2020-07-10 10:46:11 Image size of job updated: 4072 005 (5739.000.000) 2020-07-10 10:46:11 Job terminated. There is a lot of extra information in those lines, but you can see: The job ID: cluster 5739, process 0 (written 000 ) The date and local time of each event A brief description of the event: submission, execution, some information updates, and termination Some events provide no information in addition to the heading. For example: 000 (5739.000.000) 2020-07-10 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> ... and 001 (5739.000.000) 2020-07-10 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> ... Note Each event ends with a line that contains only 3 dots: ... But the periodic information update event contains some additional information: 006 (5739.000.000) 2020-07-10 10:45:20 Image size of job updated: 72 1 - MemoryUsage of job (MB) 72 - ResidentSetSize of job (KB) ... These updates record the amount of memory that the job is using on the execute machine. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need. The job termination event includes a great deal of additional information: 005 (5739.000.000) 2020-07-10 10:46:11 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 27848 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 27848 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 40 30 4203309 Memory (MB) : 1 1 1 ... Probably the most interesting information is: The return value ( 0 here, means the executable completed and didn't indicate any internal errors; non-zero usually means failure) The total number of bytes transferred each way, which could be useful if your network is slow The Partitionable Resources table, especially disk and memory usage, which will inform larger submissions. There are many other kinds of events, but the ones above will occur in almost every job log. Understanding When Job Log Events Are Written \u00b6 When are events written to the job log file? Let\u2019s find out. Read through the entire procedure below before starting, because some parts of the process are time sensitive. Change the sleep job submit file, so that the job sleeps for 2 minutes (= 120 seconds) Submit the updated sleep job As soon as the condor_submit command finishes, hit the return key a few times, to create some blank lines Right away, run a command to show the log file and keep showing updates as they occur: username@learn $ tail -f sleep.log Watch the output carefully. When do events appear in the log file? After the termination event appears, press Control-C to end the tail command and return to the shell prompt. Understanding How HTCondor Writes Files \u00b6 When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out! For this exercise, we can use the hostname job from earlier. Edit the hostname submit file so that it uses new and unique filenames for output, error, and log files. Alternatively, delete any existing output, error, and log files from previous runs of the hostname job. Submit the job three separate times in a row (there are better ways to do this, which we will cover in the next lecture) Wait for all three jobs to finish Examine the output file: How many hostnames are there? Did HTCondor erase the previous contents for each job, or add new lines? Examine the log file\u2026 carefully: What happened there? Pay close attention to the times and job IDs of the events. For further clarification about how HTCondor handles these files, reach out to your mentor or one of the other school staff via Slack or email.","title":"Exercise 1.4"},{"location":"materials/htcondor/part1-ex4-logs/#htc-exercise-14-read-and-interpret-log-files","text":"The goal of this exercise is to learn how to understand the contents of a job log file, which is where HTCondor describes the steps taken to run your job. When things go wrong with your job, the log is the best place to look for first pointers (in addition to the .err file). This exercise is short, but you'll want to at least read over it before moving on (and come back later, if you can't run through it now).","title":"HTC Exercise 1.4: Read and Interpret Log Files"},{"location":"materials/htcondor/part1-ex4-logs/#reading-a-log-file","text":"For this exercise, we can examine a log file for any previous job that you have run. The example output below is based on the sleep 60 job. A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are all of the event headings from the sleep job log (detailed output in between headings has been omitted here): 000 (5739.000.000) 2020-07-10 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> 040 (5739.000.000) 2020-07-10 10:45:10 Started transferring input files 040 (5739.000.000) 2020-07-10 10:45:10 Finished transferring input files 001 (5739.000.000) 2020-07-10 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> 006 (5739.000.000) 2020-07-10 10:45:20 Image size of job updated: 72 040 (5739.000.000) 2020-07-10 10:45:20 Started transferring output files 040 (5739.000.000) 2020-07-10 10:45:20 Finished transferring output files 006 (5739.000.000) 2020-07-10 10:46:11 Image size of job updated: 4072 005 (5739.000.000) 2020-07-10 10:46:11 Job terminated. There is a lot of extra information in those lines, but you can see: The job ID: cluster 5739, process 0 (written 000 ) The date and local time of each event A brief description of the event: submission, execution, some information updates, and termination Some events provide no information in addition to the heading. For example: 000 (5739.000.000) 2020-07-10 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> ... and 001 (5739.000.000) 2020-07-10 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> ... Note Each event ends with a line that contains only 3 dots: ... But the periodic information update event contains some additional information: 006 (5739.000.000) 2020-07-10 10:45:20 Image size of job updated: 72 1 - MemoryUsage of job (MB) 72 - ResidentSetSize of job (KB) ... These updates record the amount of memory that the job is using on the execute machine. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need. The job termination event includes a great deal of additional information: 005 (5739.000.000) 2020-07-10 10:46:11 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 27848 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 27848 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 40 30 4203309 Memory (MB) : 1 1 1 ... Probably the most interesting information is: The return value ( 0 here, means the executable completed and didn't indicate any internal errors; non-zero usually means failure) The total number of bytes transferred each way, which could be useful if your network is slow The Partitionable Resources table, especially disk and memory usage, which will inform larger submissions. There are many other kinds of events, but the ones above will occur in almost every job log.","title":"Reading a Log File"},{"location":"materials/htcondor/part1-ex4-logs/#understanding-when-job-log-events-are-written","text":"When are events written to the job log file? Let\u2019s find out. Read through the entire procedure below before starting, because some parts of the process are time sensitive. Change the sleep job submit file, so that the job sleeps for 2 minutes (= 120 seconds) Submit the updated sleep job As soon as the condor_submit command finishes, hit the return key a few times, to create some blank lines Right away, run a command to show the log file and keep showing updates as they occur: username@learn $ tail -f sleep.log Watch the output carefully. When do events appear in the log file? After the termination event appears, press Control-C to end the tail command and return to the shell prompt.","title":"Understanding When Job Log Events Are Written"},{"location":"materials/htcondor/part1-ex4-logs/#understanding-how-htcondor-writes-files","text":"When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out! For this exercise, we can use the hostname job from earlier. Edit the hostname submit file so that it uses new and unique filenames for output, error, and log files. Alternatively, delete any existing output, error, and log files from previous runs of the hostname job. Submit the job three separate times in a row (there are better ways to do this, which we will cover in the next lecture) Wait for all three jobs to finish Examine the output file: How many hostnames are there? Did HTCondor erase the previous contents for each job, or add new lines? Examine the log file\u2026 carefully: What happened there? Pay close attention to the times and job IDs of the events. For further clarification about how HTCondor handles these files, reach out to your mentor or one of the other school staff via Slack or email.","title":"Understanding How HTCondor Writes Files"},{"location":"materials/htcondor/part1-ex5-request/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.5: Declare Resource Needs \u00b6 The goal of this exercise is to demonstrate how to test and tune the request_X statements in a submit file for when you don't know what resources your job needs. There are three special resource request statements that you can use (optionally) in an HTCondor submit file: request_cpus for the number of CPUs your job will use (most softwares will take an argument to control this number, and it's usually otherwise \"1\") request_memory for the maximum amount of run-time memory your job may use request_disk for the maximum amount of disk space your job may use (including the executable and all other data that may show up during the job) HTCondor defaults to certain reasonable values for these request settings, so you do not need to use them to get small jobs to run. However, it is in YOUR best interest to always estimate resource requests before submitting any job, and to definitely tune your requests before submitting multiple jobs. In many HTCondor pools: If your job goes over the request values, it may be removed from the execute machine and held (status 'H' in the condor_q output, awaiting action on your part) without saving any partial job output files. So it is a disadvantage to not declare your resource needs or if you underestimate them. Conversely, if you overestimate them by too much, your jobs will match to fewer slots, with a longer average wait time. Additionally, by hogging up resources that you don't need, other users may be deprived of the resources they require. In the long run, it works better for all users of the pool if you declare what you really need. But how do you know what to request? In particular, we are concerned with memory and disk here; requesting multiple CPUs and using them is covered a bit in later school materials, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running and completing soonest). Determining Resource Needs Before Running Any Jobs \u00b6 Note If you are running short on time, you can skip to \"Determining Resource Needs By Running Test Jobs\", below, but try to come back and read over this part at some point. It can be very difficult to predict the memory needs of your running program without running tests. Typically, the memory size of a job changes over time, making the task even trickier. If you have knowledge ahead of time about your job\u2019s maximum memory needs, use that, or maybe a number that's just a bit higher, to be safe. Worst case scenario, you can request a fairly large amount of memory (as high as what's on your laptop or other server, if you know your program can run without crashing) for a first test job, OR you can run the program locally and 'watch' it: Examining a Running Program on a Local Computer \u00b6 When working on a shared submit server, you should not run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all uses. However, you may have access to other computers (your laptop, for example, or another server) where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage. For Memory: \u00b6 On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the ps command or the top command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples: Using ps : username@learn $ ps ux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND cat 24342 0.0 0.0 90224 1864 ? S 13:39 0:00 sshd: cat@pts/0 cat 24343 0.0 0.0 66096 1580 pts/0 Ss 13:39 0:00 -bash cat 25864 0.0 0.0 65624 996 pts/0 R+ 13:52 0:00 ps ux cat 30052 0.0 0.0 90720 2456 ? S Jun22 0:00 sshd: cat@pts/2 cat 30053 0.0 0.0 66096 1624 pts/2 Ss+ Jun22 0:00 -bash The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value. Using top : username@learn $ top -u <USERNAME> top - 13:55:31 up 11 days, 20:59, 5 users, load average: 0.12, 0.12, 0.09 Tasks: 198 total, 1 running, 197 sleeping, 0 stopped, 0 zombie Cpu(s): 1.2%us, 0.1%sy, 0.0%ni, 98.5%id, 0.2%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 4001440k total, 3558028k used, 443412k free, 258568k buffers Swap: 4194296k total, 148k used, 4194148k free, 2960760k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 24342 cat 15 0 90224 1864 1096 S 0.0 0.0 0:00.26 sshd 24343 cat 15 0 66096 1580 1232 S 0.0 0.0 0:00.07 bash 25927 cat 15 0 12760 1196 836 R 0.0 0.0 0:00.01 top 30052 cat 16 0 90720 2456 1112 S 0.0 0.1 0:00.69 sshd 30053 cat 18 0 66096 1624 1236 S 0.0 0.0 0:00.37 bash The top command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter q to quit the interactive display. Again, the highlighted RES column shows an approximation of memory usage. For Disk: \u00b6 Determining disk needs may be a bit easier, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts everything in your job sandbox toward your job\u2019s disk usage: The executable itself All \"input\" files (anything else that gets transferred TO the job, even if you don't think of it as \"input\") All files created during the job (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file. All temporary files created in the sandbox, even if they get deleted by the executable before it's done. If you can run your program within a single directory on a local computer (not on the submit server), you should be able to view files and their sizes with the ls and du commands. Determining Resource Needs By Running Test Jobs (BEST) \u00b6 Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs. For example, here is a strange Python script that does not do anything useful, but consumes some real resources while running: #!/usr/bin/env python import time import os size = 1000000 numbers = [] for i in xrange ( size ): numbers . append ( str ( i )) tempfile = open ( 'numbers.txt' , 'w' ) tempfile . write ( ' ' . join ( numbers )) tempfile . close () time . sleep ( 60 ) Without trying to figure out what this code does or how many resources it uses, create a submit file for it, and run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the job will use far more). When it is done, examine the log file. In particular, we care about these lines: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 6744 1048576 1335138 Memory (MB) : 57 1024 1024 So, now we know that HTCondor saw that the job used 6,744 KB of disk (= about 6.5 MB) and 57 MB of memory! This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. And it never hurts to round up your resource requests a little, just in case your job occasionally uses more resources. Setting Resource Requirements \u00b6 Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe: # rounded up from 57 MB request_memory = 60MB # rounded up from 6.5 MB request_disk = 7MB Pay close attention to units: Without explicit units, request_memory is in MB (megabytes) Without explicit units, request_disk is in KB (kilobytes) Allowable units are KB (kilobytes), MB (megabytes), GB (gigabytes), and TB (terabytes) HTCondor translates these requirements into attributes that become part of the job's requirements expression. However, do not put your CPU, memory, and disk requirements directly into the requirements expression; use the request_XXX statements instead. If you still have time in this working session, Add these requirements to your submit file for the Python script, rerun the job, and confirm in the log file that your requests were used. After changing the requirements in your submit file, did your job run successfully? If not, why?","title":"Exercise 1.5"},{"location":"materials/htcondor/part1-ex5-request/#htc-exercise-15-declare-resource-needs","text":"The goal of this exercise is to demonstrate how to test and tune the request_X statements in a submit file for when you don't know what resources your job needs. There are three special resource request statements that you can use (optionally) in an HTCondor submit file: request_cpus for the number of CPUs your job will use (most softwares will take an argument to control this number, and it's usually otherwise \"1\") request_memory for the maximum amount of run-time memory your job may use request_disk for the maximum amount of disk space your job may use (including the executable and all other data that may show up during the job) HTCondor defaults to certain reasonable values for these request settings, so you do not need to use them to get small jobs to run. However, it is in YOUR best interest to always estimate resource requests before submitting any job, and to definitely tune your requests before submitting multiple jobs. In many HTCondor pools: If your job goes over the request values, it may be removed from the execute machine and held (status 'H' in the condor_q output, awaiting action on your part) without saving any partial job output files. So it is a disadvantage to not declare your resource needs or if you underestimate them. Conversely, if you overestimate them by too much, your jobs will match to fewer slots, with a longer average wait time. Additionally, by hogging up resources that you don't need, other users may be deprived of the resources they require. In the long run, it works better for all users of the pool if you declare what you really need. But how do you know what to request? In particular, we are concerned with memory and disk here; requesting multiple CPUs and using them is covered a bit in later school materials, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running and completing soonest).","title":"HTC Exercise 1.5: Declare Resource Needs"},{"location":"materials/htcondor/part1-ex5-request/#determining-resource-needs-before-running-any-jobs","text":"Note If you are running short on time, you can skip to \"Determining Resource Needs By Running Test Jobs\", below, but try to come back and read over this part at some point. It can be very difficult to predict the memory needs of your running program without running tests. Typically, the memory size of a job changes over time, making the task even trickier. If you have knowledge ahead of time about your job\u2019s maximum memory needs, use that, or maybe a number that's just a bit higher, to be safe. Worst case scenario, you can request a fairly large amount of memory (as high as what's on your laptop or other server, if you know your program can run without crashing) for a first test job, OR you can run the program locally and 'watch' it:","title":"Determining Resource Needs Before Running Any Jobs"},{"location":"materials/htcondor/part1-ex5-request/#examining-a-running-program-on-a-local-computer","text":"When working on a shared submit server, you should not run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all uses. However, you may have access to other computers (your laptop, for example, or another server) where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage.","title":"Examining a Running Program on a Local Computer"},{"location":"materials/htcondor/part1-ex5-request/#for-memory","text":"On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the ps command or the top command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples: Using ps : username@learn $ ps ux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND cat 24342 0.0 0.0 90224 1864 ? S 13:39 0:00 sshd: cat@pts/0 cat 24343 0.0 0.0 66096 1580 pts/0 Ss 13:39 0:00 -bash cat 25864 0.0 0.0 65624 996 pts/0 R+ 13:52 0:00 ps ux cat 30052 0.0 0.0 90720 2456 ? S Jun22 0:00 sshd: cat@pts/2 cat 30053 0.0 0.0 66096 1624 pts/2 Ss+ Jun22 0:00 -bash The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value. Using top : username@learn $ top -u <USERNAME> top - 13:55:31 up 11 days, 20:59, 5 users, load average: 0.12, 0.12, 0.09 Tasks: 198 total, 1 running, 197 sleeping, 0 stopped, 0 zombie Cpu(s): 1.2%us, 0.1%sy, 0.0%ni, 98.5%id, 0.2%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 4001440k total, 3558028k used, 443412k free, 258568k buffers Swap: 4194296k total, 148k used, 4194148k free, 2960760k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 24342 cat 15 0 90224 1864 1096 S 0.0 0.0 0:00.26 sshd 24343 cat 15 0 66096 1580 1232 S 0.0 0.0 0:00.07 bash 25927 cat 15 0 12760 1196 836 R 0.0 0.0 0:00.01 top 30052 cat 16 0 90720 2456 1112 S 0.0 0.1 0:00.69 sshd 30053 cat 18 0 66096 1624 1236 S 0.0 0.0 0:00.37 bash The top command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter q to quit the interactive display. Again, the highlighted RES column shows an approximation of memory usage.","title":"For Memory:"},{"location":"materials/htcondor/part1-ex5-request/#for-disk","text":"Determining disk needs may be a bit easier, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts everything in your job sandbox toward your job\u2019s disk usage: The executable itself All \"input\" files (anything else that gets transferred TO the job, even if you don't think of it as \"input\") All files created during the job (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file. All temporary files created in the sandbox, even if they get deleted by the executable before it's done. If you can run your program within a single directory on a local computer (not on the submit server), you should be able to view files and their sizes with the ls and du commands.","title":"For Disk:"},{"location":"materials/htcondor/part1-ex5-request/#determining-resource-needs-by-running-test-jobs-best","text":"Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs. For example, here is a strange Python script that does not do anything useful, but consumes some real resources while running: #!/usr/bin/env python import time import os size = 1000000 numbers = [] for i in xrange ( size ): numbers . append ( str ( i )) tempfile = open ( 'numbers.txt' , 'w' ) tempfile . write ( ' ' . join ( numbers )) tempfile . close () time . sleep ( 60 ) Without trying to figure out what this code does or how many resources it uses, create a submit file for it, and run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the job will use far more). When it is done, examine the log file. In particular, we care about these lines: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 6744 1048576 1335138 Memory (MB) : 57 1024 1024 So, now we know that HTCondor saw that the job used 6,744 KB of disk (= about 6.5 MB) and 57 MB of memory! This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. And it never hurts to round up your resource requests a little, just in case your job occasionally uses more resources.","title":"Determining Resource Needs By Running Test Jobs (BEST)"},{"location":"materials/htcondor/part1-ex5-request/#setting-resource-requirements","text":"Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe: # rounded up from 57 MB request_memory = 60MB # rounded up from 6.5 MB request_disk = 7MB Pay close attention to units: Without explicit units, request_memory is in MB (megabytes) Without explicit units, request_disk is in KB (kilobytes) Allowable units are KB (kilobytes), MB (megabytes), GB (gigabytes), and TB (terabytes) HTCondor translates these requirements into attributes that become part of the job's requirements expression. However, do not put your CPU, memory, and disk requirements directly into the requirements expression; use the request_XXX statements instead. If you still have time in this working session, Add these requirements to your submit file for the Python script, rerun the job, and confirm in the log file that your requests were used. After changing the requirements in your submit file, did your job run successfully? If not, why?","title":"Setting Resource Requirements"},{"location":"materials/htcondor/part1-ex6-remove/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.6: Remove Jobs From the Queue \u00b6 The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you may want to just remove them, edit the submit files, and then submit again. Skip this exercise and come back to it if you are short on time, or until you need to remove jobs for other exercises Note Please remember to remove any jobs from the queue that you have given up on. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching). This exercise is very short, but if you are out of time, you can come back to it later. Removing a Job or Cluster From the Queue \u00b6 To practice removing jobs from the queue, you need a job in the queue! Submit a job from an earlier exercise Determine the job ID ( cluster.process ) from the condor_submit output or from condor_q Remove the job: username@learn $ condor_rm <JOB.ID> Use the full job ID this time, e.g. 5759.0 . Did the job leave the queue immediately? If not, about how long did it take? So far, we have created job clusters that contain only one job process (the .0 part of the job ID). That will change soon, so it is good to know how to remove a specific job ID. However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the .0 part of the job ID) in the condor_rm command: username@learn $ condor_rm <CLUSTER> Finally, you can include many job clusters and full job IDs in a single condor_rm command. For example: username@learn $ condor_rm 5768 5769 5770 .0 5771 .2 Removing All of Your Jobs \u00b6 If you really want to remove all of your jobs at once, you can do that with: username@learn $ condor_rm <USERNAME> If you want to test it: (optional, though you'll likely need this in the future) Quickly submit several jobs from past exercises View the jobs in the queue with condor_q Remove them all with the above command Use condor_q to track progress In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them.","title":"Exercise 1.6"},{"location":"materials/htcondor/part1-ex6-remove/#htc-exercise-16-remove-jobs-from-the-queue","text":"The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you may want to just remove them, edit the submit files, and then submit again. Skip this exercise and come back to it if you are short on time, or until you need to remove jobs for other exercises Note Please remember to remove any jobs from the queue that you have given up on. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching). This exercise is very short, but if you are out of time, you can come back to it later.","title":"HTC Exercise 1.6: Remove Jobs From the Queue"},{"location":"materials/htcondor/part1-ex6-remove/#removing-a-job-or-cluster-from-the-queue","text":"To practice removing jobs from the queue, you need a job in the queue! Submit a job from an earlier exercise Determine the job ID ( cluster.process ) from the condor_submit output or from condor_q Remove the job: username@learn $ condor_rm <JOB.ID> Use the full job ID this time, e.g. 5759.0 . Did the job leave the queue immediately? If not, about how long did it take? So far, we have created job clusters that contain only one job process (the .0 part of the job ID). That will change soon, so it is good to know how to remove a specific job ID. However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the .0 part of the job ID) in the condor_rm command: username@learn $ condor_rm <CLUSTER> Finally, you can include many job clusters and full job IDs in a single condor_rm command. For example: username@learn $ condor_rm 5768 5769 5770 .0 5771 .2","title":"Removing a Job or Cluster From the Queue"},{"location":"materials/htcondor/part1-ex6-remove/#removing-all-of-your-jobs","text":"If you really want to remove all of your jobs at once, you can do that with: username@learn $ condor_rm <USERNAME> If you want to test it: (optional, though you'll likely need this in the future) Quickly submit several jobs from past exercises View the jobs in the queue with condor_q Remove them all with the above command Use condor_q to track progress In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them.","title":"Removing All of Your Jobs"},{"location":"materials/htcondor/part1-ex7-compile/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Bonus Exercise 1.7: Compile and Run Some C Code \u00b6 The goal of this exercise is to show that compiled code works just fine in HTCondor. It is mainly of interest to people who have their own C code to run (or C++, or really any compiled code, although Java would be handled a bit differently). Preparing a C Executable \u00b6 When preparing a C program for HTCondor, it is best to compile and link the executable statically, so that it does not depend on external libraries and their particular versions. Why is this important? When your compiled C program is sent to another machine for execution, that machine may not have the same libraries that you have on your submit machine (or wherever you compile the program). If the libraries are not available or are the wrong versions, your program may fail or, perhaps worse, silently produce the wrong results. Here is a simple C program to try using (thanks, Alain Roy): #include <stdio.h> int main ( int argc , char ** argv ) { int sleep_time ; int input ; int failure ; if ( argc != 3 ) { printf ( \"Usage: simple <sleep-time> <integer> \\n \" ); failure = 1 ; } else { sleep_time = atoi ( argv [ 1 ]); input = atoi ( argv [ 2 ]); printf ( \"Thinking really hard for %d seconds... \\n \" , sleep_time ); sleep ( sleep_time ); printf ( \"We calculated: %d \\n \" , input * 2 ); failure = 0 ; } return failure ; } Save that code to a file, for example, simple.c . Compile the program with static linking: username@learn $ gcc -static -o simple simple.c As always, test that you can run your command from the command line first. First, without arguments to make sure it fails correctly: username@learn $ ./simple and then with valid arguments: username@learn $ ./simple 5 21 Running a Compiled C Program \u00b6 Running the compiled program is no different than running any other program. Here is a submit file for the C program (call it simple.sub): executable = simple arguments = \"60 64\" output = c-program.out error = c-program.err log = c-program.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_cpus = 1 request_memory = 1GB request_disk = 1MB queue Then submit the job as usual! In summary, it is easy to work with statically linked compiled code. It is possible to handle dynamically linked compiled code, but it is trickier. We will only mention this topic briefly during the lecture on Software.","title":"Bonus Exercise 1.7"},{"location":"materials/htcondor/part1-ex7-compile/#htc-bonus-exercise-17-compile-and-run-some-c-code","text":"The goal of this exercise is to show that compiled code works just fine in HTCondor. It is mainly of interest to people who have their own C code to run (or C++, or really any compiled code, although Java would be handled a bit differently).","title":"HTC Bonus Exercise 1.7: Compile and Run Some C Code"},{"location":"materials/htcondor/part1-ex7-compile/#preparing-a-c-executable","text":"When preparing a C program for HTCondor, it is best to compile and link the executable statically, so that it does not depend on external libraries and their particular versions. Why is this important? When your compiled C program is sent to another machine for execution, that machine may not have the same libraries that you have on your submit machine (or wherever you compile the program). If the libraries are not available or are the wrong versions, your program may fail or, perhaps worse, silently produce the wrong results. Here is a simple C program to try using (thanks, Alain Roy): #include <stdio.h> int main ( int argc , char ** argv ) { int sleep_time ; int input ; int failure ; if ( argc != 3 ) { printf ( \"Usage: simple <sleep-time> <integer> \\n \" ); failure = 1 ; } else { sleep_time = atoi ( argv [ 1 ]); input = atoi ( argv [ 2 ]); printf ( \"Thinking really hard for %d seconds... \\n \" , sleep_time ); sleep ( sleep_time ); printf ( \"We calculated: %d \\n \" , input * 2 ); failure = 0 ; } return failure ; } Save that code to a file, for example, simple.c . Compile the program with static linking: username@learn $ gcc -static -o simple simple.c As always, test that you can run your command from the command line first. First, without arguments to make sure it fails correctly: username@learn $ ./simple and then with valid arguments: username@learn $ ./simple 5 21","title":"Preparing a C Executable"},{"location":"materials/htcondor/part1-ex7-compile/#running-a-compiled-c-program","text":"Running the compiled program is no different than running any other program. Here is a submit file for the C program (call it simple.sub): executable = simple arguments = \"60 64\" output = c-program.out error = c-program.err log = c-program.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_cpus = 1 request_memory = 1GB request_disk = 1MB queue Then submit the job as usual! In summary, it is easy to work with statically linked compiled code. It is possible to handle dynamically linked compiled code, but it is trickier. We will only mention this topic briefly during the lecture on Software.","title":"Running a Compiled C Program"},{"location":"materials/htcondor/part2-ex1-files/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 2.1: Work With Input and Output Files \u00b6 The goal of this exercise is make input files available to your job on the execute machine, and return output files back. This small change significantly adds to the kinds of jobs that you can run. Viewing a Job Sandbox \u00b6 Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor starter process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a minimal job with no special input or output files. Save the script below in a file named sandbox.sh : #!/bin/sh echo 'Date: ' ` date ` echo 'Host: ' ` hostname ` echo 'Sandbox: ' ` pwd ` ls -alF # END Create a submit file for this script and submit it. When the job finishes, look at the contents of the output file. In the output file, note the Sandbox: line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished. Next, look at the output that appears after the Sandbox: line; it is the output from the ls command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of sandbox.sh . The files are: .chirp.config Configuration for an advanced feature .job.ad The job ClassAd .machine.ad The machine ClassAd _condor_stderr Saved standard error from the job _condor_stdout Saved standard output from the job condor_exec.exe The executable, renamed from sandbox.sh tmp/ , var/tmp/ Directories in which to put temporary files So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), renamed it ( condor_exec.exe ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the submit machine as your executable, was not transferred, nor were any other files that happened to be in directory with the submit file. Now that we know something about the sandbox, we can transfer more files to and from it. Running a Job With Input Files \u00b6 Next, you will run a job that requires an input file. Remember, the initial job sandbox will contain only the renamed job executable, unless you tell HTCondor explicitly about every other file that needs to be transferred. Fortunately, this is easy. Here is a Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts. #!/usr/bin/env python import os import sys if len ( sys . argv ) != 2 : print 'Usage: %s DATA' % ( os . path . basename ( sys . argv [ 0 ])) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} my_file = open ( input_filename , 'r' ) for line in my_file : word = line . strip () . lower () if word in words : words [ word ] += 1 else : words [ word ] = 1 my_file . close () for word in sorted ( words . keys ()): print ' %8d %s ' % ( words [ word ], word ) Save the Python script in a file named freq.py . Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool20/intro-2.1-words.txt Create a submit file for the freq.py executable. Add a line to tell HTCondor to transfer the input file: transfer_input_files = intro-2.1-words.txt As with all submit file commands, it does not matter where this line goes, as long as it comes before the word queue . Do not forget to add a line to name the input file as the argument to the Python script. Submit the job, wait for it to finish, and check the output! If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask for help in the #intro-to-htc Slack channel. Note If you want to transfer more than one input file, list all of them on a single transfer_input_files command, separated by commas. For example, if there are three input files: transfer_input_files = a.txt, b.txt, c.txt Transferring Output Files \u00b6 So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back? Let\u2019s start by exploring what happens to files that a job creates in the sandbox. We will use a very simple method for creating a new file: we will copy an input file to another name. Find or create a small input file (it is fine to use any small file from a previous exercise). Create a submit file that transfers the input file and copies it to another name (as if doing /bin/cp input.txt output.txt on the command line) Make the output filename different than any filenames that are in your submit directory What is the executable line? What is the arguments line? How do you tell HTCondor to transfer the input file? As always, use output , error , and log filenames that are different from previous exercises Submit the job and wait for it to finish. What happened? Can you tell what HTCondor did with the output file that was created (did it end up back on the submit server?), after it was created in the job sandbox? Look carefully at the list of files in your submit directory now. Transferring Specific Output Files \u00b6 As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back changed input files, too. But, this only works for files that are in the top-level sandbox directory, and not for ones contained in subdirectories. What if you want to bring back only some output files, or output files contained in subdirectories? Here is a shell script that creates several files, including a copy of an input file in a new subdirectory: #!/bin/sh if [ $# -ne 1 ] ; then echo \"Usage: $0 INPUT\" ; exit 1 ; fi date > output-timestamp.txt cal > output-calendar.txt mkdir subdirectory cp $1 subdirectory/backup- $1 First, let\u2019s confirm that HTCondor does not bring back the output file in the subdirectory: Save the shell script in a file named output.sh . Write a submit file that transfers an input file and runs output.sh on it (passing the filename as an argument). Submit the job, wait for it to finish, and examine the contents of your submit directory. Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to transfer these specific files: transfer_output_files = output-timestamp.txt, subdirectory/ Note See the trailing slash ( / ) on the subdirectory? That tells HTCondor to transfer back the files contained in the subdirectory, but not the directory itself; the files will be written directly into the submit directory. If you want HTCondor to transfer back an entire directory, leave off the trailing slash. Remove all output files from the previous run, including output-timestamp.txt and output-calendar.txt . Copy the previous submit file that ran output.sh and add the transfer_output_files line from above. Submit the job, wait for it to finish, and examine the contents of your submit directory. Did it work as you expected? Thinking About Progress So Far \u00b6 At this point, you can do just about everything that you need in order to run jobs on a local HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement! In some ways, everything after this exercise shows you how to submit multiple jobs at once and makes it easier to run certain kinds of jobs and deal with certain kinds of situations. References \u00b6 There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read \"Submitting Jobs Without a Shared Filesystem\" in the HTCondor Manual.","title":"Exercise 2.1"},{"location":"materials/htcondor/part2-ex1-files/#htc-exercise-21-work-with-input-and-output-files","text":"The goal of this exercise is make input files available to your job on the execute machine, and return output files back. This small change significantly adds to the kinds of jobs that you can run.","title":"HTC Exercise 2.1: Work With Input and Output Files"},{"location":"materials/htcondor/part2-ex1-files/#viewing-a-job-sandbox","text":"Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor starter process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a minimal job with no special input or output files. Save the script below in a file named sandbox.sh : #!/bin/sh echo 'Date: ' ` date ` echo 'Host: ' ` hostname ` echo 'Sandbox: ' ` pwd ` ls -alF # END Create a submit file for this script and submit it. When the job finishes, look at the contents of the output file. In the output file, note the Sandbox: line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished. Next, look at the output that appears after the Sandbox: line; it is the output from the ls command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of sandbox.sh . The files are: .chirp.config Configuration for an advanced feature .job.ad The job ClassAd .machine.ad The machine ClassAd _condor_stderr Saved standard error from the job _condor_stdout Saved standard output from the job condor_exec.exe The executable, renamed from sandbox.sh tmp/ , var/tmp/ Directories in which to put temporary files So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), renamed it ( condor_exec.exe ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the submit machine as your executable, was not transferred, nor were any other files that happened to be in directory with the submit file. Now that we know something about the sandbox, we can transfer more files to and from it.","title":"Viewing a Job Sandbox"},{"location":"materials/htcondor/part2-ex1-files/#running-a-job-with-input-files","text":"Next, you will run a job that requires an input file. Remember, the initial job sandbox will contain only the renamed job executable, unless you tell HTCondor explicitly about every other file that needs to be transferred. Fortunately, this is easy. Here is a Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts. #!/usr/bin/env python import os import sys if len ( sys . argv ) != 2 : print 'Usage: %s DATA' % ( os . path . basename ( sys . argv [ 0 ])) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} my_file = open ( input_filename , 'r' ) for line in my_file : word = line . strip () . lower () if word in words : words [ word ] += 1 else : words [ word ] = 1 my_file . close () for word in sorted ( words . keys ()): print ' %8d %s ' % ( words [ word ], word ) Save the Python script in a file named freq.py . Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool20/intro-2.1-words.txt Create a submit file for the freq.py executable. Add a line to tell HTCondor to transfer the input file: transfer_input_files = intro-2.1-words.txt As with all submit file commands, it does not matter where this line goes, as long as it comes before the word queue . Do not forget to add a line to name the input file as the argument to the Python script. Submit the job, wait for it to finish, and check the output! If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask for help in the #intro-to-htc Slack channel. Note If you want to transfer more than one input file, list all of them on a single transfer_input_files command, separated by commas. For example, if there are three input files: transfer_input_files = a.txt, b.txt, c.txt","title":"Running a Job With Input Files"},{"location":"materials/htcondor/part2-ex1-files/#transferring-output-files","text":"So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back? Let\u2019s start by exploring what happens to files that a job creates in the sandbox. We will use a very simple method for creating a new file: we will copy an input file to another name. Find or create a small input file (it is fine to use any small file from a previous exercise). Create a submit file that transfers the input file and copies it to another name (as if doing /bin/cp input.txt output.txt on the command line) Make the output filename different than any filenames that are in your submit directory What is the executable line? What is the arguments line? How do you tell HTCondor to transfer the input file? As always, use output , error , and log filenames that are different from previous exercises Submit the job and wait for it to finish. What happened? Can you tell what HTCondor did with the output file that was created (did it end up back on the submit server?), after it was created in the job sandbox? Look carefully at the list of files in your submit directory now.","title":"Transferring Output Files"},{"location":"materials/htcondor/part2-ex1-files/#transferring-specific-output-files","text":"As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back changed input files, too. But, this only works for files that are in the top-level sandbox directory, and not for ones contained in subdirectories. What if you want to bring back only some output files, or output files contained in subdirectories? Here is a shell script that creates several files, including a copy of an input file in a new subdirectory: #!/bin/sh if [ $# -ne 1 ] ; then echo \"Usage: $0 INPUT\" ; exit 1 ; fi date > output-timestamp.txt cal > output-calendar.txt mkdir subdirectory cp $1 subdirectory/backup- $1 First, let\u2019s confirm that HTCondor does not bring back the output file in the subdirectory: Save the shell script in a file named output.sh . Write a submit file that transfers an input file and runs output.sh on it (passing the filename as an argument). Submit the job, wait for it to finish, and examine the contents of your submit directory. Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to transfer these specific files: transfer_output_files = output-timestamp.txt, subdirectory/ Note See the trailing slash ( / ) on the subdirectory? That tells HTCondor to transfer back the files contained in the subdirectory, but not the directory itself; the files will be written directly into the submit directory. If you want HTCondor to transfer back an entire directory, leave off the trailing slash. Remove all output files from the previous run, including output-timestamp.txt and output-calendar.txt . Copy the previous submit file that ran output.sh and add the transfer_output_files line from above. Submit the job, wait for it to finish, and examine the contents of your submit directory. Did it work as you expected?","title":"Transferring Specific Output Files"},{"location":"materials/htcondor/part2-ex1-files/#thinking-about-progress-so-far","text":"At this point, you can do just about everything that you need in order to run jobs on a local HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement! In some ways, everything after this exercise shows you how to submit multiple jobs at once and makes it easier to run certain kinds of jobs and deal with certain kinds of situations.","title":"Thinking About Progress So Far"},{"location":"materials/htcondor/part2-ex1-files/#references","text":"There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read \"Submitting Jobs Without a Shared Filesystem\" in the HTCondor Manual.","title":"References"},{"location":"materials/htcondor/part2-ex2-queue-n/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 2.2: Use queue N , $(Cluster), and $(Process) \u00b6 The goal of the next several exercises is to learn to submit many jobs from a single queue statement, and to control things like filenames and arguments on a per-job basis when doing so. Suppose you have a program that you want to run many times with different arguments each time. With what you know so far, you have a couple of choices: Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026 Write many submit files that are nearly identical except for the program argument Neither of these options seems very satisfying. Fortunately, we can do better with HTCondor. Running Many Jobs With One queue Statement \u00b6 Here is a C program that uses a stochastic (random) method to estimate the value of \u03c0 \u2014 feel free to try to figure out the method from the code, but it is not critical for this exercise. The single argument to the program is the number of samples to take. More samples should result in better estimates! #include <stdio.h> #include <stdlib.h> #include <sys/time.h> int main ( int argc , char * argv []) { struct timeval my_timeval ; int iterations = 0 ; int inside_circle = 0 ; int i ; double x , y , pi_estimate ; gettimeofday ( & my_timeval , NULL ); srand48 ( my_timeval . tv_sec ^ my_timeval . tv_usec ); if ( argc == 2 ) { iterations = atoi ( argv [ 1 ]); } else { printf ( \"usage: circlepi ITERATIONS \\n \" ); exit ( 1 ); } for ( i = 0 ; i < iterations ; i ++ ) { x = ( drand48 () - 0.5 ) * 2.0 ; y = ( drand48 () - 0.5 ) * 2.0 ; if ((( x * x ) + ( y * y )) <= 1.0 ) { inside_circle ++ ; } } pi_estimate = 4.0 * (( double ) inside_circle / ( double ) iterations ); printf ( \"%d iterations, %d inside; pi = %f \\n \" , iterations , inside_circle , pi_estimate ); return 0 ; } In a new directory for this exercise, save the code to a file named circlepi.c Compile the code (we will cover this in more detail during the Software lecture): username@learn $ gcc -static -o circlepi circlepi.c Test the program with just 1000 samples: username@learn $ ./circlepi 1000 Now suppose that you want to run the program many times, to produce many estimates. To do so, we can tell HTCondor how many jobs to \"queue up\" via the queue statement we've been putting at the end of each of our submit files. Let\u2019s see how it works: Write a normal submit file for this program Pass 1 million ( 1000000 ) as the command line argument to circlepi Make sure to include log , output , and error (with filenames like circlepi.log ), and request_* lines At the end of the file, write queue 3 instead of just queue (\"queue 3 jobs\" vs. \"queue a job\"). Submit the file. Note the slightly different message from condor_submit : 3 job(s) submitted to cluster *NNNN*. Before the jobs execute, look at the job queue to see the multiple jobs Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 10228.0 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.1 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.2 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 In this sample, all three jobs are part of cluster 10228 , but the first job was assigned process 0 , the second job was assigned process 1 , and the third one was assigned process 2 . (Programmers like to start counting from 0.) Now we can understand what the first column in the output, the job ID , represents. It is a job\u2019s cluster number, a dot ( . ), and the job\u2019s process number. So in the example above, the job ID of the second job is 10228.1 . Pop Quiz: Do you remember how to ask HTCondor to list all of the jobs from one cluster? How about one specific job ID? Using queue N With Output \u00b6 When all three jobs in your single cluster are finished, examine the resulting files. What is in the output file? What is in the error file (hopefully nothing)? What is in the log file? Look carefully at the job IDs in each event. Is this what you expected? Is it what you wanted? Using $(Process) to Distinguish Jobs \u00b6 As you saw with the experiment above, each job ended up overwriting the same output and error filenames in the submission directory. After all, we didn't tell it to behave any differently when it ran three jobs. We need a way to separate output (and error) files per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily. When processing a submit file, HTCondor will replace any instance of $(Process) with the process number of the job, for each job that is queued. For example, you can use the $(Process) variable to define a separate output file name for each job: output = my-output-file-$(Process).out queue 10 Even though the output filename is defined only once, HTCondor will create separate output filenames for each job: First job my-output-file-0.out Second job my-output-file-1.out Third job my-output-file-2.out ... ... Last (tenth) job my-output-file-9.out Let\u2019s see how this works for our program that estimates \u03c0. In your submit file, change the definitions of output and error to use $(Process) in the filename, similar to the example above. Delete any output, error, and log files from previous runs. Submit the updated file. When all three jobs are finished, examine the resulting files again. How many files are there of each type? What are their names? Is this what you expected? Is it what you wanted from the \u03c0 estimation process? Using $(Cluster) to Separate Files Across Runs \u00b6 With $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well. In addition to $(Process) , there is also a $(Cluster) variable that you can use in your submit files. It works just like $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used with $(Process) , it can be used to separate files by run. For example, consider this output statement: output = my-output-file-$(Cluster)-$(Process).out For one particular run, it might result in output filenames like my-output-file-2444-0.out , myoutput-file-2444-1.out , myoutput-file-2444-2.out , etc. However, the next run would have different filenames, replacing 2444 with the new Cluster number of that run. Using $(Process) and $(Cluster) in Other Statements \u00b6 The $(Cluster) and $(Process) variables can be used in any submit file statement, although they are useful in some kinds of submit file statements and not really for others. For example, consider using $(Cluster) or $(Process) in each of the below: log transfer_input_files transfer_output_files arguments Unfortunately, HTCondor does not easily let you perform math on the $(Process) number when using it. So, for example, if you use $(Process) as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need a solution like those in the next exercises. (Optional) Defining JobBatchName for Tracking \u00b6 During the lecture, it was mentioned that you can define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used. Once again, we will use sleep jobs, so that your jobs remain in the queue long enough to experiment on. Create a submit file that runs sleep 120 (or some reasonable duration). Instead of a single queue statement, write this: jobbatchname = 1 queue 5 Submit the file. Now, quickly edit the submit file to instead say: jobbatchname = 2 Submit the file again. Check on the submissions using a normal condor_q and condor_q -nobatch . Of course, your special attribute does not appear in the condor_q -nobatch output, but it is present in the condor_q output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your condor_q output to one type of job or another. First, run this command: username@learn $ condor_q -constraint 'JobBatchName == \"1\"' Do you get the output that you expected? Using the example command above, how would you list your other five jobs? (More on constraints in later exercises.)","title":"Exercise 2.2"},{"location":"materials/htcondor/part2-ex2-queue-n/#htc-exercise-22-use-queue-n-cluster-and-process","text":"The goal of the next several exercises is to learn to submit many jobs from a single queue statement, and to control things like filenames and arguments on a per-job basis when doing so. Suppose you have a program that you want to run many times with different arguments each time. With what you know so far, you have a couple of choices: Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026 Write many submit files that are nearly identical except for the program argument Neither of these options seems very satisfying. Fortunately, we can do better with HTCondor.","title":"HTC Exercise 2.2: Use queue N, $(Cluster), and $(Process)"},{"location":"materials/htcondor/part2-ex2-queue-n/#running-many-jobs-with-one-queue-statement","text":"Here is a C program that uses a stochastic (random) method to estimate the value of \u03c0 \u2014 feel free to try to figure out the method from the code, but it is not critical for this exercise. The single argument to the program is the number of samples to take. More samples should result in better estimates! #include <stdio.h> #include <stdlib.h> #include <sys/time.h> int main ( int argc , char * argv []) { struct timeval my_timeval ; int iterations = 0 ; int inside_circle = 0 ; int i ; double x , y , pi_estimate ; gettimeofday ( & my_timeval , NULL ); srand48 ( my_timeval . tv_sec ^ my_timeval . tv_usec ); if ( argc == 2 ) { iterations = atoi ( argv [ 1 ]); } else { printf ( \"usage: circlepi ITERATIONS \\n \" ); exit ( 1 ); } for ( i = 0 ; i < iterations ; i ++ ) { x = ( drand48 () - 0.5 ) * 2.0 ; y = ( drand48 () - 0.5 ) * 2.0 ; if ((( x * x ) + ( y * y )) <= 1.0 ) { inside_circle ++ ; } } pi_estimate = 4.0 * (( double ) inside_circle / ( double ) iterations ); printf ( \"%d iterations, %d inside; pi = %f \\n \" , iterations , inside_circle , pi_estimate ); return 0 ; } In a new directory for this exercise, save the code to a file named circlepi.c Compile the code (we will cover this in more detail during the Software lecture): username@learn $ gcc -static -o circlepi circlepi.c Test the program with just 1000 samples: username@learn $ ./circlepi 1000 Now suppose that you want to run the program many times, to produce many estimates. To do so, we can tell HTCondor how many jobs to \"queue up\" via the queue statement we've been putting at the end of each of our submit files. Let\u2019s see how it works: Write a normal submit file for this program Pass 1 million ( 1000000 ) as the command line argument to circlepi Make sure to include log , output , and error (with filenames like circlepi.log ), and request_* lines At the end of the file, write queue 3 instead of just queue (\"queue 3 jobs\" vs. \"queue a job\"). Submit the file. Note the slightly different message from condor_submit : 3 job(s) submitted to cluster *NNNN*. Before the jobs execute, look at the job queue to see the multiple jobs Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 10228.0 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.1 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.2 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 In this sample, all three jobs are part of cluster 10228 , but the first job was assigned process 0 , the second job was assigned process 1 , and the third one was assigned process 2 . (Programmers like to start counting from 0.) Now we can understand what the first column in the output, the job ID , represents. It is a job\u2019s cluster number, a dot ( . ), and the job\u2019s process number. So in the example above, the job ID of the second job is 10228.1 . Pop Quiz: Do you remember how to ask HTCondor to list all of the jobs from one cluster? How about one specific job ID?","title":"Running Many Jobs With One queue Statement"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-queue-n-with-output","text":"When all three jobs in your single cluster are finished, examine the resulting files. What is in the output file? What is in the error file (hopefully nothing)? What is in the log file? Look carefully at the job IDs in each event. Is this what you expected? Is it what you wanted?","title":"Using queue N With Output"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-process-to-distinguish-jobs","text":"As you saw with the experiment above, each job ended up overwriting the same output and error filenames in the submission directory. After all, we didn't tell it to behave any differently when it ran three jobs. We need a way to separate output (and error) files per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily. When processing a submit file, HTCondor will replace any instance of $(Process) with the process number of the job, for each job that is queued. For example, you can use the $(Process) variable to define a separate output file name for each job: output = my-output-file-$(Process).out queue 10 Even though the output filename is defined only once, HTCondor will create separate output filenames for each job: First job my-output-file-0.out Second job my-output-file-1.out Third job my-output-file-2.out ... ... Last (tenth) job my-output-file-9.out Let\u2019s see how this works for our program that estimates \u03c0. In your submit file, change the definitions of output and error to use $(Process) in the filename, similar to the example above. Delete any output, error, and log files from previous runs. Submit the updated file. When all three jobs are finished, examine the resulting files again. How many files are there of each type? What are their names? Is this what you expected? Is it what you wanted from the \u03c0 estimation process?","title":"Using $(Process) to Distinguish Jobs"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-cluster-to-separate-files-across-runs","text":"With $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well. In addition to $(Process) , there is also a $(Cluster) variable that you can use in your submit files. It works just like $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used with $(Process) , it can be used to separate files by run. For example, consider this output statement: output = my-output-file-$(Cluster)-$(Process).out For one particular run, it might result in output filenames like my-output-file-2444-0.out , myoutput-file-2444-1.out , myoutput-file-2444-2.out , etc. However, the next run would have different filenames, replacing 2444 with the new Cluster number of that run.","title":"Using $(Cluster) to Separate Files Across Runs"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-process-and-cluster-in-other-statements","text":"The $(Cluster) and $(Process) variables can be used in any submit file statement, although they are useful in some kinds of submit file statements and not really for others. For example, consider using $(Cluster) or $(Process) in each of the below: log transfer_input_files transfer_output_files arguments Unfortunately, HTCondor does not easily let you perform math on the $(Process) number when using it. So, for example, if you use $(Process) as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need a solution like those in the next exercises.","title":"Using $(Process) and $(Cluster) in Other Statements"},{"location":"materials/htcondor/part2-ex2-queue-n/#optional-defining-jobbatchname-for-tracking","text":"During the lecture, it was mentioned that you can define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used. Once again, we will use sleep jobs, so that your jobs remain in the queue long enough to experiment on. Create a submit file that runs sleep 120 (or some reasonable duration). Instead of a single queue statement, write this: jobbatchname = 1 queue 5 Submit the file. Now, quickly edit the submit file to instead say: jobbatchname = 2 Submit the file again. Check on the submissions using a normal condor_q and condor_q -nobatch . Of course, your special attribute does not appear in the condor_q -nobatch output, but it is present in the condor_q output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your condor_q output to one type of job or another. First, run this command: username@learn $ condor_q -constraint 'JobBatchName == \"1\"' Do you get the output that you expected? Using the example command above, how would you list your other five jobs? (More on constraints in later exercises.)","title":"(Optional) Defining JobBatchName for Tracking"},{"location":"materials/htcondor/part2-ex3-queue-from/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 2.3: Submit with \u201cqueue from\u201d \u00b6 In this exercise and the next one, you will explore more ways to use a single submit file to submit many jobs. The goal of this exercise is to submit many jobs from a single submit file by using the queue ... from syntax to read variable values from a file. In all cases of submitting many jobs from a single submit file, the key questions are: What makes each job unique? In other words, there is one job per _____? So, how should you tell HTCondor to distinguish each job? For queue *N* , jobs are distinguished simply by the built-in \"process\" variable. But with the remaining queue forms, you help HTCondor distinguish jobs by other, more meaningful custom variables. Counting Words in Files \u00b6 Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. As mentioned in the lecture, HTCondor provides many ways to submit jobs for this task. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, all five of the last lines before queue below): executable = freq.py request_memory = 1GB request_disk = 20MB should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = AAiW.txt arguments = AAiW.txt output = AAiW.out error = AAiW.err log = AAiW.log queue This would be overly verbose and tedious. Let's do better. Queue Jobs From a List of Values \u00b6 Suppose we want to modify our word-frequency analysis from a previous exercise so that it outputs only the most common N words of a document. However, we want to experiment with different values of N . For this analysis, we will have a new version of the word-frequency counting script. First, we need a new version of the word counting program so that it accepts an extra number as a command line argument and outputs only that many of the most common words. Here is the new code (it's still not important that you understand this code): #!/usr/bin/env python import os import sys import operator if len ( sys . argv ) != 3 : print 'Usage: %s DATA NUM_WORDS' % ( os . path . basename ( sys . argv [ 0 ])) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] num_words = int ( sys . argv [ 2 ]) words = {} my_file = open ( input_filename , 'r' ) for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 my_file . close () sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words [ - num_words :]: print ' %s %8d ' % ( word [ 0 ], word [ 1 ]) To submit this program with a collection of two variable values for each run, one for the number of top words and one for the filename: Save the script as wordcount-top-n.py . Download and unpack some books from Project Gutenberg: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool20/books.zip username@learn $ unzip books.zip Create a new submit file (or base it off a previous one!) named wordcount-top.sub , including memory and disk requests of 20 MB. All of the jobs will use the same executable and log statements. Update other statements to work with two variables, book and n : output = $(book)_top_$(n).out error = $(book)_top_$(n).err transfer_input_files = $(book) arguments = \"$(book) $(n)\" queue book,n from books_n.txt Note especially the changes to the queue statement; it now tells HTCondor to read a separate text file of pairs of values, which will be assigned to book and n respectively. Create the separate text file of job variable values and save it as books_n.txt : AAiW.txt, 10 AAiW.txt, 25 AAiW.txt, 50 PandP.txt, 10 PandP.txt, 25 PandP.txt, 50 TAoSH.txt, 10 TAoSH.txt, 25 TAoSH.txt, 50 Note that we used 3 different values for n for each book. Submit the file Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created? Extra Challenge 1 \u00b6 You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., AAiW.txt ), the output filenames contain with multiple extensions (e.g., AAiW.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. Change your submit file and variable file for this exercise so that the output filenames do not include the .txt extension.","title":"Exercise 2.3"},{"location":"materials/htcondor/part2-ex3-queue-from/#htc-exercise-23-submit-with-queue-from","text":"In this exercise and the next one, you will explore more ways to use a single submit file to submit many jobs. The goal of this exercise is to submit many jobs from a single submit file by using the queue ... from syntax to read variable values from a file. In all cases of submitting many jobs from a single submit file, the key questions are: What makes each job unique? In other words, there is one job per _____? So, how should you tell HTCondor to distinguish each job? For queue *N* , jobs are distinguished simply by the built-in \"process\" variable. But with the remaining queue forms, you help HTCondor distinguish jobs by other, more meaningful custom variables.","title":"HTC Exercise 2.3: Submit with \u201cqueue from\u201d"},{"location":"materials/htcondor/part2-ex3-queue-from/#counting-words-in-files","text":"Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. As mentioned in the lecture, HTCondor provides many ways to submit jobs for this task. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, all five of the last lines before queue below): executable = freq.py request_memory = 1GB request_disk = 20MB should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = AAiW.txt arguments = AAiW.txt output = AAiW.out error = AAiW.err log = AAiW.log queue This would be overly verbose and tedious. Let's do better.","title":"Counting Words in Files"},{"location":"materials/htcondor/part2-ex3-queue-from/#queue-jobs-from-a-list-of-values","text":"Suppose we want to modify our word-frequency analysis from a previous exercise so that it outputs only the most common N words of a document. However, we want to experiment with different values of N . For this analysis, we will have a new version of the word-frequency counting script. First, we need a new version of the word counting program so that it accepts an extra number as a command line argument and outputs only that many of the most common words. Here is the new code (it's still not important that you understand this code): #!/usr/bin/env python import os import sys import operator if len ( sys . argv ) != 3 : print 'Usage: %s DATA NUM_WORDS' % ( os . path . basename ( sys . argv [ 0 ])) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] num_words = int ( sys . argv [ 2 ]) words = {} my_file = open ( input_filename , 'r' ) for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 my_file . close () sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words [ - num_words :]: print ' %s %8d ' % ( word [ 0 ], word [ 1 ]) To submit this program with a collection of two variable values for each run, one for the number of top words and one for the filename: Save the script as wordcount-top-n.py . Download and unpack some books from Project Gutenberg: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool20/books.zip username@learn $ unzip books.zip Create a new submit file (or base it off a previous one!) named wordcount-top.sub , including memory and disk requests of 20 MB. All of the jobs will use the same executable and log statements. Update other statements to work with two variables, book and n : output = $(book)_top_$(n).out error = $(book)_top_$(n).err transfer_input_files = $(book) arguments = \"$(book) $(n)\" queue book,n from books_n.txt Note especially the changes to the queue statement; it now tells HTCondor to read a separate text file of pairs of values, which will be assigned to book and n respectively. Create the separate text file of job variable values and save it as books_n.txt : AAiW.txt, 10 AAiW.txt, 25 AAiW.txt, 50 PandP.txt, 10 PandP.txt, 25 PandP.txt, 50 TAoSH.txt, 10 TAoSH.txt, 25 TAoSH.txt, 50 Note that we used 3 different values for n for each book. Submit the file Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created?","title":"Queue Jobs From a List of Values"},{"location":"materials/htcondor/part2-ex3-queue-from/#extra-challenge-1","text":"You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., AAiW.txt ), the output filenames contain with multiple extensions (e.g., AAiW.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. Change your submit file and variable file for this exercise so that the output filenames do not include the .txt extension.","title":"Extra Challenge 1"},{"location":"materials/htcondor/part2-ex4-queue-matching/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 2.4: Submit With \u201cqueue matching\u201d \u00b6 The goal of this exercise is to submit many jobs from a single submit file by using the queue ... matching syntax to submit jobs with variable values derived from files in the current directory which match a specified pattern. Counting Words in Files \u00b6 Returning to our book word-counting example, let's pretend that instead of three books, we have an entire library. While we could list all of the text files in a books.txt file and use queue book from books.txt , it could be a tedious process, especially for tens of thousands of files. Luckily HTCondor provides a mechanism for submitting jobs based on pattern-matched files. Queue Jobs By Matching Filenames \u00b6 This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The queue ... matching statement is made for this scenario. Let\u2019s see this in action. First, here is a new version of the script (note, we removed the 'top n words' restriction): #!/usr/bin/env python import os import sys import operator if len ( sys . argv ) != 2 : print 'Usage: %s DATA' % ( os . path . basename ( sys . argv [ 0 ])) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} my_file = open ( input_filename , 'r' ) for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 my_file . close () sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words : print ' %s %8d ' % ( word [ 0 ], word [ 1 ]) To use the script: Save it as wordcount.py . Verify the script by running it on one book manually. Create a new submit file to submit one job (pick a book file and model your submit file off of the one above) Modify the following submit file statements to work for all books: transfer_input_files = $(book) arguments = $(book) output = $(book).out error = $(book).err queue book matching *.txt Note As always, the order of statements in a submit file does not matter, except that the queue statement should be last. Also note that any submit file variable name (here, book , but true for process and all others) may be used in any mixture of upper- and lowercase letters. Submit the jobs. HTCondor uses the queue ... matching statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., book here) is assigned the name of the matching file, so that it can be used in output , error , and other statements. The result is the same as if we had written out a much longer submit file: ... transfer_input_files = AAiW.txt arguments = \"AAiW.txt\" output = AAiW.txt.out error = AAiW.txt.err queue transfer_input_files = PandP.txt arguments = \"PandP.txt\" output = PandP.txt.out error = PandP.txt.err queue transfer_input_files = TAoSH.txt arguments = \"TAoSH.txt\" output = TAoSH.txt.out error = TAoSH.txt.err queue ... How many jobs were created? Is this what you expected? If you ran this in the same directory as Exercise 2.3, you may have noticed that a job was submitted for the books_n.txt file that holds the variable values in the queue from statement. Beware the dangers of matching more files than intended! One solution may be to put all of the books into an books directory and queue matching books/*.txt . Can you think of other solutions? If you have time, try one! Extra Challenge 1 \u00b6 In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file. Create a new submit file that works just like the one above, except that each job writes its own log file. Extra Challenge 2 \u00b6 Between this exercise and the previous one, you have explored two of the three primary queue statements. How would you use the queue in ... list statement to accomplish the same thing(s) as one or both of the exercises?","title":"Bonus Exercise 2.4"},{"location":"materials/htcondor/part2-ex4-queue-matching/#bonus-htc-exercise-24-submit-with-queue-matching","text":"The goal of this exercise is to submit many jobs from a single submit file by using the queue ... matching syntax to submit jobs with variable values derived from files in the current directory which match a specified pattern.","title":"Bonus HTC Exercise 2.4: Submit With \u201cqueue matching\u201d"},{"location":"materials/htcondor/part2-ex4-queue-matching/#counting-words-in-files","text":"Returning to our book word-counting example, let's pretend that instead of three books, we have an entire library. While we could list all of the text files in a books.txt file and use queue book from books.txt , it could be a tedious process, especially for tens of thousands of files. Luckily HTCondor provides a mechanism for submitting jobs based on pattern-matched files.","title":"Counting Words in Files"},{"location":"materials/htcondor/part2-ex4-queue-matching/#queue-jobs-by-matching-filenames","text":"This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The queue ... matching statement is made for this scenario. Let\u2019s see this in action. First, here is a new version of the script (note, we removed the 'top n words' restriction): #!/usr/bin/env python import os import sys import operator if len ( sys . argv ) != 2 : print 'Usage: %s DATA' % ( os . path . basename ( sys . argv [ 0 ])) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} my_file = open ( input_filename , 'r' ) for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 my_file . close () sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words : print ' %s %8d ' % ( word [ 0 ], word [ 1 ]) To use the script: Save it as wordcount.py . Verify the script by running it on one book manually. Create a new submit file to submit one job (pick a book file and model your submit file off of the one above) Modify the following submit file statements to work for all books: transfer_input_files = $(book) arguments = $(book) output = $(book).out error = $(book).err queue book matching *.txt Note As always, the order of statements in a submit file does not matter, except that the queue statement should be last. Also note that any submit file variable name (here, book , but true for process and all others) may be used in any mixture of upper- and lowercase letters. Submit the jobs. HTCondor uses the queue ... matching statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., book here) is assigned the name of the matching file, so that it can be used in output , error , and other statements. The result is the same as if we had written out a much longer submit file: ... transfer_input_files = AAiW.txt arguments = \"AAiW.txt\" output = AAiW.txt.out error = AAiW.txt.err queue transfer_input_files = PandP.txt arguments = \"PandP.txt\" output = PandP.txt.out error = PandP.txt.err queue transfer_input_files = TAoSH.txt arguments = \"TAoSH.txt\" output = TAoSH.txt.out error = TAoSH.txt.err queue ... How many jobs were created? Is this what you expected? If you ran this in the same directory as Exercise 2.3, you may have noticed that a job was submitted for the books_n.txt file that holds the variable values in the queue from statement. Beware the dangers of matching more files than intended! One solution may be to put all of the books into an books directory and queue matching books/*.txt . Can you think of other solutions? If you have time, try one!","title":"Queue Jobs By Matching Filenames"},{"location":"materials/htcondor/part2-ex4-queue-matching/#extra-challenge-1","text":"In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file. Create a new submit file that works just like the one above, except that each job writes its own log file.","title":"Extra Challenge 1"},{"location":"materials/htcondor/part2-ex4-queue-matching/#extra-challenge-2","text":"Between this exercise and the previous one, you have explored two of the three primary queue statements. How would you use the queue in ... list statement to accomplish the same thing(s) as one or both of the exercises?","title":"Extra Challenge 2"},{"location":"materials/htcondor/part3-ex1-queue/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 3.1: Explore condor_q \u00b6 The goal of this exercise is try out some of the most common options to the condor_q command, so that you can view jobs effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_q expert! Selecting Jobs \u00b6 The condor_q program has many options for selecting which jobs are listed. You have already seen that the default mode is to show only your jobs in \"batch\" mode: username@learn $ condor_q You've seen that you can view all jobs (all users) in the submit node's queue by using the -all argument: username@learn $ condor_q -all And you've seen that you can view more details about queued jobs, with each separate job on a single line using the -nobatch option: username@learn $ condor_q -nobatch username@learn $ condor_q -all -nobatch Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once? username@learn $ condor_q <USERNAME1> <USERNAME2> <USERNAME3> To list just the jobs associated with a single cluster number: username@learn $ condor_q <CLUSTER> For example, if you want to see the jobs in cluster 5678 (i.e., 5678.0 , 5678.1 , etc.), you use condor_q 5678 . To list a specific job (i.e., cluster.process, as in 5678.0): username@learn $ condor_q <JOB.ID> For example, to see job ID 5678.1, you use condor_q 5678.1 . Note You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all of the named clusters and/or job IDs are listed. Let\u2019s get some practice using condor_q selections! Using a previous exercise, submit several sleep jobs. List all jobs in the queue \u2014 are there others besides your own? Practice using all forms of condor_q that you have learned: List just your jobs, with and without batching. List a specific cluster. List a specific job ID. Try listing several users at once. Try listing several clusters and job IDs at once. When there are a variety of jobs in the queue, try combining a username and a different user's cluster or job ID in the same command \u2014 what happens? Viewing a Job ClassAd \u00b6 You may have wondered why it is useful to be able to list a single job ID using condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful! If you add the -long option to condor_q (or its short form, -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like: username@learn $ condor_q -long <JOB.ID> The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from condor_q output (except with some line breaks added to the Requirements attribute): MyType = \"Job\" Err = \"sleep.err\" UserLog = \"/home/cat/intro-2.1-queue/sleep.log\" Requirements = ( IsOSGSchoolSlot =?= true ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) ClusterId = 2420 WhenToTransferOutput = \"ON_EXIT\" Owner = \"cat\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Out = \"sleep.out\" Cmd = \"/bin/sleep\" Arguments = \"120\" Note Attributes are listed in no particular order and may change from time to time. Do not assume anything about the order of attributes in condor_q output. See what you can find in a job ClassAd from your own job. Using a previous exercise, submit a sleep job that sleeps for at least 3 minutes (180 seconds). Before the job executes, capture its ClassAd and save to a file: condor_q -l <JOB.ID> > classad-1.txt After the job starts execution but before it finishes, capture its ClassAd again and save to a file condor_q -l <JOB.ID> > classad-2.txt Now examine each saved ClassAd file. Here are a few things to look for: Can you find attributes that came from your submit file? (E.g., Cmd, Arguments, Out, Err, UserLog, and so forth) Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements) How many of the following attributes can you guess the meaning of? DiskUsage ImageSize BytesSent JobStatus Why Is My Job Not Running? \u00b6 Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help. To ask HTCondor why your job is not running, add the -better-analyze option to condor_q for the specific job. For example, for job ID 2423.0, the command is: username@learn $ condor_q -better-analyze 2423 .0 Of course, replace the job ID with your own. Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use: executable = /bin/hostname output = norun.out error = norun.err log = norun.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_disk = 10MB request_memory = 8TB queue (Do you see what I did?) Save and submit this file. Run condor_q -better-analyze on the job ID. There is a lot of output, but a few items are worth highlighting. Here is a sample from my own job (with some lines omitted): -- Schedd: learn.chtc.wisc.edu : <128.104.100.148:9618?... ... Job 98096.000 defines the following attributes: RequestDisk = 10240 RequestMemory = 8388608 The Requirements expression for job 98096.000 reduces to these conditions: Slots Step Matched Condition ----- -------- --------- [1] 11227 Target.OpSysMajorVer == 7 [9] 13098 TARGET.Disk >= RequestDisk [11] 0 TARGET.Memory >= RequestMemory No successful match recorded. Last failed match: Fri Jul 12 15:36:30 2019 Reason for last match failure: no match found 98096.000: Run analysis summary ignoring user priority. Of 710 machines, 710 are rejected by your job's requirements 0 reject your job because of their own requirements 0 match and are already running your jobs 0 match but are serving other users 0 are able to run your job ... At the end of the summary, condor_q provides a breakdown of how machines and their own requirements match against my own job's requirements. 710 total machines were considered above, and all of them were rejected based on my job's requirements . In other words, I am asking for something that is not available. But what? Further up in the output, there is an analysis of the job's requirements, along with how many slots within the pool match each of those requirements. The example above reports that 13098 slots match our small disk request request, but none of the slots matched the TARGET.Memory >= RequestMemory condition. The output also reports the value used for the RequestMemory attribute: my job asked for 8 terabytes of memory (8,388,608 MB) -- of course no machines matched that part of the expression! That's a lot of memory on today's machines. The output from condor_q -analyze (and condor_q -better-analyze ) may be helpful or it may not be, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching. Bonus: Automatic Formatting Output \u00b6 Do this exercise only if you have time, though it's pretty awesome! There is a way to select the specific job attributes you want condor_q to tell you about with the -autoformat or -af option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). (To tell HTCondor how to specially format this information, yourself, you could use the -format option, which we're not covering.) To use autoformatting, use the -af option followed by the attribute name, for each attribute that you want to output: username@learn $ condor_q -all -af Owner ClusterId Cmd moate 2418 /share/test.sh cat 2421 /bin/sleep cat 2422 /bin/sleep Bonus Question : If you wanted to print out the Requirements expression of a job, how would you do that with -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of plain values, you can use -af:r to view the expressions, instead of what it's current evaluation.) References \u00b6 As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_q man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in Appendix A of the HTCondor Manual","title":"Bonus Exercise 3.1"},{"location":"materials/htcondor/part3-ex1-queue/#bonus-htc-exercise-31-explore-condor_q","text":"The goal of this exercise is try out some of the most common options to the condor_q command, so that you can view jobs effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_q expert!","title":"Bonus HTC Exercise 3.1: Explore condor_q"},{"location":"materials/htcondor/part3-ex1-queue/#selecting-jobs","text":"The condor_q program has many options for selecting which jobs are listed. You have already seen that the default mode is to show only your jobs in \"batch\" mode: username@learn $ condor_q You've seen that you can view all jobs (all users) in the submit node's queue by using the -all argument: username@learn $ condor_q -all And you've seen that you can view more details about queued jobs, with each separate job on a single line using the -nobatch option: username@learn $ condor_q -nobatch username@learn $ condor_q -all -nobatch Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once? username@learn $ condor_q <USERNAME1> <USERNAME2> <USERNAME3> To list just the jobs associated with a single cluster number: username@learn $ condor_q <CLUSTER> For example, if you want to see the jobs in cluster 5678 (i.e., 5678.0 , 5678.1 , etc.), you use condor_q 5678 . To list a specific job (i.e., cluster.process, as in 5678.0): username@learn $ condor_q <JOB.ID> For example, to see job ID 5678.1, you use condor_q 5678.1 . Note You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all of the named clusters and/or job IDs are listed. Let\u2019s get some practice using condor_q selections! Using a previous exercise, submit several sleep jobs. List all jobs in the queue \u2014 are there others besides your own? Practice using all forms of condor_q that you have learned: List just your jobs, with and without batching. List a specific cluster. List a specific job ID. Try listing several users at once. Try listing several clusters and job IDs at once. When there are a variety of jobs in the queue, try combining a username and a different user's cluster or job ID in the same command \u2014 what happens?","title":"Selecting Jobs"},{"location":"materials/htcondor/part3-ex1-queue/#viewing-a-job-classad","text":"You may have wondered why it is useful to be able to list a single job ID using condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful! If you add the -long option to condor_q (or its short form, -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like: username@learn $ condor_q -long <JOB.ID> The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from condor_q output (except with some line breaks added to the Requirements attribute): MyType = \"Job\" Err = \"sleep.err\" UserLog = \"/home/cat/intro-2.1-queue/sleep.log\" Requirements = ( IsOSGSchoolSlot =?= true ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) ClusterId = 2420 WhenToTransferOutput = \"ON_EXIT\" Owner = \"cat\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Out = \"sleep.out\" Cmd = \"/bin/sleep\" Arguments = \"120\" Note Attributes are listed in no particular order and may change from time to time. Do not assume anything about the order of attributes in condor_q output. See what you can find in a job ClassAd from your own job. Using a previous exercise, submit a sleep job that sleeps for at least 3 minutes (180 seconds). Before the job executes, capture its ClassAd and save to a file: condor_q -l <JOB.ID> > classad-1.txt After the job starts execution but before it finishes, capture its ClassAd again and save to a file condor_q -l <JOB.ID> > classad-2.txt Now examine each saved ClassAd file. Here are a few things to look for: Can you find attributes that came from your submit file? (E.g., Cmd, Arguments, Out, Err, UserLog, and so forth) Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements) How many of the following attributes can you guess the meaning of? DiskUsage ImageSize BytesSent JobStatus","title":"Viewing a Job ClassAd"},{"location":"materials/htcondor/part3-ex1-queue/#why-is-my-job-not-running","text":"Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help. To ask HTCondor why your job is not running, add the -better-analyze option to condor_q for the specific job. For example, for job ID 2423.0, the command is: username@learn $ condor_q -better-analyze 2423 .0 Of course, replace the job ID with your own. Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use: executable = /bin/hostname output = norun.out error = norun.err log = norun.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_disk = 10MB request_memory = 8TB queue (Do you see what I did?) Save and submit this file. Run condor_q -better-analyze on the job ID. There is a lot of output, but a few items are worth highlighting. Here is a sample from my own job (with some lines omitted): -- Schedd: learn.chtc.wisc.edu : <128.104.100.148:9618?... ... Job 98096.000 defines the following attributes: RequestDisk = 10240 RequestMemory = 8388608 The Requirements expression for job 98096.000 reduces to these conditions: Slots Step Matched Condition ----- -------- --------- [1] 11227 Target.OpSysMajorVer == 7 [9] 13098 TARGET.Disk >= RequestDisk [11] 0 TARGET.Memory >= RequestMemory No successful match recorded. Last failed match: Fri Jul 12 15:36:30 2019 Reason for last match failure: no match found 98096.000: Run analysis summary ignoring user priority. Of 710 machines, 710 are rejected by your job's requirements 0 reject your job because of their own requirements 0 match and are already running your jobs 0 match but are serving other users 0 are able to run your job ... At the end of the summary, condor_q provides a breakdown of how machines and their own requirements match against my own job's requirements. 710 total machines were considered above, and all of them were rejected based on my job's requirements . In other words, I am asking for something that is not available. But what? Further up in the output, there is an analysis of the job's requirements, along with how many slots within the pool match each of those requirements. The example above reports that 13098 slots match our small disk request request, but none of the slots matched the TARGET.Memory >= RequestMemory condition. The output also reports the value used for the RequestMemory attribute: my job asked for 8 terabytes of memory (8,388,608 MB) -- of course no machines matched that part of the expression! That's a lot of memory on today's machines. The output from condor_q -analyze (and condor_q -better-analyze ) may be helpful or it may not be, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching.","title":"Why Is My Job Not Running?"},{"location":"materials/htcondor/part3-ex1-queue/#bonus-automatic-formatting-output","text":"Do this exercise only if you have time, though it's pretty awesome! There is a way to select the specific job attributes you want condor_q to tell you about with the -autoformat or -af option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). (To tell HTCondor how to specially format this information, yourself, you could use the -format option, which we're not covering.) To use autoformatting, use the -af option followed by the attribute name, for each attribute that you want to output: username@learn $ condor_q -all -af Owner ClusterId Cmd moate 2418 /share/test.sh cat 2421 /bin/sleep cat 2422 /bin/sleep Bonus Question : If you wanted to print out the Requirements expression of a job, how would you do that with -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of plain values, you can use -af:r to view the expressions, instead of what it's current evaluation.)","title":"Bonus: Automatic Formatting Output"},{"location":"materials/htcondor/part3-ex1-queue/#references","text":"As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_q man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in Appendix A of the HTCondor Manual","title":"References"},{"location":"materials/htcondor/part3-ex2-status/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 3.2: Explore condor_status \u00b6 The goal of this exercise is try out some of the most common options to the condor_status command, so that you can view slots effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_status expert! Selecting Slots \u00b6 The condor_status program has many options for selecting which slots are listed. You've already learned the basic condor_status and the condor_status -compact variation (which you may wish to retry now, before proceeding). Another convenient option is to list only those slots that are available now: username@learn $ condor_status -avail Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all condor_status output, not just with the -avail option. Similar to condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine: username@learn $ condor_status <hostname> For example, if you want to see the slots on e2337.chtc.wisc.edu (in the CHTC pool): username@learn $ condor_status e2337.chtc.wisc.edu To list a specific slot on a machine: username@learn $ condor_status <slot>@<hostname> For example, to see the \u201cfirst\u201d slot on the machine above: username@learn $ condor_status slot1@e2337.chtc.wisc.edu Note You can name more than one hostname, slot, or combination thereof on the command line, in which case slots for all of the named hostnames and/or slots are listed. Let\u2019s get some practice using condor_status selections! List all slots in the pool \u2014 how many are there total? Practice using all forms of condor_status that you have learned: List the available slots. List the slots on a specific machine (e.g., e2337.chtc.wisc.edu ). List a specific slot from that machine. Try listing the slots from a few (but not all) machines at once. Try using a mix of hostnames and slot IDs at once. Viewing a Slot ClassAd \u00b6 Just as with condor_q , you can use condor_status to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad): username@learn $ condor_status -long <slot>@<hostname> Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above. Here are some examples of common, interesting attributes taken directly from condor_status output: OpSys = \"LINUX\" DetectedCpus = 24 OpSysAndVer = \"SL6\" MyType = \"Machine\" LoadAvg = 0.99 TotalDisk = 798098404 OSIssue = \"Scientific Linux release 6.6 (Carbon)\" TotalMemory = 24016 Machine = \"e242.chtc.wisc.edu\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Memory = 1024 As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular. Go ahead and examine a machine ClassAd now. I suggest looking at one of the slots on, say, e2337.chtc.wisc.edu because of its relatively simple configuration. Viewing Slots by ClassAd Expression \u00b6 Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the -constraint option and a ClassAd expression. For example, suppose we want to list all slots that are running Scientific Linux 7 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is: username@learn $ condor_status -constraint 'OpSysAndVer == \"CentOS7\" && Memory >= 16000' Note Be very careful with using quote characters appropriately in these commands. In the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status untouched, and the double quotes ( \" ) surround a string value within the expression itself. Currently on CHTC, there are only a few slots that meet these criteria (our high-memory servers, mainly used for metagenomics assemblies). If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is definitely advanced material, so if you do not want to read it, that is fine. But if you do, take some time to practice writing expressions for the condor_status -constraint command. Note The condor_q command accepts the -constraint option as well! As you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression. Bonus: Formatting Output \u00b6 The condor_status command accepts the same -autoformat ( -af ) options that condor_q accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available. For example, I was curious about the host name and operating system of the slots with more than 32GB of memory: username@learn $ condor_status -af Machine -af OpSysAndVer -constraint 'Memory >= 32000' If you like, spend a few minutes now or later experimenting with condor_status formatting. References \u00b6 As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_status man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in the appendix of the HTCondor Manual Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual","title":"Bonus Exercise 3.2"},{"location":"materials/htcondor/part3-ex2-status/#bonus-htc-exercise-32-explore-condor_status","text":"The goal of this exercise is try out some of the most common options to the condor_status command, so that you can view slots effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_status expert!","title":"Bonus HTC Exercise 3.2: Explore condor_status"},{"location":"materials/htcondor/part3-ex2-status/#selecting-slots","text":"The condor_status program has many options for selecting which slots are listed. You've already learned the basic condor_status and the condor_status -compact variation (which you may wish to retry now, before proceeding). Another convenient option is to list only those slots that are available now: username@learn $ condor_status -avail Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all condor_status output, not just with the -avail option. Similar to condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine: username@learn $ condor_status <hostname> For example, if you want to see the slots on e2337.chtc.wisc.edu (in the CHTC pool): username@learn $ condor_status e2337.chtc.wisc.edu To list a specific slot on a machine: username@learn $ condor_status <slot>@<hostname> For example, to see the \u201cfirst\u201d slot on the machine above: username@learn $ condor_status slot1@e2337.chtc.wisc.edu Note You can name more than one hostname, slot, or combination thereof on the command line, in which case slots for all of the named hostnames and/or slots are listed. Let\u2019s get some practice using condor_status selections! List all slots in the pool \u2014 how many are there total? Practice using all forms of condor_status that you have learned: List the available slots. List the slots on a specific machine (e.g., e2337.chtc.wisc.edu ). List a specific slot from that machine. Try listing the slots from a few (but not all) machines at once. Try using a mix of hostnames and slot IDs at once.","title":"Selecting Slots"},{"location":"materials/htcondor/part3-ex2-status/#viewing-a-slot-classad","text":"Just as with condor_q , you can use condor_status to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad): username@learn $ condor_status -long <slot>@<hostname> Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above. Here are some examples of common, interesting attributes taken directly from condor_status output: OpSys = \"LINUX\" DetectedCpus = 24 OpSysAndVer = \"SL6\" MyType = \"Machine\" LoadAvg = 0.99 TotalDisk = 798098404 OSIssue = \"Scientific Linux release 6.6 (Carbon)\" TotalMemory = 24016 Machine = \"e242.chtc.wisc.edu\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Memory = 1024 As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular. Go ahead and examine a machine ClassAd now. I suggest looking at one of the slots on, say, e2337.chtc.wisc.edu because of its relatively simple configuration.","title":"Viewing a Slot ClassAd"},{"location":"materials/htcondor/part3-ex2-status/#viewing-slots-by-classad-expression","text":"Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the -constraint option and a ClassAd expression. For example, suppose we want to list all slots that are running Scientific Linux 7 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is: username@learn $ condor_status -constraint 'OpSysAndVer == \"CentOS7\" && Memory >= 16000' Note Be very careful with using quote characters appropriately in these commands. In the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status untouched, and the double quotes ( \" ) surround a string value within the expression itself. Currently on CHTC, there are only a few slots that meet these criteria (our high-memory servers, mainly used for metagenomics assemblies). If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is definitely advanced material, so if you do not want to read it, that is fine. But if you do, take some time to practice writing expressions for the condor_status -constraint command. Note The condor_q command accepts the -constraint option as well! As you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression.","title":"Viewing Slots by ClassAd Expression"},{"location":"materials/htcondor/part3-ex2-status/#bonus-formatting-output","text":"The condor_status command accepts the same -autoformat ( -af ) options that condor_q accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available. For example, I was curious about the host name and operating system of the slots with more than 32GB of memory: username@learn $ condor_status -af Machine -af OpSysAndVer -constraint 'Memory >= 32000' If you like, spend a few minutes now or later experimenting with condor_status formatting.","title":"Bonus: Formatting Output"},{"location":"materials/htcondor/part3-ex2-status/#references","text":"As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_status man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in the appendix of the HTCondor Manual Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual","title":"References"},{"location":"materials/htcondor/part3-ex3-job-retry/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 3.3: Retries \u00b6 The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it. This first part of the exercise should take only a few minutes, and is designed to setup the next exercises. Bad Job \u00b6 Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like. Below is a Python script that fails once in a while. We will not fix it, but instead use it to simulate a program that can fail and that we cannot fix. #!/usr/bin/env python3 # murphy.py simulates a real program with real problems import random import sys import time # For one out of every three attempts, simulate a runtime error if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. In a new directory for this exercise, save the script above as murphy.py . Write a submit file for the script; queue 20 instances of the job and be sure to ask for 20 MB of memory and disk. Submit the file, note the ClusterId, and wait for the jobs to finish. What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute. The following command will show the ExitCode for a given cluster of jobs: username@learn $ condor_history <CLUSTER> -af ProcId ExitCode (Be sure to replace <cluster> with your actual cluster ID. The command may take a minute or so to complete.) How many of the jobs succeeded? How many failed? Retrying Failed Jobs \u00b6 Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption. From the lecture materials, implement the max_retries feature to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Did your change work? After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the NumJobStarts attribute of a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their ExitCode ; and what about the ExitCode from earlier execution attempts? A (Too) Long Running Job \u00b6 Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code, and it may just need to be re-run (or run on a different execute server) to complete without getting stuck. We can modify our Python program to simulate this kind of bad job with the following file: #!/usr/bin/env python3 # murphy.py simulate a real program with real problems import random import sys import time # For one out of every three attempts, simulate an \"infinite\" loop if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output time . sleep ( 3600 ) sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. Save the script to a new file named murphy2.py . Copy your previous submit file to a new name and change the executable to murphy2.py . If you like, submit the new file \u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs. Now try to change the submit file to automatically remove any jobs that run for more than one minute. You can make this change with just a single line in your submit file periodic_remove = (JobStatus == 2) && ( (CurrentTime - EnteredCurrentStatus) > 60 ) Submit the new file. Do the long running jobs get removed? What does condor_history show for the cluster after all jobs are done? Which job status (i.e. idle, held, running) do you think JobStatus == 2 corresponds to? Bonus Exercise \u00b6 If you have time, edit your submit file so that instead of removing long running jobs, HTCondor will automatically put the long-running job on hold, and then automatically release it.","title":"Bonus Exercise 3.3"},{"location":"materials/htcondor/part3-ex3-job-retry/#bonus-htc-exercise-33-retries","text":"The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it. This first part of the exercise should take only a few minutes, and is designed to setup the next exercises.","title":"Bonus HTC Exercise 3.3: Retries"},{"location":"materials/htcondor/part3-ex3-job-retry/#bad-job","text":"Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like. Below is a Python script that fails once in a while. We will not fix it, but instead use it to simulate a program that can fail and that we cannot fix. #!/usr/bin/env python3 # murphy.py simulates a real program with real problems import random import sys import time # For one out of every three attempts, simulate a runtime error if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. In a new directory for this exercise, save the script above as murphy.py . Write a submit file for the script; queue 20 instances of the job and be sure to ask for 20 MB of memory and disk. Submit the file, note the ClusterId, and wait for the jobs to finish. What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute. The following command will show the ExitCode for a given cluster of jobs: username@learn $ condor_history <CLUSTER> -af ProcId ExitCode (Be sure to replace <cluster> with your actual cluster ID. The command may take a minute or so to complete.) How many of the jobs succeeded? How many failed?","title":"Bad Job"},{"location":"materials/htcondor/part3-ex3-job-retry/#retrying-failed-jobs","text":"Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption. From the lecture materials, implement the max_retries feature to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Did your change work? After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the NumJobStarts attribute of a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their ExitCode ; and what about the ExitCode from earlier execution attempts?","title":"Retrying Failed Jobs"},{"location":"materials/htcondor/part3-ex3-job-retry/#a-too-long-running-job","text":"Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code, and it may just need to be re-run (or run on a different execute server) to complete without getting stuck. We can modify our Python program to simulate this kind of bad job with the following file: #!/usr/bin/env python3 # murphy.py simulate a real program with real problems import random import sys import time # For one out of every three attempts, simulate an \"infinite\" loop if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output time . sleep ( 3600 ) sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. Save the script to a new file named murphy2.py . Copy your previous submit file to a new name and change the executable to murphy2.py . If you like, submit the new file \u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs. Now try to change the submit file to automatically remove any jobs that run for more than one minute. You can make this change with just a single line in your submit file periodic_remove = (JobStatus == 2) && ( (CurrentTime - EnteredCurrentStatus) > 60 ) Submit the new file. Do the long running jobs get removed? What does condor_history show for the cluster after all jobs are done? Which job status (i.e. idle, held, running) do you think JobStatus == 2 corresponds to?","title":"A (Too) Long Running Job"},{"location":"materials/htcondor/part3-ex3-job-retry/#bonus-exercise","text":"If you have time, edit your submit file so that instead of removing long running jobs, HTCondor will automatically put the long-running job on hold, and then automatically release it.","title":"Bonus Exercise"},{"location":"materials/osg/part1-ex1-submit-refresher/","text":"OSG Exercise 1: Refresher \u2014 Submitting Multiple Jobs \u00b6 The goal of this exercise is to map the physical locations of some worker servers in our local cluster. We will provide the executable and associated data, so your job will be to write a submit file that queues multiple jobs. Once complete, you will manually collate the results. Where in the world are my jobs? \u00b6 To find the physical location of the computers your jobs our running on, you will use a method called geolocation . Geolocation uses a registry to match a computer\u2019s network address to an approximate latitude and longitude. Geolocating several servers \u00b6 Now, let\u2019s try to remember some basic HTCondor ideas from the HTC exercises: Log in to learn.chtc.wisc.edu (yes, still at CHTC!) Create and change into a new folder for this exercise, for example osg-ex1 Download the geolocation code: user@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool20/location-wrapper.sh \\ http://proxy.chtc.wisc.edu/SQUID/osgschool20/wn-geoip.tar.gz You will be using location-wrapper.sh as your executable and wn-geoip.tar.gz as an input file. Create a submit file that queues fifty jobs that run location-wrapper.sh , transfers wn-geoip.tar.gz as an input file, and uses the $(Process) macro to write different output and error files. Also, add the following requirement to the submit file (it\u2019s not important to know what it does): requirements = (HAS_CVMFS_oasis_opensciencegrid_org == TRUE) && (IsOsgVoContainer != True) Try to do this step without looking at materials from the earlier exercises. But if you are stuck, see HTC Exercise 2.2 . Submit your jobs and wait for the results Collating your results \u00b6 Now that you have your results, it\u2019s time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format location-#.out (e.g., location-10.out ), your command will look something like this: user@learn $ cat location-*.out The * is a wildcard so the above cat command runs on all files that start with location- and end in .out . Additionally, you can use cat in combination with the sort and uniq commands using \"pipes\" ( | ) to print only the unique results: user@learn $ cat location-*.out | sort | uniq Mapping your results \u00b6 To visualize the locations of the servers that your jobs ran on, you will be using http://www.mapcustomizer.com/ . Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Where did your jobs run? Next exercise \u00b6 Once completed, move onto the next exercise: Logging in to the OSG submit server","title":"Exercise 1.1"},{"location":"materials/osg/part1-ex1-submit-refresher/#osg-exercise-1-refresher-submitting-multiple-jobs","text":"The goal of this exercise is to map the physical locations of some worker servers in our local cluster. We will provide the executable and associated data, so your job will be to write a submit file that queues multiple jobs. Once complete, you will manually collate the results.","title":"OSG Exercise 1: Refresher \u2014 Submitting Multiple Jobs"},{"location":"materials/osg/part1-ex1-submit-refresher/#where-in-the-world-are-my-jobs","text":"To find the physical location of the computers your jobs our running on, you will use a method called geolocation . Geolocation uses a registry to match a computer\u2019s network address to an approximate latitude and longitude.","title":"Where in the world are my jobs?"},{"location":"materials/osg/part1-ex1-submit-refresher/#geolocating-several-servers","text":"Now, let\u2019s try to remember some basic HTCondor ideas from the HTC exercises: Log in to learn.chtc.wisc.edu (yes, still at CHTC!) Create and change into a new folder for this exercise, for example osg-ex1 Download the geolocation code: user@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool20/location-wrapper.sh \\ http://proxy.chtc.wisc.edu/SQUID/osgschool20/wn-geoip.tar.gz You will be using location-wrapper.sh as your executable and wn-geoip.tar.gz as an input file. Create a submit file that queues fifty jobs that run location-wrapper.sh , transfers wn-geoip.tar.gz as an input file, and uses the $(Process) macro to write different output and error files. Also, add the following requirement to the submit file (it\u2019s not important to know what it does): requirements = (HAS_CVMFS_oasis_opensciencegrid_org == TRUE) && (IsOsgVoContainer != True) Try to do this step without looking at materials from the earlier exercises. But if you are stuck, see HTC Exercise 2.2 . Submit your jobs and wait for the results","title":"Geolocating several servers"},{"location":"materials/osg/part1-ex1-submit-refresher/#collating-your-results","text":"Now that you have your results, it\u2019s time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format location-#.out (e.g., location-10.out ), your command will look something like this: user@learn $ cat location-*.out The * is a wildcard so the above cat command runs on all files that start with location- and end in .out . Additionally, you can use cat in combination with the sort and uniq commands using \"pipes\" ( | ) to print only the unique results: user@learn $ cat location-*.out | sort | uniq","title":"Collating your results"},{"location":"materials/osg/part1-ex1-submit-refresher/#mapping-your-results","text":"To visualize the locations of the servers that your jobs ran on, you will be using http://www.mapcustomizer.com/ . Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Where did your jobs run?","title":"Mapping your results"},{"location":"materials/osg/part1-ex1-submit-refresher/#next-exercise","text":"Once completed, move onto the next exercise: Logging in to the OSG submit server","title":"Next exercise"},{"location":"materials/osg/part1-ex2-login-scp/","text":"OSG Exercise 2: Log In to the OSG Submit Server \u00b6 The main goal of this exercise is to log in to an Open Science Pool Access Point so that you can start submitting jobs into the OS Pool instead of the local cluster at UW\u2013Madison. But before doing that, you will prepare a file on learn to copy to the OS Pool Access Point. And will learn how to efficiently copy files between the CHTC and OS Pool Access Points. If you have trouble getting ssh access to the OS Pool Access Point, ask the instructors right away! Gaining access is critical for all remaining exercises. Part 1: On the CHTC Access Point \u00b6 The first few sections below are to be completed on learn.chtc.wisc.edu , the UW\u2013Madison CHTC Access Point. This is still the same Access Point you have been using since yesterday. Preparing files for transfer \u00b6 When transferring files between computers, it\u2019s best to limit the number of files as well as their size. Smaller files transfer more quickly and, if your network connection fails, restarting the transfer is less painful than it would be if you were transferring large files. Archiving tools (WinZip, 7zip, Archive Utility, etc.) can compress the size of your files and place them into a single, smaller archive file. The Unix tar command is a one-stop shop for creating, extracting, and viewing the contents of tar archives (called tarballs ). Its usage is as follows: To create a tarball named <archive filename> containing <archive contents> , use the following command: user@learn $ tar -czvf <archive filename> <archive contents> Where <archive filename> should end in .tar.gz and <archive contents> can be a list of any number of files and/or folders, separated by spaces. To extract the files from a tarball into the current directory: user@learn $ tar -xzvf <archive filename> To list the files within a tarball: user@learn $ tar -tzvf <archive filename> Using the guidance above, log into learn.chtc.wisc.edu , create a tarball that contains the OSG exercise 1.1 directory, and verify that it contains all the proper files. Comparing compressed sizes \u00b6 You can adjust the level of compression of tar by prepending your command with GZIP=--<COMPRESSION> , where <COMPRESSION> can be either fast for the least compression, or best for the most compression (the default compression is between best and fast ). While still logged in to learn.chtc.wisc.edu : Create and change into a new folder for this exercise, for example osg-ex2 Use wget to download the following files from our web server: Text file: http://proxy.chtc.wisc.edu/SQUID/osgschool21/random_text Archive: http://proxy.chtc.wisc.edu/SQUID/osgschool21/pdbaa.tar.gz Image: http://proxy.chtc.wisc.edu/SQUID/osgschool21/obligatory_cat.jpg Use tar on each file and use ls -l to compare the sizes of the original file and the compressed version. Which files were compressed the least? Why? Part 2: On the Open Science Pool Access Point \u00b6 For many of the remaining exercises, you will be using an OSG Connect Access Point, which submits jobs into the Open Science Pool. For the School, the default server is named login04.osgconnect.net ; however, if you had an OSG Connect account from before the School (you know who you are), you may be using login05.osgconnect.net , so just change the examples as needed. To log in to the OSG Connect Access Point, use the username and SSH key that you made when you set up OSG Connect. If you have any issues logging in to login04 (or login05 , if that\u2019s you), please ask for help right away! So please ssh in to the server and take a look around: Log in using ssh login04.osgconnect.net (or login05 , if that\u2019s you) Try some Linux and HTCondor commands; for example: Linux commands: hostname , pwd , ls , and so on What is the operating system? uname and (in this case) cat /etc/redhat-release HTCondor commands: condor_version , condor_q , condor_status -total Transferring files \u00b6 In the next exercise, you will submit the same kind of job as in the previous exercise. Wouldn\u2019t it be nice to copy the files instead of starting from scratch? And in general, being able to copy files between servers is helpful, so let\u2019s explore a way to do that. Using secure copy \u00b6 Secure copy ( scp ) is a command based on SSH that lets you securely copy files between two different servers. It takes similar arguments to the Unix cp command but also takes additional information about servers. Its general form is like this: scp <source 1> <source 2>...<source N> [username@]<remote server>:<remote path> <remote path> may be omitted if you want to copy your sources to your remote home directory and [username@] may be omitted if your usernames are the same across both servers. For example, if you are logged in to login04.osgconnect.net and wanted to copy the file foo from your current directory to your home directory on learn.chtc.wisc.edu , and if your usernames are the same on both servers, the command would look like this: user@login04 $ scp foo learn.chtc.wisc.edu: Additionally, you could pull files from learn.chtc.wisc.edu to login04.osgconnect.net . The following command copies bar from your home directory on learn.chtc.wisc.edu to your current directory on login04.osgconnect.net ; and in this case, the username (Net ID) for learn is specified: user@login04 $ scp net_id@learn.chtc.wisc.edu:bar . Also, you can copy folders between servers using the -r option. If you kept all your files from the HTCondor exercise 1.3 in a folder named htc-1.3 on learn.chtc.wisc.edu , you could use the following command to copy them to your home directory on login04.osgconnect.net : user@login04 $ scp -r net_id@learn.chtc.wisc.edu:htc-1.3 . Using this information, try this: From login04.osgconnect.net , try copying the tarball you created earlier in this exercise on learn.chtc.wisc.edu to login04.osgconnect.net . Secure copy to your laptop \u00b6 During your research, you may need to transfer output files from your submit server to inspect them on your personal computer, which can also be done with scp ! To use scp on your laptop, follow the instructions relevant to your computer\u2018s operating system: Mac and Linux users \u00b6 scp should be included by default and available via the terminal on both Mac and Linux operating systems. Open a terminal window on your laptop and try copying the tarball containing the OSG exercise 1.1 from login04.osgconnect.net to your laptop. Windows users \u00b6 WinSCP is an scp client for Windows operating systems. Install WinSCP from https://winscp.net/eng/index.php Start WinSCP and enter your SSH credentials for login04.osgconnect.net Copy the tarball containing OSG exercise 1.1 to your laptop Extra challenge: Using rsync \u00b6 (This last section is about a more advanced tool; you may skip this section if you want.) scp is a common and useful tool for file transfers between computers, but there are better tools if you find yourself transferring the same set of files to the same location repeatedly. Another common tool available on many Linux servers is rsync , which has more features than scp and is correspondingly more complex. The invocation is similar to scp : You can transfer files and/or folders, but the options are different and, when transferring folders, pay close attention to a trailing slash ( / ), because it means different things to include or omit that single character! Here is the general format of an rsync command: rsync -Pavz <source 1> <source 2>...<source N> [username@]<remote server>:<remote path> rsync has many benefits over scp , but two of its biggest features are built-in compression (so you don't have to create a tarball) and the ability to only transfer files that have changed. Both of these features are helpful when you have network issues so that you do not need to restart the transfer from scratch every time your connection fails. Log in to login04.osgconnect.net Use rsync to transfer the folder containing OSG exercise 1.1 on learn.chtc.wisc.edu to login04.osgconnect.net In a separate terminal window, log in to learn.chtc.wisc.edu Create a new file in your OSG exercise 1.1 folder on learn.chtc.wisc.edu with the touch command: user@learn $ touch <filename> From login04.osgconnect.net , use the same rsync command to transfer the folder with the new file you just created. How many files were transferred the first time? How many files were transferred if you run the same rsync command again? Next exercise \u00b6 Once completed, move onto the next exercise: Running jobs in the OSG","title":"Exercise 1.2"},{"location":"materials/osg/part1-ex2-login-scp/#osg-exercise-2-log-in-to-the-osg-submit-server","text":"The main goal of this exercise is to log in to an Open Science Pool Access Point so that you can start submitting jobs into the OS Pool instead of the local cluster at UW\u2013Madison. But before doing that, you will prepare a file on learn to copy to the OS Pool Access Point. And will learn how to efficiently copy files between the CHTC and OS Pool Access Points. If you have trouble getting ssh access to the OS Pool Access Point, ask the instructors right away! Gaining access is critical for all remaining exercises.","title":"OSG Exercise 2: Log In to the OSG Submit Server"},{"location":"materials/osg/part1-ex2-login-scp/#part-1-on-the-chtc-access-point","text":"The first few sections below are to be completed on learn.chtc.wisc.edu , the UW\u2013Madison CHTC Access Point. This is still the same Access Point you have been using since yesterday.","title":"Part 1: On the CHTC Access Point"},{"location":"materials/osg/part1-ex2-login-scp/#preparing-files-for-transfer","text":"When transferring files between computers, it\u2019s best to limit the number of files as well as their size. Smaller files transfer more quickly and, if your network connection fails, restarting the transfer is less painful than it would be if you were transferring large files. Archiving tools (WinZip, 7zip, Archive Utility, etc.) can compress the size of your files and place them into a single, smaller archive file. The Unix tar command is a one-stop shop for creating, extracting, and viewing the contents of tar archives (called tarballs ). Its usage is as follows: To create a tarball named <archive filename> containing <archive contents> , use the following command: user@learn $ tar -czvf <archive filename> <archive contents> Where <archive filename> should end in .tar.gz and <archive contents> can be a list of any number of files and/or folders, separated by spaces. To extract the files from a tarball into the current directory: user@learn $ tar -xzvf <archive filename> To list the files within a tarball: user@learn $ tar -tzvf <archive filename> Using the guidance above, log into learn.chtc.wisc.edu , create a tarball that contains the OSG exercise 1.1 directory, and verify that it contains all the proper files.","title":"Preparing files for transfer"},{"location":"materials/osg/part1-ex2-login-scp/#comparing-compressed-sizes","text":"You can adjust the level of compression of tar by prepending your command with GZIP=--<COMPRESSION> , where <COMPRESSION> can be either fast for the least compression, or best for the most compression (the default compression is between best and fast ). While still logged in to learn.chtc.wisc.edu : Create and change into a new folder for this exercise, for example osg-ex2 Use wget to download the following files from our web server: Text file: http://proxy.chtc.wisc.edu/SQUID/osgschool21/random_text Archive: http://proxy.chtc.wisc.edu/SQUID/osgschool21/pdbaa.tar.gz Image: http://proxy.chtc.wisc.edu/SQUID/osgschool21/obligatory_cat.jpg Use tar on each file and use ls -l to compare the sizes of the original file and the compressed version. Which files were compressed the least? Why?","title":"Comparing compressed sizes"},{"location":"materials/osg/part1-ex2-login-scp/#part-2-on-the-open-science-pool-access-point","text":"For many of the remaining exercises, you will be using an OSG Connect Access Point, which submits jobs into the Open Science Pool. For the School, the default server is named login04.osgconnect.net ; however, if you had an OSG Connect account from before the School (you know who you are), you may be using login05.osgconnect.net , so just change the examples as needed. To log in to the OSG Connect Access Point, use the username and SSH key that you made when you set up OSG Connect. If you have any issues logging in to login04 (or login05 , if that\u2019s you), please ask for help right away! So please ssh in to the server and take a look around: Log in using ssh login04.osgconnect.net (or login05 , if that\u2019s you) Try some Linux and HTCondor commands; for example: Linux commands: hostname , pwd , ls , and so on What is the operating system? uname and (in this case) cat /etc/redhat-release HTCondor commands: condor_version , condor_q , condor_status -total","title":"Part 2: On the Open Science Pool Access Point"},{"location":"materials/osg/part1-ex2-login-scp/#transferring-files","text":"In the next exercise, you will submit the same kind of job as in the previous exercise. Wouldn\u2019t it be nice to copy the files instead of starting from scratch? And in general, being able to copy files between servers is helpful, so let\u2019s explore a way to do that.","title":"Transferring files"},{"location":"materials/osg/part1-ex2-login-scp/#using-secure-copy","text":"Secure copy ( scp ) is a command based on SSH that lets you securely copy files between two different servers. It takes similar arguments to the Unix cp command but also takes additional information about servers. Its general form is like this: scp <source 1> <source 2>...<source N> [username@]<remote server>:<remote path> <remote path> may be omitted if you want to copy your sources to your remote home directory and [username@] may be omitted if your usernames are the same across both servers. For example, if you are logged in to login04.osgconnect.net and wanted to copy the file foo from your current directory to your home directory on learn.chtc.wisc.edu , and if your usernames are the same on both servers, the command would look like this: user@login04 $ scp foo learn.chtc.wisc.edu: Additionally, you could pull files from learn.chtc.wisc.edu to login04.osgconnect.net . The following command copies bar from your home directory on learn.chtc.wisc.edu to your current directory on login04.osgconnect.net ; and in this case, the username (Net ID) for learn is specified: user@login04 $ scp net_id@learn.chtc.wisc.edu:bar . Also, you can copy folders between servers using the -r option. If you kept all your files from the HTCondor exercise 1.3 in a folder named htc-1.3 on learn.chtc.wisc.edu , you could use the following command to copy them to your home directory on login04.osgconnect.net : user@login04 $ scp -r net_id@learn.chtc.wisc.edu:htc-1.3 . Using this information, try this: From login04.osgconnect.net , try copying the tarball you created earlier in this exercise on learn.chtc.wisc.edu to login04.osgconnect.net .","title":"Using secure copy"},{"location":"materials/osg/part1-ex2-login-scp/#secure-copy-to-your-laptop","text":"During your research, you may need to transfer output files from your submit server to inspect them on your personal computer, which can also be done with scp ! To use scp on your laptop, follow the instructions relevant to your computer\u2018s operating system:","title":"Secure copy to your laptop"},{"location":"materials/osg/part1-ex2-login-scp/#mac-and-linux-users","text":"scp should be included by default and available via the terminal on both Mac and Linux operating systems. Open a terminal window on your laptop and try copying the tarball containing the OSG exercise 1.1 from login04.osgconnect.net to your laptop.","title":"Mac and Linux users"},{"location":"materials/osg/part1-ex2-login-scp/#windows-users","text":"WinSCP is an scp client for Windows operating systems. Install WinSCP from https://winscp.net/eng/index.php Start WinSCP and enter your SSH credentials for login04.osgconnect.net Copy the tarball containing OSG exercise 1.1 to your laptop","title":"Windows users"},{"location":"materials/osg/part1-ex2-login-scp/#extra-challenge-using-rsync","text":"(This last section is about a more advanced tool; you may skip this section if you want.) scp is a common and useful tool for file transfers between computers, but there are better tools if you find yourself transferring the same set of files to the same location repeatedly. Another common tool available on many Linux servers is rsync , which has more features than scp and is correspondingly more complex. The invocation is similar to scp : You can transfer files and/or folders, but the options are different and, when transferring folders, pay close attention to a trailing slash ( / ), because it means different things to include or omit that single character! Here is the general format of an rsync command: rsync -Pavz <source 1> <source 2>...<source N> [username@]<remote server>:<remote path> rsync has many benefits over scp , but two of its biggest features are built-in compression (so you don't have to create a tarball) and the ability to only transfer files that have changed. Both of these features are helpful when you have network issues so that you do not need to restart the transfer from scratch every time your connection fails. Log in to login04.osgconnect.net Use rsync to transfer the folder containing OSG exercise 1.1 on learn.chtc.wisc.edu to login04.osgconnect.net In a separate terminal window, log in to learn.chtc.wisc.edu Create a new file in your OSG exercise 1.1 folder on learn.chtc.wisc.edu with the touch command: user@learn $ touch <filename> From login04.osgconnect.net , use the same rsync command to transfer the folder with the new file you just created. How many files were transferred the first time? How many files were transferred if you run the same rsync command again?","title":"Extra challenge: Using rsync"},{"location":"materials/osg/part1-ex2-login-scp/#next-exercise","text":"Once completed, move onto the next exercise: Running jobs in the OSG","title":"Next exercise"},{"location":"materials/osg/part1-ex3-submit-osg/","text":"OSG Exercise 3: Running Jobs in OSG \u00b6 The goal of this exercise is for you to run jobs in OSG, specifically the Open Science Pool, and map their geographical locations. Where in the world are my jobs? (Part 2) \u00b6 In this version of the geolocating exercise, you will submit jobs to the OS Pool from login04.osgconnect.net and hopefully get back interesting results! You will use the same job (software) as you did in OSG exercise 1.1 . Gathering network information from the OSG \u00b6 As a preliminary step, you will create a submit file that will run in the OS Pool! If not already logged in, ssh into login04.osgconnect.net Set the default project associated with your account by running the following command: connect project If you are given a choice and do not know which project to select, reach out to OSG staff. The default project will stay in effect until changed at a later time. Make a new directory for this exercise, osg-ex3 and change into it Copy files from learn (if not done already) Use scp or rsync from OSG exercise 1.2 to copy the executable and input file from the osg-ex1 directory from learn ; or, if you copied the OSG exercise 1.1 tarball earlier, extract the files from it. Copy or re-create the submit file from OSG exercise 1.1, except this time around change your submit file so that it submits five hundred jobs! Do you think you should test a smaller number of jobs first and scale up? Submit your file and wait for the results Mapping your jobs \u00b6 As before, you will be using https://www.mapcustomizer.com/ to visualize where your jobs have landed in the OSG. Copy and paste the collated results from your job output into the Bulk Entry area. Where did your jobs end up? Next exercise \u00b6 Once completed, move onto the next exercise: Hardware Differences in the OSG Extra Challenge: Cleaning up your submit directory \u00b6 If you run ls in the directory from which you submitted your job, you may see that you now have thousands of files! Proper data management starts to become a requirement as you start to develop truly HTC workflows; it may be helpful to separate your submit files, code, and input data from your output data. Try editing your submit file so that all your output and error files are saved to separate directories within your submit directory. Tip Experiment with fewer job submissions until you\u2019re confident you have it right, then go back to submitting 500 jobs. Remember: Test small and scale up! Submit your file and track the status of your jobs. Did your jobs complete successfully with output and error files saved in separate directories? If not, can you find any useful information in the job logs or hold messages? If you get stuck, review the slides from Tuesday .","title":"Exercise 1.3"},{"location":"materials/osg/part1-ex3-submit-osg/#osg-exercise-3-running-jobs-in-osg","text":"The goal of this exercise is for you to run jobs in OSG, specifically the Open Science Pool, and map their geographical locations.","title":"OSG Exercise 3: Running Jobs in OSG"},{"location":"materials/osg/part1-ex3-submit-osg/#where-in-the-world-are-my-jobs-part-2","text":"In this version of the geolocating exercise, you will submit jobs to the OS Pool from login04.osgconnect.net and hopefully get back interesting results! You will use the same job (software) as you did in OSG exercise 1.1 .","title":"Where in the world are my jobs? (Part 2)"},{"location":"materials/osg/part1-ex3-submit-osg/#gathering-network-information-from-the-osg","text":"As a preliminary step, you will create a submit file that will run in the OS Pool! If not already logged in, ssh into login04.osgconnect.net Set the default project associated with your account by running the following command: connect project If you are given a choice and do not know which project to select, reach out to OSG staff. The default project will stay in effect until changed at a later time. Make a new directory for this exercise, osg-ex3 and change into it Copy files from learn (if not done already) Use scp or rsync from OSG exercise 1.2 to copy the executable and input file from the osg-ex1 directory from learn ; or, if you copied the OSG exercise 1.1 tarball earlier, extract the files from it. Copy or re-create the submit file from OSG exercise 1.1, except this time around change your submit file so that it submits five hundred jobs! Do you think you should test a smaller number of jobs first and scale up? Submit your file and wait for the results","title":"Gathering network information from the OSG"},{"location":"materials/osg/part1-ex3-submit-osg/#mapping-your-jobs","text":"As before, you will be using https://www.mapcustomizer.com/ to visualize where your jobs have landed in the OSG. Copy and paste the collated results from your job output into the Bulk Entry area. Where did your jobs end up?","title":"Mapping your jobs"},{"location":"materials/osg/part1-ex3-submit-osg/#next-exercise","text":"Once completed, move onto the next exercise: Hardware Differences in the OSG","title":"Next exercise"},{"location":"materials/osg/part1-ex3-submit-osg/#extra-challenge-cleaning-up-your-submit-directory","text":"If you run ls in the directory from which you submitted your job, you may see that you now have thousands of files! Proper data management starts to become a requirement as you start to develop truly HTC workflows; it may be helpful to separate your submit files, code, and input data from your output data. Try editing your submit file so that all your output and error files are saved to separate directories within your submit directory. Tip Experiment with fewer job submissions until you\u2019re confident you have it right, then go back to submitting 500 jobs. Remember: Test small and scale up! Submit your file and track the status of your jobs. Did your jobs complete successfully with output and error files saved in separate directories? If not, can you find any useful information in the job logs or hold messages? If you get stuck, review the slides from Tuesday .","title":"Extra Challenge: Cleaning up your submit directory"},{"location":"materials/osg/part1-ex4-hardware-diffs/","text":"OSG Exercise 4: Hardware Differences in the OSG \u00b6 The goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW\u2013Madison) and an OSG glidein pool. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different. In the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one requests; we call this a parameter sweep , in that we are testing many possible values of a parameter. We will request memory from 8\u201364GB, doubling the memory each time. One set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run. Checking CHTC memory availability \u00b6 In this first part, you will create the submit file for both the local and OSG jobs, then submit the local set. Yet another queue syntax \u00b6 Earlier, you learned about the queue statement and some of the different ways it can be invoked to submit multiple jobs. Similar to the queue from statement to submit jobs based on lines from a specific file, you can use queue in to submit jobs based on a list directly from your submit file: queue <# of jobs> <variable> in ( <item 1> <item 2> <item 3> ... ) For example, to submit 6 total jobs that sleep for 5 , 5 , 10 , 10 , 15 , and 15 seconds, you could write the following submit file: executable = /bin/sleep request_cpus = 1 request_memory = 1MB request_disk = 1MB queue 2 arguments in ( 5 10 15 ) Try submitting this yourself and check the jobs that end up in the queue with condor_q -nobatch . Create the submit files \u00b6 To create our parameter sweep, we will create a new submit file with multiple queue statements and change the value of our parameter ( request_memory ) for each batch of jobs. If not already, log in to learn.chtc.wisc.edu Create and change into a new subdirectory called osg-ex4 Create a submit file that is named sleep.sub that executes the command /bin/sleep 300 . Note If you do not remember all of the submit statements to write this file, or just to go faster, find a similar submit file from a previous exercise. Copy the file and rename it here, and make sure the argument to sleep is 300 . Use the queue in syntax to submit 10 jobs each for the following memory requests: 8, 16, 32, and 64GB. You should have 10 jobs requesting 8GB, 10 jobs requesting 16GB, etc. Save the submit file and exit your editor Submit your jobs Monitoring the local jobs \u00b6 Every few minutes, run condor_q and see how your sleep jobs are doing. To easily see how many jobs of each type you have left, run the following command: user@learn $ condor_q <Cluster ID> -af RequestMemory | sort -n | uniq -c The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of memory you requested in MB. Consider making a little table like the one below to track progress. Memory Remaining #1 Remaining #2 Remaining #3 8 GB 10 6 16 GB 10 7 32 GB 10 8 64 GB 10 9 In the meantime, between checking on your local jobs, start the next section \u2013 taking a break every few minutes to record progress on your local jobs. Checking OSG memory availability \u00b6 For the second part of the exercise, you will just copy over the directory from the above section on learn.chtc.wisc.edu to login04.osgconnect.net and resubmit your jobs to the OSG. If you get stuck during the copying process, refer to OSG exercise 2 . Monitoring the remote jobs \u00b6 As you did in the first part, use condor_q to track how your sleep jobs are doing. You can move onto the next exercise but keep tracking the status of your jobs. After you are done with the next exercise , come back to this exercise, and move onto analyzing the results. Analyzing the results \u00b6 Now that you've finished the other exercise, how many jobs have completed locally? How many have completed remotely? Due to the dynamic nature of the remote pool, the OSG may have noticed the demand for higher memory jobs and leased more high memory slots for our pool. That being said, 64GB+ slots are a high-demand, low-availability resource in the OSG so it's unlikely that all of your 64GB+ jobs matched and ran to completion, if any. On the other hand, the local cluster has a fair number of 64GB+ slots so all your jobs have a high chance of running.","title":"Exercise 1.4"},{"location":"materials/osg/part1-ex4-hardware-diffs/#osg-exercise-4-hardware-differences-in-the-osg","text":"The goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW\u2013Madison) and an OSG glidein pool. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different. In the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one requests; we call this a parameter sweep , in that we are testing many possible values of a parameter. We will request memory from 8\u201364GB, doubling the memory each time. One set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run.","title":"OSG Exercise 4: Hardware Differences in the OSG"},{"location":"materials/osg/part1-ex4-hardware-diffs/#checking-chtc-memory-availability","text":"In this first part, you will create the submit file for both the local and OSG jobs, then submit the local set.","title":"Checking CHTC memory availability"},{"location":"materials/osg/part1-ex4-hardware-diffs/#yet-another-queue-syntax","text":"Earlier, you learned about the queue statement and some of the different ways it can be invoked to submit multiple jobs. Similar to the queue from statement to submit jobs based on lines from a specific file, you can use queue in to submit jobs based on a list directly from your submit file: queue <# of jobs> <variable> in ( <item 1> <item 2> <item 3> ... ) For example, to submit 6 total jobs that sleep for 5 , 5 , 10 , 10 , 15 , and 15 seconds, you could write the following submit file: executable = /bin/sleep request_cpus = 1 request_memory = 1MB request_disk = 1MB queue 2 arguments in ( 5 10 15 ) Try submitting this yourself and check the jobs that end up in the queue with condor_q -nobatch .","title":"Yet another queue syntax"},{"location":"materials/osg/part1-ex4-hardware-diffs/#create-the-submit-files","text":"To create our parameter sweep, we will create a new submit file with multiple queue statements and change the value of our parameter ( request_memory ) for each batch of jobs. If not already, log in to learn.chtc.wisc.edu Create and change into a new subdirectory called osg-ex4 Create a submit file that is named sleep.sub that executes the command /bin/sleep 300 . Note If you do not remember all of the submit statements to write this file, or just to go faster, find a similar submit file from a previous exercise. Copy the file and rename it here, and make sure the argument to sleep is 300 . Use the queue in syntax to submit 10 jobs each for the following memory requests: 8, 16, 32, and 64GB. You should have 10 jobs requesting 8GB, 10 jobs requesting 16GB, etc. Save the submit file and exit your editor Submit your jobs","title":"Create the submit files"},{"location":"materials/osg/part1-ex4-hardware-diffs/#monitoring-the-local-jobs","text":"Every few minutes, run condor_q and see how your sleep jobs are doing. To easily see how many jobs of each type you have left, run the following command: user@learn $ condor_q <Cluster ID> -af RequestMemory | sort -n | uniq -c The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of memory you requested in MB. Consider making a little table like the one below to track progress. Memory Remaining #1 Remaining #2 Remaining #3 8 GB 10 6 16 GB 10 7 32 GB 10 8 64 GB 10 9 In the meantime, between checking on your local jobs, start the next section \u2013 taking a break every few minutes to record progress on your local jobs.","title":"Monitoring the local jobs"},{"location":"materials/osg/part1-ex4-hardware-diffs/#checking-osg-memory-availability","text":"For the second part of the exercise, you will just copy over the directory from the above section on learn.chtc.wisc.edu to login04.osgconnect.net and resubmit your jobs to the OSG. If you get stuck during the copying process, refer to OSG exercise 2 .","title":"Checking OSG memory availability"},{"location":"materials/osg/part1-ex4-hardware-diffs/#monitoring-the-remote-jobs","text":"As you did in the first part, use condor_q to track how your sleep jobs are doing. You can move onto the next exercise but keep tracking the status of your jobs. After you are done with the next exercise , come back to this exercise, and move onto analyzing the results.","title":"Monitoring the remote jobs"},{"location":"materials/osg/part1-ex4-hardware-diffs/#analyzing-the-results","text":"Now that you've finished the other exercise, how many jobs have completed locally? How many have completed remotely? Due to the dynamic nature of the remote pool, the OSG may have noticed the demand for higher memory jobs and leased more high memory slots for our pool. That being said, 64GB+ slots are a high-demand, low-availability resource in the OSG so it's unlikely that all of your 64GB+ jobs matched and ran to completion, if any. On the other hand, the local cluster has a fair number of 64GB+ slots so all your jobs have a high chance of running.","title":"Analyzing the results"},{"location":"materials/osg/part1-ex5-software-diffs/","text":"OSG Exercise 5: Software Differences in the OSG \u00b6 The goal of this exercise is to see the differences in availability of software in the OSG. At your local cluster, you may be used to having certain versions of software but out on the OSG, it's possible that the software you need won't even be installed. Refresher - condor_status \u00b6 The OSG pool, like the local pool you used earlier, is just another HTCondor pool. This means that the commands you use will be the same and the jobs you submit can have similar payloads but there is one major difference: the slots are different! As before, you can use the condor_status command to inspect these differences. Open two terminal windows side-by-side Log in to learn.chtc.wisc.edu in one window and login04.osgconnect.net in the other Run condor_status in both windows Note For login04.osgconnect.net you will need to add -pool flock.opensciencegrid.org to your condor_status command. Notice any differences? Comparing operating systems \u00b6 To really see differences between slots in the local cluster vs the OSG, you will want to compare the slot ClassAds between the two pools. Rather than inspecting the very long ClassAd for each slot, you will look at a specific attribute called OpSysAndVer , which tells us the operating system version of the server where a slot resides. An easy way to show this attribute for all slots is by using condor_status in conjunction with the -autoformat (or -af for short) option. -autoformat like the -format option you learned about earlier will print out the attributes you're interested in for each slot but as you probably guessed, it does some automatic formatting for you. So to show the operating system and version of each slot, run the following command in both of your terminal windows: user@submit-server $ condor_status -autoformat OpSysAndVer You will see many values with the type of operating system at the front and the version number at the end (i.e. SL6 stands for Scientific Linux 6). The only problem is that with hundreds or thousands of slots, it's difficult to get a feel for the composition of each pool from this output. You can find a count for each operating system by passing the condor_status output into the sort and uniq commands. Your command line should look something like this: user@learn $ condor_status -autoformat OpSysAndVer | sort | uniq -c Can you spot the differences between the two pools now? Submitting probe jobs \u00b6 Knowing the type and version of the operating systems is a step in the right direction to knowing what kind of software will be available on the servers that your jobs land on. However it still only serves as a proxy to the information that you really want: does the server have the software that you want? Does it have the correct version? Software probe code \u00b6 The following shell script probes for software and returns the version if it is installed: #!/bin/sh get_version (){ program = $1 $program --version > /dev/null 2 > & 1 double_dash_rc = $? $program -version > /dev/null 2 > & 1 single_dash_rc = $? which $program > /dev/null 2 > & 1 which_rc = $? if [ $double_dash_rc -eq 0 ] ; then $program --version 2 > & 1 elif [ $single_dash_rc -eq 0 ] ; then $program -version 2 > & 1 elif [ $which_rc -eq 0 ] ; then echo \" $program installed but could not find version information\" else echo \" $program not installed\" fi } get_version 'R' get_version 'cmake' get_version 'python' get_version 'python3' If there's a specific command line program that your research requires, feel free to add it to the script! For example, if you wanted to test for the existence and version of nslookup , you would add the following to the end of the script: get_version 'nslookup' Probing several servers \u00b6 For this part of the exercise, try creating a submit file without referring to previous exercises! Log in to login04.osgconnect.net Create and change into a new folder for this exercise, e.g. osg-ex5 Save the above script as a file named sw_probe.sh Create a submit file that runs sw_probe.sh 100 times and uses macros to write different output , error , and log files Submit your job and wait for the results Will you be able to do your research on the OSG with what's available? Don't fret if it doesn't look like you can: over the next few days, you'll learn how to make your jobs portable enough so that they can run anywhere!","title":"Exercise 1.5"},{"location":"materials/osg/part1-ex5-software-diffs/#osg-exercise-5-software-differences-in-the-osg","text":"The goal of this exercise is to see the differences in availability of software in the OSG. At your local cluster, you may be used to having certain versions of software but out on the OSG, it's possible that the software you need won't even be installed.","title":"OSG Exercise 5: Software Differences in the OSG"},{"location":"materials/osg/part1-ex5-software-diffs/#refresher-condor_status","text":"The OSG pool, like the local pool you used earlier, is just another HTCondor pool. This means that the commands you use will be the same and the jobs you submit can have similar payloads but there is one major difference: the slots are different! As before, you can use the condor_status command to inspect these differences. Open two terminal windows side-by-side Log in to learn.chtc.wisc.edu in one window and login04.osgconnect.net in the other Run condor_status in both windows Note For login04.osgconnect.net you will need to add -pool flock.opensciencegrid.org to your condor_status command. Notice any differences?","title":"Refresher - condor_status"},{"location":"materials/osg/part1-ex5-software-diffs/#comparing-operating-systems","text":"To really see differences between slots in the local cluster vs the OSG, you will want to compare the slot ClassAds between the two pools. Rather than inspecting the very long ClassAd for each slot, you will look at a specific attribute called OpSysAndVer , which tells us the operating system version of the server where a slot resides. An easy way to show this attribute for all slots is by using condor_status in conjunction with the -autoformat (or -af for short) option. -autoformat like the -format option you learned about earlier will print out the attributes you're interested in for each slot but as you probably guessed, it does some automatic formatting for you. So to show the operating system and version of each slot, run the following command in both of your terminal windows: user@submit-server $ condor_status -autoformat OpSysAndVer You will see many values with the type of operating system at the front and the version number at the end (i.e. SL6 stands for Scientific Linux 6). The only problem is that with hundreds or thousands of slots, it's difficult to get a feel for the composition of each pool from this output. You can find a count for each operating system by passing the condor_status output into the sort and uniq commands. Your command line should look something like this: user@learn $ condor_status -autoformat OpSysAndVer | sort | uniq -c Can you spot the differences between the two pools now?","title":"Comparing operating systems"},{"location":"materials/osg/part1-ex5-software-diffs/#submitting-probe-jobs","text":"Knowing the type and version of the operating systems is a step in the right direction to knowing what kind of software will be available on the servers that your jobs land on. However it still only serves as a proxy to the information that you really want: does the server have the software that you want? Does it have the correct version?","title":"Submitting probe jobs"},{"location":"materials/osg/part1-ex5-software-diffs/#software-probe-code","text":"The following shell script probes for software and returns the version if it is installed: #!/bin/sh get_version (){ program = $1 $program --version > /dev/null 2 > & 1 double_dash_rc = $? $program -version > /dev/null 2 > & 1 single_dash_rc = $? which $program > /dev/null 2 > & 1 which_rc = $? if [ $double_dash_rc -eq 0 ] ; then $program --version 2 > & 1 elif [ $single_dash_rc -eq 0 ] ; then $program -version 2 > & 1 elif [ $which_rc -eq 0 ] ; then echo \" $program installed but could not find version information\" else echo \" $program not installed\" fi } get_version 'R' get_version 'cmake' get_version 'python' get_version 'python3' If there's a specific command line program that your research requires, feel free to add it to the script! For example, if you wanted to test for the existence and version of nslookup , you would add the following to the end of the script: get_version 'nslookup'","title":"Software probe code"},{"location":"materials/osg/part1-ex5-software-diffs/#probing-several-servers","text":"For this part of the exercise, try creating a submit file without referring to previous exercises! Log in to login04.osgconnect.net Create and change into a new folder for this exercise, e.g. osg-ex5 Save the above script as a file named sw_probe.sh Create a submit file that runs sw_probe.sh 100 times and uses macros to write different output , error , and log files Submit your job and wait for the results Will you be able to do your research on the OSG with what's available? Don't fret if it doesn't look like you can: over the next few days, you'll learn how to make your jobs portable enough so that they can run anywhere!","title":"Probing several servers"},{"location":"materials/software/part1-ex1-download/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 1.1: Using a Pre-compiled Binary \u00b6 In this exercise, we will run a job using a downloaded, pre-compiled binary. This exercise should take 10-15 minutes. Background \u00b6 This is the simplest scenario for using a particular software program on the Open Science Grid - downloading a pre-compiled binary and using it to run jobs. Our Software Example \u00b6 The software we will be using for this example is a common tool for aligning genome and protein sequences against a reference database, the BLAST program. Search the internet for the BLAST software. Searches might include \"blast executable or \"download blast software\". Hopefully these searches will lead you to a BLAST website page that looks like this: Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code . You should end up on a page with a list of each version of BLAST that is available for different operating systems. We could download the source and compile it ourselves, but instead, we're going to use one of the pre-built binaries. Before proceeding, look at the list of downloads and try to determine which one you want. Based on our operating system, we want to use the Linux binary, which is labelled with the x64-linux suffix. All the other links are either for source code or other operating systems. While logged into login04.osgconnect.net , create a directory for this exercise. Then download the appropriate tar.gz file and un-tar/decompress it it. If you want to do this all from the command line, the sequence will look like this (using wget as the download command.) user@login $ wget https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.12.0+-x64-linux.tar.gz user@login $ tar -xzf ncbi-blast-2.12.0+-x64-linux.tar.gz We're going to be using the blastx binary in our job. Where is it in the directory you just decompressed? Copy the Input Files \u00b6 To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: username@login $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/pdbaa.tar.gz username@login $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/mouse.fa Untar the pdbaa database: username@login $ tar -xzf pdbaa.tar.gz Submitting the Job \u00b6 We now have our program (the pre-compiled blastx binary) and our input files, so all that remains is to create the submit file. The form of a typical blastx command looks something like this: blastx -db <database_dir/prefix> -query <input_file> -out <output_file> Copy a submit file from one of the Day 2 exercises to use for this exercise. Think about which lines you will need to change or add to your submit file in order to submit the job successfully. In particular: What is the executable? How can you indicate the entire command line sequence above? Which files need to be transferred in addition to the executable? Does this job require a certain type of operating system? Do you have any idea how much memory or disk to request? Try to answer these questions and modify your submit file appropriately. Once you have done all you can, check your submit file against the lines below, which contain the exact components to run this particular job. The executable is blastx , which is located in the bin directory of our downloaded BLAST directory. We need to use the arguments line in the submit file to express the rest of the command. executable = ncbi-blast-2.12.0+/bin/blastx arguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt The BLAST program requires our input file and database, so they must be transferred with transfer_input_files . transfer_input_files = pdbaa, mouse.fa Let's assume that we've run this program before, and we know that 1GB of disk and 1GB of memory will be MORE than enough (the 'log' file will tell us how accurate we are, after the job runs): request_memory = 1GB request_disk = 1GB Because we downloaded a Linux-specific binary, we need to request machines that are running Linux. requirements = (OSGVO_OS_STRING == \"RHEL 7\") Submit the blast job using condor_submit . Once the job starts, it should run in just a few minutes and produce a file called results.txt .","title":"Exercise 1.1"},{"location":"materials/software/part1-ex1-download/#software-exercise-11-using-a-pre-compiled-binary","text":"In this exercise, we will run a job using a downloaded, pre-compiled binary. This exercise should take 10-15 minutes.","title":"Software Exercise 1.1: Using a Pre-compiled Binary"},{"location":"materials/software/part1-ex1-download/#background","text":"This is the simplest scenario for using a particular software program on the Open Science Grid - downloading a pre-compiled binary and using it to run jobs.","title":"Background"},{"location":"materials/software/part1-ex1-download/#our-software-example","text":"The software we will be using for this example is a common tool for aligning genome and protein sequences against a reference database, the BLAST program. Search the internet for the BLAST software. Searches might include \"blast executable or \"download blast software\". Hopefully these searches will lead you to a BLAST website page that looks like this: Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code . You should end up on a page with a list of each version of BLAST that is available for different operating systems. We could download the source and compile it ourselves, but instead, we're going to use one of the pre-built binaries. Before proceeding, look at the list of downloads and try to determine which one you want. Based on our operating system, we want to use the Linux binary, which is labelled with the x64-linux suffix. All the other links are either for source code or other operating systems. While logged into login04.osgconnect.net , create a directory for this exercise. Then download the appropriate tar.gz file and un-tar/decompress it it. If you want to do this all from the command line, the sequence will look like this (using wget as the download command.) user@login $ wget https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.12.0+-x64-linux.tar.gz user@login $ tar -xzf ncbi-blast-2.12.0+-x64-linux.tar.gz We're going to be using the blastx binary in our job. Where is it in the directory you just decompressed?","title":"Our Software Example"},{"location":"materials/software/part1-ex1-download/#copy-the-input-files","text":"To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: username@login $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/pdbaa.tar.gz username@login $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/mouse.fa Untar the pdbaa database: username@login $ tar -xzf pdbaa.tar.gz","title":"Copy the Input Files"},{"location":"materials/software/part1-ex1-download/#submitting-the-job","text":"We now have our program (the pre-compiled blastx binary) and our input files, so all that remains is to create the submit file. The form of a typical blastx command looks something like this: blastx -db <database_dir/prefix> -query <input_file> -out <output_file> Copy a submit file from one of the Day 2 exercises to use for this exercise. Think about which lines you will need to change or add to your submit file in order to submit the job successfully. In particular: What is the executable? How can you indicate the entire command line sequence above? Which files need to be transferred in addition to the executable? Does this job require a certain type of operating system? Do you have any idea how much memory or disk to request? Try to answer these questions and modify your submit file appropriately. Once you have done all you can, check your submit file against the lines below, which contain the exact components to run this particular job. The executable is blastx , which is located in the bin directory of our downloaded BLAST directory. We need to use the arguments line in the submit file to express the rest of the command. executable = ncbi-blast-2.12.0+/bin/blastx arguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt The BLAST program requires our input file and database, so they must be transferred with transfer_input_files . transfer_input_files = pdbaa, mouse.fa Let's assume that we've run this program before, and we know that 1GB of disk and 1GB of memory will be MORE than enough (the 'log' file will tell us how accurate we are, after the job runs): request_memory = 1GB request_disk = 1GB Because we downloaded a Linux-specific binary, we need to request machines that are running Linux. requirements = (OSGVO_OS_STRING == \"RHEL 7\") Submit the blast job using condor_submit . Once the job starts, it should run in just a few minutes and produce a file called results.txt .","title":"Submitting the Job"},{"location":"materials/software/part1-ex2-wrapper/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 1.2: Writing a Wrapper Script \u00b6 In this exercise, you will create a wrapper script to run the same program ( blastx ) as the previous exercise . Background \u00b6 Wrapper scripts are a useful tool for running software that can't be compiled into one piece, needs to be installed with every job, or just for running extra steps. A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the Open Science Grid. For this exercise, we will write a wrapper script as an alternate way to run the same job as the previous exercise. Wrapper Script, part 1 \u00b6 Our wrapper script will be a bash script that runs several commands. In the same directory as the last exercise (still logged into login04.osgconnect.net ) make a file called run_blast.sh . The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the script? Once you have an idea, check against the example below: #!/bin/bash ncbi-blast-2.12.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt Note The \"header\" of #!/bin/bash will tell the computer that this is a bash shell script and can be run in the same way that you would run individual commands on the command line. Submit File Changes \u00b6 We now need to make some changes to our submit file. Make a copy of your previous submit file and open it to edit. Since we are now using a wrapper script, that will be our job's executable. Replace the original blastx exeuctable with the name of our wrapper script and comment out the arguments line. executable = run_blast.sh #arguments = Note that since the blastx program is no longer listed as the executable, it will be need to be included in transfer_input_files . Instead of transferring just that program, we will transfer the original downloaded tar.gz file. To achieve efficiency, we'll also transfer the pdbaa database as the original tar.gz file instead of as the unzipped folder: transfer_input_files = pdbaa.tar.gz, mouse.fa, ncbi-blast-2.12.0+-x64-linux.tar.gz If you really want to be on top of things, look at the log file for the last exercise, and update your memory and disk requests to be just slightly above the actual \"Usage\" values in the log. Before submitting, make sure to make the below additional changes to the wrapper script! Wrapper Script, part 2 \u00b6 Now that our database and BLAST software are being transferred to the job as tar.gz files, our script needs to accommodate. Opening your run_blast.sh script, add two commands at the start to un-tar the BLAST and pdbaa tar.gz files. See the previous exercise if you're not sure what these commands looks like. In order to distinguish this job from our previous job, change the output file name to something besides results.txt . The completed script run_blast.sh should look like this: #/bin/bash tar -xzf ncbi-blast-2.12.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ncbi-blast-2.12.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: username@login $ chmod u+x run_blast.sh Your job is now ready to submit. Submit it using condor_submit and monitor using condor_q .","title":"Exercise 1.2"},{"location":"materials/software/part1-ex2-wrapper/#software-exercise-12-writing-a-wrapper-script","text":"In this exercise, you will create a wrapper script to run the same program ( blastx ) as the previous exercise .","title":"Software Exercise 1.2: Writing a Wrapper Script"},{"location":"materials/software/part1-ex2-wrapper/#background","text":"Wrapper scripts are a useful tool for running software that can't be compiled into one piece, needs to be installed with every job, or just for running extra steps. A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the Open Science Grid. For this exercise, we will write a wrapper script as an alternate way to run the same job as the previous exercise.","title":"Background"},{"location":"materials/software/part1-ex2-wrapper/#wrapper-script-part-1","text":"Our wrapper script will be a bash script that runs several commands. In the same directory as the last exercise (still logged into login04.osgconnect.net ) make a file called run_blast.sh . The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the script? Once you have an idea, check against the example below: #!/bin/bash ncbi-blast-2.12.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt Note The \"header\" of #!/bin/bash will tell the computer that this is a bash shell script and can be run in the same way that you would run individual commands on the command line.","title":"Wrapper Script, part 1"},{"location":"materials/software/part1-ex2-wrapper/#submit-file-changes","text":"We now need to make some changes to our submit file. Make a copy of your previous submit file and open it to edit. Since we are now using a wrapper script, that will be our job's executable. Replace the original blastx exeuctable with the name of our wrapper script and comment out the arguments line. executable = run_blast.sh #arguments = Note that since the blastx program is no longer listed as the executable, it will be need to be included in transfer_input_files . Instead of transferring just that program, we will transfer the original downloaded tar.gz file. To achieve efficiency, we'll also transfer the pdbaa database as the original tar.gz file instead of as the unzipped folder: transfer_input_files = pdbaa.tar.gz, mouse.fa, ncbi-blast-2.12.0+-x64-linux.tar.gz If you really want to be on top of things, look at the log file for the last exercise, and update your memory and disk requests to be just slightly above the actual \"Usage\" values in the log. Before submitting, make sure to make the below additional changes to the wrapper script!","title":"Submit File Changes"},{"location":"materials/software/part1-ex2-wrapper/#wrapper-script-part-2","text":"Now that our database and BLAST software are being transferred to the job as tar.gz files, our script needs to accommodate. Opening your run_blast.sh script, add two commands at the start to un-tar the BLAST and pdbaa tar.gz files. See the previous exercise if you're not sure what these commands looks like. In order to distinguish this job from our previous job, change the output file name to something besides results.txt . The completed script run_blast.sh should look like this: #/bin/bash tar -xzf ncbi-blast-2.12.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ncbi-blast-2.12.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: username@login $ chmod u+x run_blast.sh Your job is now ready to submit. Submit it using condor_submit and monitor using condor_q .","title":"Wrapper Script, part 2"},{"location":"materials/software/part1-ex3-arguments/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 1.3: Passing Arguments Through the Wrapper Script \u00b6 In this exercise, you will change the wrapper script and submit file from the previous exercise to use arguments. Background \u00b6 In Exercise 1.2, the wrapper script had all of the input information - the database and input file - written directly into the script. However, imagine if you wanted to run the same program on different inputs. Instead of writing new wrapper scripts for each job, you can modify the script so that some of the values are set by arguments . Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line: But in our world of job submission, the arguments will be listed in the submit file, in the arguments line. Identifying Potential Arguments \u00b6 In the same directory as the last exercise (still logged into login04.osgconnect.net ), make sure you're in the directory with your BLAST job submission. What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times. In this example, some values we might want to change are the name of the comparison database, the input file, and the output file. Modifying Files \u00b6 We are going to add three arguments to the wrapper script, controlling the database, input and output file. Make a copy of your last submit file and open it for editing. Add an arguments line, or uncomment the one that exists, and add the three input values mentioned above. The arguments line in your submit file should look like this: arguments = pdbaa mouse.fa results3.txt (We're using results3.txt ) to distinguish between the previous two runs.) Now go back to the wrapper script. Each scripting language (bash, perl, python, R, etc.) will have its own particular syntax for capturing command line arguments. For bash (the language of our current wrapper script), the variables $1 , $2 and $3 represent the first, second, and third arguments, respectively. Thus, in the main command of the script, replace the various names with these variables: ncbi-blast-2.12.0+/bin/blastx -db $1 / $1 -query $2 -out $3 If your wrapper script is in a different language, you should use that language's syntax for reading in variables from the command line. Once these changes are made, submit your jobs with condor_submit . Use condor_q -nobatch to see what the job command looks like to HTCondor. It is now easy to change the inputs for the job; we can write them into the arguments line of the submit file and they will be propagated to the command in the wrapper script. We can even turn the submit file arguments into their own variables when submitting multiple jobs at once. Readability with Variables \u00b6 One of the downsides of this approach, is that our command has become harder to read. The original script contains all the information at a glance: ncbi-blast-2.12.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt But our new version is more cryptic -- what is $1 ?: ncbi-blast-2.10.1+/bin/blastx -db $1 -query $2 -out $3 One way to overcome this is to create our own variable names inside the wrapper script and assign the argument values to them. Here is an example for our BLAST script: #/bin/bash DATABASE = $1 INFILE = $2 OUTFILE = $3 tar -xzf ncbi-blast-2.10.1+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ncbi-blast-2.10.1+/bin/blastx -db $DATABASE / $DATABASE -query $INFILE -out $OUTFILE Here, we are assigning the input arguments ( $1 , $2 and $3 ) to new variable names, and then using those names ( $DATABASE , $INFILE , and $OUTFILE ) in the command, which is easier to read. Edit your script to match the above syntax. Submit your jobs with condor_submit . When the job finishes, look at the job's standard output file to see how the variables printed.","title":"Exercise 1.3"},{"location":"materials/software/part1-ex3-arguments/#software-exercise-13-passing-arguments-through-the-wrapper-script","text":"In this exercise, you will change the wrapper script and submit file from the previous exercise to use arguments.","title":"Software Exercise 1.3: Passing Arguments Through the Wrapper Script"},{"location":"materials/software/part1-ex3-arguments/#background","text":"In Exercise 1.2, the wrapper script had all of the input information - the database and input file - written directly into the script. However, imagine if you wanted to run the same program on different inputs. Instead of writing new wrapper scripts for each job, you can modify the script so that some of the values are set by arguments . Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line: But in our world of job submission, the arguments will be listed in the submit file, in the arguments line.","title":"Background"},{"location":"materials/software/part1-ex3-arguments/#identifying-potential-arguments","text":"In the same directory as the last exercise (still logged into login04.osgconnect.net ), make sure you're in the directory with your BLAST job submission. What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times. In this example, some values we might want to change are the name of the comparison database, the input file, and the output file.","title":"Identifying Potential Arguments"},{"location":"materials/software/part1-ex3-arguments/#modifying-files","text":"We are going to add three arguments to the wrapper script, controlling the database, input and output file. Make a copy of your last submit file and open it for editing. Add an arguments line, or uncomment the one that exists, and add the three input values mentioned above. The arguments line in your submit file should look like this: arguments = pdbaa mouse.fa results3.txt (We're using results3.txt ) to distinguish between the previous two runs.) Now go back to the wrapper script. Each scripting language (bash, perl, python, R, etc.) will have its own particular syntax for capturing command line arguments. For bash (the language of our current wrapper script), the variables $1 , $2 and $3 represent the first, second, and third arguments, respectively. Thus, in the main command of the script, replace the various names with these variables: ncbi-blast-2.12.0+/bin/blastx -db $1 / $1 -query $2 -out $3 If your wrapper script is in a different language, you should use that language's syntax for reading in variables from the command line. Once these changes are made, submit your jobs with condor_submit . Use condor_q -nobatch to see what the job command looks like to HTCondor. It is now easy to change the inputs for the job; we can write them into the arguments line of the submit file and they will be propagated to the command in the wrapper script. We can even turn the submit file arguments into their own variables when submitting multiple jobs at once.","title":"Modifying Files"},{"location":"materials/software/part1-ex3-arguments/#readability-with-variables","text":"One of the downsides of this approach, is that our command has become harder to read. The original script contains all the information at a glance: ncbi-blast-2.12.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt But our new version is more cryptic -- what is $1 ?: ncbi-blast-2.10.1+/bin/blastx -db $1 -query $2 -out $3 One way to overcome this is to create our own variable names inside the wrapper script and assign the argument values to them. Here is an example for our BLAST script: #/bin/bash DATABASE = $1 INFILE = $2 OUTFILE = $3 tar -xzf ncbi-blast-2.10.1+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ncbi-blast-2.10.1+/bin/blastx -db $DATABASE / $DATABASE -query $INFILE -out $OUTFILE Here, we are assigning the input arguments ( $1 , $2 and $3 ) to new variable names, and then using those names ( $DATABASE , $INFILE , and $OUTFILE ) in the command, which is easier to read. Edit your script to match the above syntax. Submit your jobs with condor_submit . When the job finishes, look at the job's standard output file to see how the variables printed.","title":"Readability with Variables"},{"location":"materials/software/part2-ex1-compiling/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 2.1: Compile Statically Linked Code \u00b6 The goal of this exercise is to compile and statically link a piece of code and then submit it as a job. This exercise should take 5-10 minutes. Background \u00b6 There is a large amount of scientific software that is available as source code. Source code is usually a group of text files (code) meant to be downloaded and then compiled into a binary file which a computer can understand. Sometimes the source code depends on other pieces of code called libraries. If the source code is linked statically , these libraries are bundled into the compilation with the source code, creating a static binary which can be run on any computer with the same operating system. Our Software Example \u00b6 For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked. Our C code prints 7 rows of Pascal's triangle. Log into the OSG submit node login04.osgconnect.net . Create a directory for this exercise and cd into it. Copy and paste the following code into a file named pascal.c . #include \"stdio.h\" long factorial ( int ); int main () { int i , n , c ; n = 7 ; for ( i = 0 ; i < n ; i ++ ){ for ( c = 0 ; c <= ( n - i - 2 ); c ++ ) printf ( \" \" ); for ( c = 0 ; c <= i ; c ++ ) printf ( \"%ld \" , factorial ( i ) / ( factorial ( c ) * factorial ( i - c ))); printf ( \" \\n \" ); } return 0 ; } long factorial ( int n ) { int c ; long result = 1 ; for ( c = 1 ; c <= n ; c ++ ) result = result * c ; return result ; } Compiling \u00b6 In order to use this code in a job, we will first need to statically compile the code. Recall the slide from the lecture - where can we compile and where should we compile? In particular: Where is the compiler available? How computationally intensive will this compilation be? Think about these questions before moving on. Where do you think we should compile? Most linux servers (including our submit node) have the gcc (GNU compiler collection) installed, so we already have a compiler on the submit node. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the submit node. Compile the code, using the command: username@login $ gcc -static pascal.c -o pascal Note that we have added the -static option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. Verify that the compiled binary was statically linked: username@login $ file pascal The Linux file command provides information about the type or kind of file that is given as an argument. In this case, you should get output like this: username@host $ file pascal pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.18, not stripped Note the blue text, which clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text dynamically linked (uses shared libs) instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the file command on lots of other files) Submit the Job \u00b6 Now that our code is compiled, we can use it to submit a job. Think about what submit file lines we need to use to run this job: Are there input files? Are there command line arguments? Where is its output written? Based on what you thought about in 1., find a submit file from earlier that you can modify to run our compiled pascal code. Copy it to the directory with the pascal binary and make those changes. Submit the job using condor_submit . Once the job has run and left the queue, you should be able to see the results (seven rows of Pascal's triangle) in the .out file created by the job.","title":"Exercise 2.1"},{"location":"materials/software/part2-ex1-compiling/#software-exercise-21-compile-statically-linked-code","text":"The goal of this exercise is to compile and statically link a piece of code and then submit it as a job. This exercise should take 5-10 minutes.","title":"Software Exercise 2.1: Compile Statically Linked Code"},{"location":"materials/software/part2-ex1-compiling/#background","text":"There is a large amount of scientific software that is available as source code. Source code is usually a group of text files (code) meant to be downloaded and then compiled into a binary file which a computer can understand. Sometimes the source code depends on other pieces of code called libraries. If the source code is linked statically , these libraries are bundled into the compilation with the source code, creating a static binary which can be run on any computer with the same operating system.","title":"Background"},{"location":"materials/software/part2-ex1-compiling/#our-software-example","text":"For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked. Our C code prints 7 rows of Pascal's triangle. Log into the OSG submit node login04.osgconnect.net . Create a directory for this exercise and cd into it. Copy and paste the following code into a file named pascal.c . #include \"stdio.h\" long factorial ( int ); int main () { int i , n , c ; n = 7 ; for ( i = 0 ; i < n ; i ++ ){ for ( c = 0 ; c <= ( n - i - 2 ); c ++ ) printf ( \" \" ); for ( c = 0 ; c <= i ; c ++ ) printf ( \"%ld \" , factorial ( i ) / ( factorial ( c ) * factorial ( i - c ))); printf ( \" \\n \" ); } return 0 ; } long factorial ( int n ) { int c ; long result = 1 ; for ( c = 1 ; c <= n ; c ++ ) result = result * c ; return result ; }","title":"Our Software Example"},{"location":"materials/software/part2-ex1-compiling/#compiling","text":"In order to use this code in a job, we will first need to statically compile the code. Recall the slide from the lecture - where can we compile and where should we compile? In particular: Where is the compiler available? How computationally intensive will this compilation be? Think about these questions before moving on. Where do you think we should compile? Most linux servers (including our submit node) have the gcc (GNU compiler collection) installed, so we already have a compiler on the submit node. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the submit node. Compile the code, using the command: username@login $ gcc -static pascal.c -o pascal Note that we have added the -static option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. Verify that the compiled binary was statically linked: username@login $ file pascal The Linux file command provides information about the type or kind of file that is given as an argument. In this case, you should get output like this: username@host $ file pascal pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.18, not stripped Note the blue text, which clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text dynamically linked (uses shared libs) instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the file command on lots of other files)","title":"Compiling"},{"location":"materials/software/part2-ex1-compiling/#submit-the-job","text":"Now that our code is compiled, we can use it to submit a job. Think about what submit file lines we need to use to run this job: Are there input files? Are there command line arguments? Where is its output written? Based on what you thought about in 1., find a submit file from earlier that you can modify to run our compiled pascal code. Copy it to the directory with the pascal binary and make those changes. Submit the job using condor_submit . Once the job has run and left the queue, you should be able to see the results (seven rows of Pascal's triangle) in the .out file created by the job.","title":"Submit the Job"},{"location":"materials/software/part2-ex2-prepackaged/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 2.2: Pre-package a Research Code \u00b6 In this exercise, you will create an installation of a Bayesian inference package (OpenBUGS) and then create a wrapper script to unpack that installation to run jobs. It should take 30-35 minutes. Background \u00b6 Some software cannot be compiled into a single executable, whether you compile it yourself (as in Software Exercise 2.1 ) or download it already compiled (as in Software Exercise 1.1 ). In this case, it is necessary to download or create a portable copy of the software and then use a wrapper script (as in this exercise ) to \"install\" or run the software on a per job basis. This script can either install the software from the source code, or (as in this exercise), unpack a portable software package that you've pre-built yourself. Our Software Example \u00b6 For this exercise, we will be using the Bayseian inference package OpenBUGS. OpenBUGS is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library. Do an internet search to find the Open BUGS software downloads page. Create a directory for this exercise on the CHTC submit server learn.chtc.wisc.edu ( not training.osgconnect.net ), Because you can't download the OpenBUGS source tarball directly, download it from our \"squid\" webserver: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/OpenBUGS-3.2.3.tar.gz Where to Prepare \u00b6 Our goal is to pre-build an OpenBUGS installation, and then write a script that will unpack that installation and run a simulation. Where can we create this pre-built installation? Based on the end of the lecture, what are our options and which would be most appropriate? Make a guess before moving on. Because we're on the CHTC-based submit node ( learn.chtc.wisc.edu ), we have the option of using an interactive job to build the OpenBUGS installation. This is a good option because the submit server is already busy with lots of users and we don't know how long the OpenBUGS install will take. We'll also target specific build servers with extra tools by adding some special requirements to our interactive job. Copy the following lines into a file named build.submit log = build.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = +IsBuildJob = true requirements = (IsBuildSlot == true) request_cpus = 1 request_disk = 2GB request_memory = 2GB queue Note the lack of executable. Condor doesn't need an executable for this job because it will be interactive, meaning you are running the commands instead of Condor. In order to create the installation, we will need the source code to come with us. The transfer_input_files line is blank - fill it in with the name of our Open BUGS source tarball . To request an interactive job, we will add a -i flag to the condor_submit command. The whole command you enter should look like this: username@learn $ condor_submit -i build.submit Read Through Installation Documentation \u00b6 While you're waiting for the interactive job to start, you can start reading the Open BUGS installation documentation online. Find the installation instructions for Open BUGS. On the downloads page, there are short instructions for how to install Open BUGS. There are two options shown for installation -- which should we use? The first installation option given uses sudo -- which is an administrative permission that you won't have as a normal user. Luckily, as described in the instructions, you can use the --prefix option to set where Open BUGS will be installed, which will allow us to install it without administrative permissions. Installation \u00b6 Your interactive job should have started by now and we've learned about installing our program. Let's test it out. Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. username@host $ mkdir openbugs Now run the commands to unpack the source code: username@host $ tar -zxf OpenBUGS-3.2.3.tar.gz username@host $ cd OpenBUGS-3.2.3 Now we can follow the second set of installation instructions. For the prefix, we'll use the variable $PWD to capture the name of our current working directory and then a relative path to the openbugs directory we created in step 1: username@host $ ./configure --prefix = $PWD /../openbugs username@host $ make username@host $ make install Go back to the job's main working directory : username@host $ cd .. and confirm that our installation procedure created bin , lib , and share directories. username@host $ ls openbugs bin lib share Now we want to package up our installation, so we can use it in other jobs. We can do this by compressing any necessary directories into a single gzipped tarball. username@host $ tar -czf openbugs.tar.gz openbugs/ Once everything is complete, type exit to leave the interactive job. Make sure that your tarball is in the main working directory - it will be transferred back to the submit server automatically. username@learn $ exit Note that we now have two tarballs in our directory -- the source tarball ( OpenBUGS-3.2.3.tar.gz ), which we will no longer need and our newly built installation ( openbugs.tar.gz ) which is what we will actually be using to run jobs. Wrapper Script \u00b6 Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in a previous exercise . These steps should be performed back on the submit server ( learn.chtc.wisc.edu ). Create a script called run_openbugs.sh . The script will first need to untar our installation, so the script should start out like this: #!/bin/bash tar -xzf openbugs.tar.gz We're going to use the same $(pwd) trick from the installation in order to tell the computer how to find Open BUGS. We will do this by setting the PATH environment variable, to include the directory where Open BUGS is installed: export PATH = $( pwd ) /openbugs/bin: $PATH Finally, the wrapper script needs to not only setup Open BUGS, but actually run the program. Add the following lines to your run_openbugs.sh wrapper script. OpenBUGS < input.txt > results.txt Make sure the wrapper script has executable permissions: username@learn $ chmod u+x run_openbugs.sh Run a Open BUGS job \u00b6 We're almost ready! We need two more pieces to run a OpenBUGS job. Download the necessary input files to your directory on the submit server and then untar them. username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/openbugs_files.tar.gz username@learn $ tar -xzf openbugs_files.tar.gz Our last step is to create a submit file for our Open BUGS job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from a previous exercise as a base) and modify it as you think necessary. The two most important lines to modify for this job are listed below; check them against your own submit file: executable = run_openbugs.sh transfer_input_files = openbugs.tar.gz, openbugs_files/ A wrapper script will always be a job's executable . When using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files . Note The / in the transfer_input_files line indicates that we are transferring the contents of that directory (which in this case, is what we want), rather than the directory itself. Submit the job with condor_submit . Once the job completes, it should produce a results.txt file.","title":"Exercise 2.2"},{"location":"materials/software/part2-ex2-prepackaged/#software-exercise-22-pre-package-a-research-code","text":"In this exercise, you will create an installation of a Bayesian inference package (OpenBUGS) and then create a wrapper script to unpack that installation to run jobs. It should take 30-35 minutes.","title":"Software Exercise 2.2: Pre-package a Research Code"},{"location":"materials/software/part2-ex2-prepackaged/#background","text":"Some software cannot be compiled into a single executable, whether you compile it yourself (as in Software Exercise 2.1 ) or download it already compiled (as in Software Exercise 1.1 ). In this case, it is necessary to download or create a portable copy of the software and then use a wrapper script (as in this exercise ) to \"install\" or run the software on a per job basis. This script can either install the software from the source code, or (as in this exercise), unpack a portable software package that you've pre-built yourself.","title":"Background"},{"location":"materials/software/part2-ex2-prepackaged/#our-software-example","text":"For this exercise, we will be using the Bayseian inference package OpenBUGS. OpenBUGS is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library. Do an internet search to find the Open BUGS software downloads page. Create a directory for this exercise on the CHTC submit server learn.chtc.wisc.edu ( not training.osgconnect.net ), Because you can't download the OpenBUGS source tarball directly, download it from our \"squid\" webserver: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/OpenBUGS-3.2.3.tar.gz","title":"Our Software Example"},{"location":"materials/software/part2-ex2-prepackaged/#where-to-prepare","text":"Our goal is to pre-build an OpenBUGS installation, and then write a script that will unpack that installation and run a simulation. Where can we create this pre-built installation? Based on the end of the lecture, what are our options and which would be most appropriate? Make a guess before moving on. Because we're on the CHTC-based submit node ( learn.chtc.wisc.edu ), we have the option of using an interactive job to build the OpenBUGS installation. This is a good option because the submit server is already busy with lots of users and we don't know how long the OpenBUGS install will take. We'll also target specific build servers with extra tools by adding some special requirements to our interactive job. Copy the following lines into a file named build.submit log = build.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = +IsBuildJob = true requirements = (IsBuildSlot == true) request_cpus = 1 request_disk = 2GB request_memory = 2GB queue Note the lack of executable. Condor doesn't need an executable for this job because it will be interactive, meaning you are running the commands instead of Condor. In order to create the installation, we will need the source code to come with us. The transfer_input_files line is blank - fill it in with the name of our Open BUGS source tarball . To request an interactive job, we will add a -i flag to the condor_submit command. The whole command you enter should look like this: username@learn $ condor_submit -i build.submit","title":"Where to Prepare"},{"location":"materials/software/part2-ex2-prepackaged/#read-through-installation-documentation","text":"While you're waiting for the interactive job to start, you can start reading the Open BUGS installation documentation online. Find the installation instructions for Open BUGS. On the downloads page, there are short instructions for how to install Open BUGS. There are two options shown for installation -- which should we use? The first installation option given uses sudo -- which is an administrative permission that you won't have as a normal user. Luckily, as described in the instructions, you can use the --prefix option to set where Open BUGS will be installed, which will allow us to install it without administrative permissions.","title":"Read Through Installation Documentation"},{"location":"materials/software/part2-ex2-prepackaged/#installation","text":"Your interactive job should have started by now and we've learned about installing our program. Let's test it out. Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. username@host $ mkdir openbugs Now run the commands to unpack the source code: username@host $ tar -zxf OpenBUGS-3.2.3.tar.gz username@host $ cd OpenBUGS-3.2.3 Now we can follow the second set of installation instructions. For the prefix, we'll use the variable $PWD to capture the name of our current working directory and then a relative path to the openbugs directory we created in step 1: username@host $ ./configure --prefix = $PWD /../openbugs username@host $ make username@host $ make install Go back to the job's main working directory : username@host $ cd .. and confirm that our installation procedure created bin , lib , and share directories. username@host $ ls openbugs bin lib share Now we want to package up our installation, so we can use it in other jobs. We can do this by compressing any necessary directories into a single gzipped tarball. username@host $ tar -czf openbugs.tar.gz openbugs/ Once everything is complete, type exit to leave the interactive job. Make sure that your tarball is in the main working directory - it will be transferred back to the submit server automatically. username@learn $ exit Note that we now have two tarballs in our directory -- the source tarball ( OpenBUGS-3.2.3.tar.gz ), which we will no longer need and our newly built installation ( openbugs.tar.gz ) which is what we will actually be using to run jobs.","title":"Installation"},{"location":"materials/software/part2-ex2-prepackaged/#wrapper-script","text":"Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in a previous exercise . These steps should be performed back on the submit server ( learn.chtc.wisc.edu ). Create a script called run_openbugs.sh . The script will first need to untar our installation, so the script should start out like this: #!/bin/bash tar -xzf openbugs.tar.gz We're going to use the same $(pwd) trick from the installation in order to tell the computer how to find Open BUGS. We will do this by setting the PATH environment variable, to include the directory where Open BUGS is installed: export PATH = $( pwd ) /openbugs/bin: $PATH Finally, the wrapper script needs to not only setup Open BUGS, but actually run the program. Add the following lines to your run_openbugs.sh wrapper script. OpenBUGS < input.txt > results.txt Make sure the wrapper script has executable permissions: username@learn $ chmod u+x run_openbugs.sh","title":"Wrapper Script"},{"location":"materials/software/part2-ex2-prepackaged/#run-a-open-bugs-job","text":"We're almost ready! We need two more pieces to run a OpenBUGS job. Download the necessary input files to your directory on the submit server and then untar them. username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/osgschool21/openbugs_files.tar.gz username@learn $ tar -xzf openbugs_files.tar.gz Our last step is to create a submit file for our Open BUGS job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from a previous exercise as a base) and modify it as you think necessary. The two most important lines to modify for this job are listed below; check them against your own submit file: executable = run_openbugs.sh transfer_input_files = openbugs.tar.gz, openbugs_files/ A wrapper script will always be a job's executable . When using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files . Note The / in the transfer_input_files line indicates that we are transferring the contents of that directory (which in this case, is what we want), rather than the directory itself. Submit the job with condor_submit . Once the job completes, it should produce a results.txt file.","title":"Run a Open BUGS job"},{"location":"materials/software/part2-ex3-python/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 2.3: Using Python, Pre-Built \u00b6 In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes. Background \u00b6 We chose Python as the language for this example because: a) it is a common language used for scientific computing and b) it has a straightforward installation process and is fairly portable. Running any Python script requires an installation of the Python interpreter. The Python interpreter is what we're using when we type python at the command line. In order to run Python jobs on a distributed system, you will need to install the Python interpreter (what we often refer to as just \"installing Python\"), within the job, then run your Python script. There are two installation approaches. The approach we will cover in this exercise is that of \"pre-building\" the installation. We will install Python to a specific directory, and then create a tarball of that installation directory. We can then use our tarball within jobs to run Python scripts. Interactive Job for Pre-Building \u00b6 The first step in our job process is building a Python installation that we can package up. Create a directory for this exercise on learn.chtc.wisc.edu and cd into it. Download the Python source code from https://www.python.org/ . username@learn $ wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz Of our options - submit server, interactive job, personal computer - which should we use for this installation/packaging process? Once you have a guess, move to the next step. Due to the number of people on our submit server, we shouldn't use the submit server. Your own computer probably doesn't have the right operating system. The best place to install will be an interactive job. For this job, we can use the same interactive submit file as Exercise 2.2, with one change. What is it? Make a copy of the interactive submit file from Exercise 2.2 and change the transfer_input_files line to the Python tarball you just downloaded. Then submit it using the -i flag. username@learn $ condor_submit -i build.submit Once the interactive job begins, we can start our installation process. First, we have to determine how to install Python to a specific location in our working directory. Untar the Python source tarball and look at the README.rst file in the Python-3.7.0 directory. You'll want to look for the \"Build Instructions\" header. What will the main installation steps be? What command is required for the final installation? Once you've tried to answer these questions, move to the next step. There are some basic installation instructions near the top of the README . Based on that short introduction, we can see the main steps of installation will be: ./configure make make test sudo make install This three-stage process (configure, make, make install) is a common way to install many software packages. The default installation location for Python requires sudo (administrative privileges) to install. However, we'd like to install to a specific location in the working directory so that we can compress that installation directory into a tarball. You can often use an option called -prefix with the configure script to change the default installation directory. Let's see if the Python configure script has this option by using the \"help\" option (as suggested in the README.rst file): username@host $ ./configure --help Sure enough, there's a list of all the different options that can be passed to the configure script, which includes --prefix . (To see the --prefix option, you may need to scroll towards the top of the output.) Therefore, we can use the $PWD command in order to set the path correctly to a custom installation directory. Now let's actually install Python! From the job's main working directory , create a directory to hold the installation. username@host $ cd $_CONDOR_SCRATCH_DIR username@host $ mkdir python Move into the Python-3.7.0 directory and run the installation commands. These may take a few minutes each. username@host $ cd Python-3.7.0 username@host $ ./configure --prefix = $PWD /../python username@host $ make username@host $ make install Note The installation instructions in the README.rst file have a make test step between the make and make install steps. As this step isn't strictly necessary (and takes a long time), it's been omitted above. If I move back to the main job working directory, and look in the python subdirectory, I should see a Python installation. username@host $ cd .. username@host $ ls python/ bin include lib share I have successfully created a self-contained Python installation. Now it just needs to be tarred up! username@host $ tar -czf prebuilt_python.tar.gz python/ Before exiting, we might want to know how we installed Python for later reference. Enter the following commands to save our history to a file: username@host $ history > python_install.txt Exit the interactive job. username@host $ exit Python Script \u00b6 Create a script with the following lines called fib.py . import sys import os if len ( sys . argv ) != 2 : print ( 'Usage: %s MAXIMUM' % ( os . path . basename ( sys . argv [ 0 ]))) sys . exit ( 1 ) maximum = int ( sys . argv [ 1 ]) n1 = n2 = 1 while n2 <= maximum : n1 , n2 = n2 , n1 + n2 print ( 'The greatest Fibonacci number up to %d is %d ' % ( maximum , n1 )) What command line arguments does this script take? Try running it on the submit server. Wrapper Script \u00b6 We now have our Python installation and our Python script - we just need to write a wrapper script to run them. What steps do you think the wrapper script needs to perform? Create a file called run_fib.sh and write them out in plain English before moving to the next step. Our script will need to untar our prebuilt_python.tar.gz file access the python command from our installation to run our fib.py script Try turning your plain English steps into commands that the computer can run. Your final run_fib.sh script should look something like this: #!/bin/bash tar -xzf prebuilt_python.tar.gz python/bin/python3 fib.py 90 or #!/bin/bash tar -xzf prebuilt_python.tar.gz export PATH = $( pwd ) /python/bin: $PATH python3 fib.py 90 Make sure your run_fib.sh script is executable. Submit File \u00b6 Make a copy of a previous submit file in your local directory (the OpenBugs submit file could be a good starting point). What changes need to be made to run this Python job? Modify your submit file, then make sure you've included the key lines below: executable = run_fib.sh transfer_input_files = fib.py, prebuilt_python.tar.gz Submit the job using condor_submit . Check the .out file to see if the job completed.","title":"Exercise 2.3"},{"location":"materials/software/part2-ex3-python/#software-exercise-23-using-python-pre-built","text":"In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes.","title":"Software Exercise 2.3: Using Python, Pre-Built"},{"location":"materials/software/part2-ex3-python/#background","text":"We chose Python as the language for this example because: a) it is a common language used for scientific computing and b) it has a straightforward installation process and is fairly portable. Running any Python script requires an installation of the Python interpreter. The Python interpreter is what we're using when we type python at the command line. In order to run Python jobs on a distributed system, you will need to install the Python interpreter (what we often refer to as just \"installing Python\"), within the job, then run your Python script. There are two installation approaches. The approach we will cover in this exercise is that of \"pre-building\" the installation. We will install Python to a specific directory, and then create a tarball of that installation directory. We can then use our tarball within jobs to run Python scripts.","title":"Background"},{"location":"materials/software/part2-ex3-python/#interactive-job-for-pre-building","text":"The first step in our job process is building a Python installation that we can package up. Create a directory for this exercise on learn.chtc.wisc.edu and cd into it. Download the Python source code from https://www.python.org/ . username@learn $ wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz Of our options - submit server, interactive job, personal computer - which should we use for this installation/packaging process? Once you have a guess, move to the next step. Due to the number of people on our submit server, we shouldn't use the submit server. Your own computer probably doesn't have the right operating system. The best place to install will be an interactive job. For this job, we can use the same interactive submit file as Exercise 2.2, with one change. What is it? Make a copy of the interactive submit file from Exercise 2.2 and change the transfer_input_files line to the Python tarball you just downloaded. Then submit it using the -i flag. username@learn $ condor_submit -i build.submit Once the interactive job begins, we can start our installation process. First, we have to determine how to install Python to a specific location in our working directory. Untar the Python source tarball and look at the README.rst file in the Python-3.7.0 directory. You'll want to look for the \"Build Instructions\" header. What will the main installation steps be? What command is required for the final installation? Once you've tried to answer these questions, move to the next step. There are some basic installation instructions near the top of the README . Based on that short introduction, we can see the main steps of installation will be: ./configure make make test sudo make install This three-stage process (configure, make, make install) is a common way to install many software packages. The default installation location for Python requires sudo (administrative privileges) to install. However, we'd like to install to a specific location in the working directory so that we can compress that installation directory into a tarball. You can often use an option called -prefix with the configure script to change the default installation directory. Let's see if the Python configure script has this option by using the \"help\" option (as suggested in the README.rst file): username@host $ ./configure --help Sure enough, there's a list of all the different options that can be passed to the configure script, which includes --prefix . (To see the --prefix option, you may need to scroll towards the top of the output.) Therefore, we can use the $PWD command in order to set the path correctly to a custom installation directory. Now let's actually install Python! From the job's main working directory , create a directory to hold the installation. username@host $ cd $_CONDOR_SCRATCH_DIR username@host $ mkdir python Move into the Python-3.7.0 directory and run the installation commands. These may take a few minutes each. username@host $ cd Python-3.7.0 username@host $ ./configure --prefix = $PWD /../python username@host $ make username@host $ make install Note The installation instructions in the README.rst file have a make test step between the make and make install steps. As this step isn't strictly necessary (and takes a long time), it's been omitted above. If I move back to the main job working directory, and look in the python subdirectory, I should see a Python installation. username@host $ cd .. username@host $ ls python/ bin include lib share I have successfully created a self-contained Python installation. Now it just needs to be tarred up! username@host $ tar -czf prebuilt_python.tar.gz python/ Before exiting, we might want to know how we installed Python for later reference. Enter the following commands to save our history to a file: username@host $ history > python_install.txt Exit the interactive job. username@host $ exit","title":"Interactive Job for Pre-Building"},{"location":"materials/software/part2-ex3-python/#python-script","text":"Create a script with the following lines called fib.py . import sys import os if len ( sys . argv ) != 2 : print ( 'Usage: %s MAXIMUM' % ( os . path . basename ( sys . argv [ 0 ]))) sys . exit ( 1 ) maximum = int ( sys . argv [ 1 ]) n1 = n2 = 1 while n2 <= maximum : n1 , n2 = n2 , n1 + n2 print ( 'The greatest Fibonacci number up to %d is %d ' % ( maximum , n1 )) What command line arguments does this script take? Try running it on the submit server.","title":"Python Script"},{"location":"materials/software/part2-ex3-python/#wrapper-script","text":"We now have our Python installation and our Python script - we just need to write a wrapper script to run them. What steps do you think the wrapper script needs to perform? Create a file called run_fib.sh and write them out in plain English before moving to the next step. Our script will need to untar our prebuilt_python.tar.gz file access the python command from our installation to run our fib.py script Try turning your plain English steps into commands that the computer can run. Your final run_fib.sh script should look something like this: #!/bin/bash tar -xzf prebuilt_python.tar.gz python/bin/python3 fib.py 90 or #!/bin/bash tar -xzf prebuilt_python.tar.gz export PATH = $( pwd ) /python/bin: $PATH python3 fib.py 90 Make sure your run_fib.sh script is executable.","title":"Wrapper Script"},{"location":"materials/software/part2-ex3-python/#submit-file","text":"Make a copy of a previous submit file in your local directory (the OpenBugs submit file could be a good starting point). What changes need to be made to run this Python job? Modify your submit file, then make sure you've included the key lines below: executable = run_fib.sh transfer_input_files = fib.py, prebuilt_python.tar.gz Submit the job using condor_submit . Check the .out file to see if the job completed.","title":"Submit File"},{"location":"materials/software/part2-ex4-matlab/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 2.4: Running Compiled Matlab \u00b6 The goal of this exercise is to compile Matlab code and run it. This exercise will draw on the idea of writing a wrapper script to install and run code, first introduced in Exercise 1.2 and should take 25-30 minutes. Background \u00b6 Matlab is licensed; however, unlike most licensed software, it has the ability to be compiled and the compiled code can be run without a license. We will be compiling Matlab .m files into a binary file and running that binary using a set of files called the Matlab runtime. Matlab Code \u00b6 Log in to the CHTC submit server ( learn.chtc.wisc.edu ). Create a directory for this exercise and cd into it . Copy the following code into a file called matrix.m A = randi(100,4,4) b = randi(100,4,1); x = A*b save results.txt x -ascii Compiling Matlab Code \u00b6 The first step in making Matlab portable is compiling our Matlab script. To compile this code, we need to access the machines with the Matlab compiler installed. For this exercise, we will use the compilers installed on special CHTC build machines. In the CHTC pool, you can't use ssh to directly connect to these machines. Instead, you must submit an interactive job (similar to Exercise 2.2 ) that specifically requests these build machines. Create a file called compile.submit with the lines below: log = compile.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = matrix.m +IsBuildJob = true request_memory = 1GB request_disk = 512MB queue You can initiate the interactive job by using condor_submit 's -i option. Enter the following command: username@learn $ condor_submit -i compile.submit Make sure you've submitted this command from learn.chtc.wisc.edu ! Once the job starts, continue with the following instructions. Since you are a guest user on our system, you will need to set your HOME directory by running this command: username@build $ export HOME = $PWD The Matlab software on these build servers is accessible via modules, just like the software installed on OSG Connect. Check which modules are available and then load the older version of Matlab. username@build $ module load MATLAB/R2015b Once the module is loaded (you can check by running module list ), compile the matrix.m file with this command: username@build $ mcc -m -R -singleCompThread -R -nodisplay -R -nojvm matrix.m The extra arguments to the mcc command are very important here. Matlab, by default, will run on as many CPUs as it can find. This can be a big problem when running on someone else's computers, because your Matlab code might interfere with what the owner wants. The -singleCompThread option compiles the code to run on a single CPU, avoiding this problem. In addition, the -nodisplay and -nojvm options turn off the display (which won't exist where the code runs). To exit the interactive session, type exit Now that you're back on the submit server, look at the files that were created by the Matlab compiler. Which one is the compiled binary? Matlab Runtime \u00b6 The newly compiled binary will require the 2015b Matlab runtime to run. You can download the runtime from the Mathworks website and build it yourself, but to save time, for this exercise you can use the pre-built runtimes hosted by CHTC. Download the 2015b Matlab runtime hosted by CHTC: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/r2015b.tar.gz Wrapper Script \u00b6 We will need a wrapper script to open the Matlab runtime and then run our compiled Matlab code. Our wrapper script will need to accomplish the following steps: Unpack the transferred runtime Set the environment variables Run our compiled matlab code Fortunately, the Matlab compiler has pre-written most of this wrapper script for us! Take a look at run_matrix.sh . Which of the above steps do we need to add? Once you have an idea, move to the next step. We'll need to add commands to unpack the runtime (which will have been transferred with the job). Add this line to the beginning of the run_matrix.sh file, after #!/bin/bash and the comments, but before exe_name=$0 : tar -xzf r2015b.tar.gz Look at readme.txt to determine what arguments our wrapper script requires. Once you have an idea, move to the next step. The name of the Matlab runtime directory is a required argument to the wrapper script run_matrix.sh . We'll have to do a little extra work to find out the name of that directory. Run this command tar -tf r2015b.tar.gz The output of the previous command is a list of all the files in the tar.gz file. What is the name of the first folder of the path for each file? This is the name of the runtime directory, and the argument you should pass to run_matrix.sh . Submitting the Job \u00b6 Copy an existing submit file into your current directory. The submit file we used for Exercise 2.2 example would be a good candidate, as that example also used a wrapper script. Modify your submit file for this job. Check your changes against the list below. The executable for this job is going to be our wrapper script run_matrix.sh . executable = run_matrix.sh You need to transfer the compiled binary matrix , as well as the runtime .tar.gz file, using transfer_input_files . transfer_input_files = matrix, r2015b.tar.gz The argument for the executable ( run_matrix.sh ) is \"v90\", as that is the name of the un-tarred runtime directory. arguments = v90 We need to request plenty of disk space for the runtime. request_disk = 2GB Submit the job using condor_submit . After it completes, the job should have produced a file called results.txt .","title":"Exercise 2.4"},{"location":"materials/software/part2-ex4-matlab/#software-exercise-24-running-compiled-matlab","text":"The goal of this exercise is to compile Matlab code and run it. This exercise will draw on the idea of writing a wrapper script to install and run code, first introduced in Exercise 1.2 and should take 25-30 minutes.","title":"Software Exercise 2.4: Running Compiled Matlab"},{"location":"materials/software/part2-ex4-matlab/#background","text":"Matlab is licensed; however, unlike most licensed software, it has the ability to be compiled and the compiled code can be run without a license. We will be compiling Matlab .m files into a binary file and running that binary using a set of files called the Matlab runtime.","title":"Background"},{"location":"materials/software/part2-ex4-matlab/#matlab-code","text":"Log in to the CHTC submit server ( learn.chtc.wisc.edu ). Create a directory for this exercise and cd into it . Copy the following code into a file called matrix.m A = randi(100,4,4) b = randi(100,4,1); x = A*b save results.txt x -ascii","title":"Matlab Code"},{"location":"materials/software/part2-ex4-matlab/#compiling-matlab-code","text":"The first step in making Matlab portable is compiling our Matlab script. To compile this code, we need to access the machines with the Matlab compiler installed. For this exercise, we will use the compilers installed on special CHTC build machines. In the CHTC pool, you can't use ssh to directly connect to these machines. Instead, you must submit an interactive job (similar to Exercise 2.2 ) that specifically requests these build machines. Create a file called compile.submit with the lines below: log = compile.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = matrix.m +IsBuildJob = true request_memory = 1GB request_disk = 512MB queue You can initiate the interactive job by using condor_submit 's -i option. Enter the following command: username@learn $ condor_submit -i compile.submit Make sure you've submitted this command from learn.chtc.wisc.edu ! Once the job starts, continue with the following instructions. Since you are a guest user on our system, you will need to set your HOME directory by running this command: username@build $ export HOME = $PWD The Matlab software on these build servers is accessible via modules, just like the software installed on OSG Connect. Check which modules are available and then load the older version of Matlab. username@build $ module load MATLAB/R2015b Once the module is loaded (you can check by running module list ), compile the matrix.m file with this command: username@build $ mcc -m -R -singleCompThread -R -nodisplay -R -nojvm matrix.m The extra arguments to the mcc command are very important here. Matlab, by default, will run on as many CPUs as it can find. This can be a big problem when running on someone else's computers, because your Matlab code might interfere with what the owner wants. The -singleCompThread option compiles the code to run on a single CPU, avoiding this problem. In addition, the -nodisplay and -nojvm options turn off the display (which won't exist where the code runs). To exit the interactive session, type exit Now that you're back on the submit server, look at the files that were created by the Matlab compiler. Which one is the compiled binary?","title":"Compiling Matlab Code"},{"location":"materials/software/part2-ex4-matlab/#matlab-runtime","text":"The newly compiled binary will require the 2015b Matlab runtime to run. You can download the runtime from the Mathworks website and build it yourself, but to save time, for this exercise you can use the pre-built runtimes hosted by CHTC. Download the 2015b Matlab runtime hosted by CHTC: username@learn $ wget http://proxy.chtc.wisc.edu/SQUID/r2015b.tar.gz","title":"Matlab Runtime"},{"location":"materials/software/part2-ex4-matlab/#wrapper-script","text":"We will need a wrapper script to open the Matlab runtime and then run our compiled Matlab code. Our wrapper script will need to accomplish the following steps: Unpack the transferred runtime Set the environment variables Run our compiled matlab code Fortunately, the Matlab compiler has pre-written most of this wrapper script for us! Take a look at run_matrix.sh . Which of the above steps do we need to add? Once you have an idea, move to the next step. We'll need to add commands to unpack the runtime (which will have been transferred with the job). Add this line to the beginning of the run_matrix.sh file, after #!/bin/bash and the comments, but before exe_name=$0 : tar -xzf r2015b.tar.gz Look at readme.txt to determine what arguments our wrapper script requires. Once you have an idea, move to the next step. The name of the Matlab runtime directory is a required argument to the wrapper script run_matrix.sh . We'll have to do a little extra work to find out the name of that directory. Run this command tar -tf r2015b.tar.gz The output of the previous command is a list of all the files in the tar.gz file. What is the name of the first folder of the path for each file? This is the name of the runtime directory, and the argument you should pass to run_matrix.sh .","title":"Wrapper Script"},{"location":"materials/software/part2-ex4-matlab/#submitting-the-job","text":"Copy an existing submit file into your current directory. The submit file we used for Exercise 2.2 example would be a good candidate, as that example also used a wrapper script. Modify your submit file for this job. Check your changes against the list below. The executable for this job is going to be our wrapper script run_matrix.sh . executable = run_matrix.sh You need to transfer the compiled binary matrix , as well as the runtime .tar.gz file, using transfer_input_files . transfer_input_files = matrix, r2015b.tar.gz The argument for the executable ( run_matrix.sh ) is \"v90\", as that is the name of the un-tarred runtime directory. arguments = v90 We need to request plenty of disk space for the runtime. request_disk = 2GB Submit the job using condor_submit . After it completes, the job should have produced a file called results.txt .","title":"Submitting the Job"},{"location":"materials/software/part2-ex5-conda/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 2.5: Using Conda Environments (Beta!) \u00b6 This exercise covers how to use Python environments managed by miniconda. Introduction \u00b6 Many Python users manage their Python installation and environments with either the Anaconda or miniconda distributions. These distribution tools are great for creating portable Python installations and can be used on HTC systems with some help from a tool called conda pack . Create and Pack a Conda Environment \u00b6 (For a generic version of these instructions, see the CHTC User Guide ) Our first step is to create a miniconda installation on the submit server. Log into either learn.chtc.wisc.edu or login04.osgconnect.net Download the latest Linux miniconda installer user@login $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Run the installer to install miniconda; you'll need to accept the license terms and you can use the default installation location: [user@login]$ sh Miniconda3-latest-Linux-x86_64.sh At the end, you can choose whether or not to \"initialize Miniconda3 by running conda init?\" The default is no; you would then run the eval command listed by the installer to \"activate\" Miniconda. If you choose \"no\" you'll want to save this command so that you can reactivate the Miniconda installation when needed in the future. Next we'll create our conda \"environment\" with numpy (we've called the environment \"py3-numpy\"): (base) [user@login]$ conda create -n py3-numpy (base) [user@login]$ conda activate py3-numpy (py3-numpy) [user@login]$ conda install numpy Once everything is installed, deactivate the environment to go back to the Miniconda \"base\" environment. (py3-numpy) [user@login]$ conda deactivate We'll now install a tool that will pack up the just created conda environment so we can run it elsewhere. Make sure that your job's Miniconda environment is created, but deactivated, so that you're in the \"base\" Miniconda environment, then run: (base) [user@login]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, we will run the conda pack command, which will automatically create a tar.gz file with our environment: (base) [user@login]$ conda pack -n py3-numpy Submit a Job \u00b6 Create a copy of the rand_array.py script from the Docker Build Exercise The executable for this job will need to be a wrapper script. What steps do you think need to be included? Write down a rough draft, then compare with the following script. Create a wrapper script like the following: #!/bin/bash set -e export PATH mkdir py3-numpy tar -xzf py3-numpy.tar.gz -C py3-numpy . py3-numpy/bin/activate python3 rand_array.py What needs to be included in your submit file for the job to run successfully? Try yourself and then check the suggestions in the next point. In your submit file, make sure to have the following: Your executable should be the the bash script you created in the previous step. Remember to transfer your Python script and the environment tar.gz file via transfer_input_files . Submit the job and see what happens!","title":"Part2 ex5 conda"},{"location":"materials/software/part2-ex5-conda/#software-exercise-25-using-conda-environments-beta","text":"This exercise covers how to use Python environments managed by miniconda.","title":"Software Exercise 2.5: Using Conda Environments (Beta!)"},{"location":"materials/software/part2-ex5-conda/#introduction","text":"Many Python users manage their Python installation and environments with either the Anaconda or miniconda distributions. These distribution tools are great for creating portable Python installations and can be used on HTC systems with some help from a tool called conda pack .","title":"Introduction"},{"location":"materials/software/part2-ex5-conda/#create-and-pack-a-conda-environment","text":"(For a generic version of these instructions, see the CHTC User Guide ) Our first step is to create a miniconda installation on the submit server. Log into either learn.chtc.wisc.edu or login04.osgconnect.net Download the latest Linux miniconda installer user@login $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Run the installer to install miniconda; you'll need to accept the license terms and you can use the default installation location: [user@login]$ sh Miniconda3-latest-Linux-x86_64.sh At the end, you can choose whether or not to \"initialize Miniconda3 by running conda init?\" The default is no; you would then run the eval command listed by the installer to \"activate\" Miniconda. If you choose \"no\" you'll want to save this command so that you can reactivate the Miniconda installation when needed in the future. Next we'll create our conda \"environment\" with numpy (we've called the environment \"py3-numpy\"): (base) [user@login]$ conda create -n py3-numpy (base) [user@login]$ conda activate py3-numpy (py3-numpy) [user@login]$ conda install numpy Once everything is installed, deactivate the environment to go back to the Miniconda \"base\" environment. (py3-numpy) [user@login]$ conda deactivate We'll now install a tool that will pack up the just created conda environment so we can run it elsewhere. Make sure that your job's Miniconda environment is created, but deactivated, so that you're in the \"base\" Miniconda environment, then run: (base) [user@login]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, we will run the conda pack command, which will automatically create a tar.gz file with our environment: (base) [user@login]$ conda pack -n py3-numpy","title":"Create and Pack a Conda Environment"},{"location":"materials/software/part2-ex5-conda/#submit-a-job","text":"Create a copy of the rand_array.py script from the Docker Build Exercise The executable for this job will need to be a wrapper script. What steps do you think need to be included? Write down a rough draft, then compare with the following script. Create a wrapper script like the following: #!/bin/bash set -e export PATH mkdir py3-numpy tar -xzf py3-numpy.tar.gz -C py3-numpy . py3-numpy/bin/activate python3 rand_array.py What needs to be included in your submit file for the job to run successfully? Try yourself and then check the suggestions in the next point. In your submit file, make sure to have the following: Your executable should be the the bash script you created in the previous step. Remember to transfer your Python script and the environment tar.gz file via transfer_input_files . Submit the job and see what happens!","title":"Submit a Job"},{"location":"materials/software/part3-ex1-singularity/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 3.1: Use Singularity from OSG Connect \u00b6 Background \u00b6 Containers are another way to manage software installations. We don't have the time to go fully into the details of building and using containers, but can use pre-existing containers to run jobs. One caveat for using containers: not all systems will support them. HTCondor has built-in features for using Docker and many Open Science Grid resources have Singularity installed, but they are not always available everywhere. Setup \u00b6 Make sure you are logged into login04.osgconnect.net (the OSG Connect submit server for this workshop). For this exercise we will be using Singularity containers that are hosted by OSG Connect, in a very similar way to the software modules. To get an idea on what container images are available on the OSG, take a look at the directory path /cvmfs/singularity.opensciencegrid.org/opensciencegrid . Job Submission \u00b6 For this job, we will use the OSG Connect Ubuntu \"Xenial\" image. Copy a submit file you used for a previous exercise on OSG Connect and add the following lines: requirements = HAS_SINGULARITY == true +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest\" If you had other requirements in the submit file, remove them. These options will do two things: require that your job runs on servers that have Singularity installed and can access the OSG Connect repository of Singularity containers tells the job which Singularity container to use To test and see if our job is really running in Ubuntu, use this script for the job's executable: #!/bin/bash hostname lsb_release -a Submit the job and look at the output file.","title":"Exercise 3.1"},{"location":"materials/software/part3-ex1-singularity/#software-exercise-31-use-singularity-from-osg-connect","text":"","title":"Software Exercise 3.1: Use Singularity from OSG Connect"},{"location":"materials/software/part3-ex1-singularity/#background","text":"Containers are another way to manage software installations. We don't have the time to go fully into the details of building and using containers, but can use pre-existing containers to run jobs. One caveat for using containers: not all systems will support them. HTCondor has built-in features for using Docker and many Open Science Grid resources have Singularity installed, but they are not always available everywhere.","title":"Background"},{"location":"materials/software/part3-ex1-singularity/#setup","text":"Make sure you are logged into login04.osgconnect.net (the OSG Connect submit server for this workshop). For this exercise we will be using Singularity containers that are hosted by OSG Connect, in a very similar way to the software modules. To get an idea on what container images are available on the OSG, take a look at the directory path /cvmfs/singularity.opensciencegrid.org/opensciencegrid .","title":"Setup"},{"location":"materials/software/part3-ex1-singularity/#job-submission","text":"For this job, we will use the OSG Connect Ubuntu \"Xenial\" image. Copy a submit file you used for a previous exercise on OSG Connect and add the following lines: requirements = HAS_SINGULARITY == true +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest\" If you had other requirements in the submit file, remove them. These options will do two things: require that your job runs on servers that have Singularity installed and can access the OSG Connect repository of Singularity containers tells the job which Singularity container to use To test and see if our job is really running in Ubuntu, use this script for the job's executable: #!/bin/bash hostname lsb_release -a Submit the job and look at the output file.","title":"Job Submission"},{"location":"materials/software/part4-ex1-python-extras/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 4.1: More Python Job Features \u00b6 If you have time and did the previous Python exercise ( Exercise 2.3 ), try one (or more) of these exercises. Arguments \u00b6 Similar to what was shown in a previous bonus exercise , can you modify your submit file and shell script so that the number provided to the fib.py script is an argument from the submit file? Multiple Jobs \u00b6 Given this list of numbers: 0 25 80 110 250 3000 Can you submit a job for each number? Packages \u00b6 We haven't talked about adding Python packages (like numpy or matplotlib ) to the Python installation. Where would that go in the installation process? Try creating a Python installation that includes the numpy package.","title":"Exercise 4.1"},{"location":"materials/software/part4-ex1-python-extras/#software-exercise-41-more-python-job-features","text":"If you have time and did the previous Python exercise ( Exercise 2.3 ), try one (or more) of these exercises.","title":"Software Exercise 4.1: More Python Job Features"},{"location":"materials/software/part4-ex1-python-extras/#arguments","text":"Similar to what was shown in a previous bonus exercise , can you modify your submit file and shell script so that the number provided to the fib.py script is an argument from the submit file?","title":"Arguments"},{"location":"materials/software/part4-ex1-python-extras/#multiple-jobs","text":"Given this list of numbers: 0 25 80 110 250 3000 Can you submit a job for each number?","title":"Multiple Jobs"},{"location":"materials/software/part4-ex1-python-extras/#packages","text":"We haven't talked about adding Python packages (like numpy or matplotlib ) to the Python installation. Where would that go in the installation process? Try creating a Python installation that includes the numpy package.","title":"Packages"},{"location":"materials/software/part4-ex2-docker/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 4.2: Using Docker \u00b6 In this exercise, you will run the same Python script as the other Python exercises, but using a Docker container. Setup \u00b6 For this exercise, you will need to be logged into learn.chtc.wisc.edu , not login04.osgconnect.net . If you haven't done one of the previous Python exercises, make sure to get a copy of the script and recommended commands here . Submit File Changes \u00b6 Make a copy of your submit file from the previous Python exercise or build from an existing submit file. Add the following lines to the submit file or modify existing lines to match the lines below: universe = docker docker_image = python:3.7.0-stretch Here we are requesting HTCondor's Docker universe and using a pre-built python image that, by default, will be pulled from a public website of Docker images called DockerHub. The requirements line will ensure that we run on computers whose operating system can support Docker. Adjust the executable and arguments lines. The executable can now be the Python script itself, with the appropriate arguments: executable = fib.py arguments = 90 Finally, we no longer need to transfer a Python tarball (whether source code or pre-built) or our Python script. You can remove both from the transfer_input_files line of the submit file if it's already present. Python Script \u00b6 Open the Python script and add the following line at the top: #!/usr/bin/env python3 This will ensure that the script uses the version of Python that comes in the Docker container. Once these steps are done, submit the job.","title":"Exercise 4.2"},{"location":"materials/software/part4-ex2-docker/#software-exercise-42-using-docker","text":"In this exercise, you will run the same Python script as the other Python exercises, but using a Docker container.","title":"Software Exercise 4.2: Using Docker"},{"location":"materials/software/part4-ex2-docker/#setup","text":"For this exercise, you will need to be logged into learn.chtc.wisc.edu , not login04.osgconnect.net . If you haven't done one of the previous Python exercises, make sure to get a copy of the script and recommended commands here .","title":"Setup"},{"location":"materials/software/part4-ex2-docker/#submit-file-changes","text":"Make a copy of your submit file from the previous Python exercise or build from an existing submit file. Add the following lines to the submit file or modify existing lines to match the lines below: universe = docker docker_image = python:3.7.0-stretch Here we are requesting HTCondor's Docker universe and using a pre-built python image that, by default, will be pulled from a public website of Docker images called DockerHub. The requirements line will ensure that we run on computers whose operating system can support Docker. Adjust the executable and arguments lines. The executable can now be the Python script itself, with the appropriate arguments: executable = fib.py arguments = 90 Finally, we no longer need to transfer a Python tarball (whether source code or pre-built) or our Python script. You can remove both from the transfer_input_files line of the submit file if it's already present.","title":"Submit File Changes"},{"location":"materials/software/part4-ex2-docker/#python-script","text":"Open the Python script and add the following line at the top: #!/usr/bin/env python3 This will ensure that the script uses the version of Python that comes in the Docker container. Once these steps are done, submit the job.","title":"Python Script"},{"location":"materials/software/part4-ex3-docker-build/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 4.3: Building Your Own Docker Container (Beta!) \u00b6 This exercise will walk you through the steps to build your own Docker container based on Python, with the numpy Python library added on. Sample Script \u00b6 For this example, create a script called rand_array.py on learn.chtc.wisc.edu : import numpy as np #numpy array with random values a = np.random.rand(4,2,3) print(a) To run this script, we will need a copy of Python with the numpy library. Getting Set Up \u00b6 Before building your own Docker container, you need to go through the following set up steps: Install Docker Dekstop on your computer. Docker Desktop page You may need to create a Docker Hub user name to download Docker Desktop; if not created at that step, create a user name for Docker Hub now. (Optional): Once Docker is up and running on your computer, you are welcome to take some time to explore the basics of downloading and running a container, as shown in the initial sections of this Docker lesson: Introduction to Docker However, this isn't strictly necessary for building your own container. Building a Container \u00b6 In order to make our container reproducible, we will be using Docker's capability to build a container image from a specification file. First, create an empty build directory on your computer , not the CHTC or OSG submit servers. In the build directory, create a file called Dockerfile (no file extension!) with the following contents: # Start with this image as a \"base\". # It's as if all the commands that created that image were inserted here. # Always use a specific tag like \"4.7.12\", never \"latest\"! # The version referenced by \"latest\" can change, so the build will be # more stable when building from a specific version tag. FROM continuumio/miniconda:4.7.12 # Use RUN to execute commands inside the image as it is being built up. RUN conda install --yes numpy # RUN multiple commands together. # Try to always \"clean up\" after yourself to reduce the final size of your image. RUN apt-get update \\ && apt-get --yes install --no-install-recommends graphviz\\ && apt-get --yes clean \\ && rm -rf /var/lib/apt/lists/* This is our specification file and provides Docker with the information it needs to build our new container. There are other options besides FROM and RUN ; see the Docker documentation for more information. Note that our container is starting from an existing container continuumio/miniconda:4.7.12 . This container is produced by the continuumio organization; the number 4.7.12 indicates the container version. When we create our new container, we will want to use a similar naming scheme of: USERNAME/CONTAINER:VERSIONTAG In what follows, you will want to replace USERNAME with your DockerHub user name. The CONTAINER name and VERSIONTAG are your choice; in what follows, we will use py3-numpy as the container name and 2020-07 as the version tag. To build and name the new container, open a command line window on your computer where you can run Docker commands. Use the cd command to change your working directory to the build directory with the Dockerfile inside. $ docker build -t USERNAME/py3-numpy:2020-07 . Note the . at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside. Upload Container and Submit Job \u00b6 Right now the container image only exists on your computer. To use it in CHTC or elsewhere, it needs to be added to a public registry like Docker Hub. To put your container image in Docker Hub, use the docker push command on the command line: $ docker push USERNAME/py3-numpy:2020-07 If the push doesn't work, you may need to run docker login first, enter your Docker Hub username and password and then try the push again. Once your container image is in DockerHub, you can use it in jobs as described in [Exercise 4.3][/materials/sw/part4-ex3-docker]. Thanks to Josh Karpel for providing the sample Dockerfile !","title":"Exercise 4.3"},{"location":"materials/software/part4-ex3-docker-build/#software-exercise-43-building-your-own-docker-container-beta","text":"This exercise will walk you through the steps to build your own Docker container based on Python, with the numpy Python library added on.","title":"Software Exercise 4.3: Building Your Own Docker Container (Beta!)"},{"location":"materials/software/part4-ex3-docker-build/#sample-script","text":"For this example, create a script called rand_array.py on learn.chtc.wisc.edu : import numpy as np #numpy array with random values a = np.random.rand(4,2,3) print(a) To run this script, we will need a copy of Python with the numpy library.","title":"Sample Script"},{"location":"materials/software/part4-ex3-docker-build/#getting-set-up","text":"Before building your own Docker container, you need to go through the following set up steps: Install Docker Dekstop on your computer. Docker Desktop page You may need to create a Docker Hub user name to download Docker Desktop; if not created at that step, create a user name for Docker Hub now. (Optional): Once Docker is up and running on your computer, you are welcome to take some time to explore the basics of downloading and running a container, as shown in the initial sections of this Docker lesson: Introduction to Docker However, this isn't strictly necessary for building your own container.","title":"Getting Set Up"},{"location":"materials/software/part4-ex3-docker-build/#building-a-container","text":"In order to make our container reproducible, we will be using Docker's capability to build a container image from a specification file. First, create an empty build directory on your computer , not the CHTC or OSG submit servers. In the build directory, create a file called Dockerfile (no file extension!) with the following contents: # Start with this image as a \"base\". # It's as if all the commands that created that image were inserted here. # Always use a specific tag like \"4.7.12\", never \"latest\"! # The version referenced by \"latest\" can change, so the build will be # more stable when building from a specific version tag. FROM continuumio/miniconda:4.7.12 # Use RUN to execute commands inside the image as it is being built up. RUN conda install --yes numpy # RUN multiple commands together. # Try to always \"clean up\" after yourself to reduce the final size of your image. RUN apt-get update \\ && apt-get --yes install --no-install-recommends graphviz\\ && apt-get --yes clean \\ && rm -rf /var/lib/apt/lists/* This is our specification file and provides Docker with the information it needs to build our new container. There are other options besides FROM and RUN ; see the Docker documentation for more information. Note that our container is starting from an existing container continuumio/miniconda:4.7.12 . This container is produced by the continuumio organization; the number 4.7.12 indicates the container version. When we create our new container, we will want to use a similar naming scheme of: USERNAME/CONTAINER:VERSIONTAG In what follows, you will want to replace USERNAME with your DockerHub user name. The CONTAINER name and VERSIONTAG are your choice; in what follows, we will use py3-numpy as the container name and 2020-07 as the version tag. To build and name the new container, open a command line window on your computer where you can run Docker commands. Use the cd command to change your working directory to the build directory with the Dockerfile inside. $ docker build -t USERNAME/py3-numpy:2020-07 . Note the . at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside.","title":"Building a Container"},{"location":"materials/software/part4-ex3-docker-build/#upload-container-and-submit-job","text":"Right now the container image only exists on your computer. To use it in CHTC or elsewhere, it needs to be added to a public registry like Docker Hub. To put your container image in Docker Hub, use the docker push command on the command line: $ docker push USERNAME/py3-numpy:2020-07 If the push doesn't work, you may need to run docker login first, enter your Docker Hub username and password and then try the push again. Once your container image is in DockerHub, you can use it in jobs as described in [Exercise 4.3][/materials/sw/part4-ex3-docker]. Thanks to Josh Karpel for providing the sample Dockerfile !","title":"Upload Container and Submit Job"},{"location":"materials/workflows/part1-ex1-simple-dag/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Workflows Exercise 1.1: Coordinating a Set of Jobs With a Simple DAG \u00b6 The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. What is DAGMan? \u00b6 In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. This might look like this, assuming you want to sweep over five parameters: DAGMan has many abilities, such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the HTCondor manual . Submitting a Simple DAG \u00b6 For our job, we will return briefly to the sleep program. executable = /bin/sleep arguments = 4 log = simple.log output = simple.out error = simple.error request_memory = 1GB request_disk = 1GB request_cpus = 1 queue We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window, you'll submit the job. In another you will watch the queue, and in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put this into a file named simple.dag . Job Simple simple.sub In your first window, submit the DAG: username@learn $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, check the queue (what you see may be slightly different): username@learn $ condor_q -nobatch -wide:80 -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended In the third window, watch what DAGMan does (what you see may be slightly different): username@learn $ tail -f --lines = 500 simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... Here's where the job is submitted 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) Here's where DAGMan noticed that the job is running 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) Here's where DAGMan noticed that the job finished. 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held Here's where DAGMan noticed that all the work is done. 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: username@learn $ cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... Looking at DAGMan's various files, we see that DAGMan itself ran as a job (specifically, a \"scheduler\" universe job). username@learn $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out username@learn $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal = ? = 11 || ( ExitCode = ! = UNDEFINED && ExitCode > = 0 && ExitCode < = 2 )) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed ( e.g., during a reboot ) . on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue If you want to clean up some of these files (you may not want to, at least not yet), run: username@learn $ rm simple.dag.* Challenge \u00b6 What is the scheduler universe? Why does DAGMan use it?","title":"Exercise 1.1"},{"location":"materials/workflows/part1-ex1-simple-dag/#workflows-exercise-11-coordinating-a-set-of-jobs-with-a-simple-dag","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Workflows Exercise 1.1: Coordinating a Set of Jobs With a Simple DAG"},{"location":"materials/workflows/part1-ex1-simple-dag/#what-is-dagman","text":"In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. This might look like this, assuming you want to sweep over five parameters: DAGMan has many abilities, such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the HTCondor manual .","title":"What is DAGMan?"},{"location":"materials/workflows/part1-ex1-simple-dag/#submitting-a-simple-dag","text":"For our job, we will return briefly to the sleep program. executable = /bin/sleep arguments = 4 log = simple.log output = simple.out error = simple.error request_memory = 1GB request_disk = 1GB request_cpus = 1 queue We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window, you'll submit the job. In another you will watch the queue, and in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put this into a file named simple.dag . Job Simple simple.sub In your first window, submit the DAG: username@learn $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, check the queue (what you see may be slightly different): username@learn $ condor_q -nobatch -wide:80 -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended In the third window, watch what DAGMan does (what you see may be slightly different): username@learn $ tail -f --lines = 500 simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... Here's where the job is submitted 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) Here's where DAGMan noticed that the job is running 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) Here's where DAGMan noticed that the job finished. 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held Here's where DAGMan noticed that all the work is done. 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: username@learn $ cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... Looking at DAGMan's various files, we see that DAGMan itself ran as a job (specifically, a \"scheduler\" universe job). username@learn $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out username@learn $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal = ? = 11 || ( ExitCode = ! = UNDEFINED && ExitCode > = 0 && ExitCode < = 2 )) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed ( e.g., during a reboot ) . on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue If you want to clean up some of these files (you may not want to, at least not yet), run: username@learn $ rm simple.dag.*","title":"Submitting a Simple DAG"},{"location":"materials/workflows/part1-ex1-simple-dag/#challenge","text":"What is the scheduler universe? Why does DAGMan use it?","title":"Challenge"},{"location":"materials/workflows/part1-ex2-mandelbrot/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Workflows Exercise 1.2: A Brief Detour Through the Mandelbrot Set \u00b6 Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures! We have a small program that draws pictures of the Mandelbrot set. You can read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal. We have a simple program that can draw the Mandelbrot set. It's called goatbrot . Before beginning, ensure that you are connected to learn.chtc.wisc.edu . Create a directory for this exercise and cd into it. Running goatbrot From the Command Line \u00b6 You can generate the Mandelbrot set as a quick test with two simple commands. Generate a PPM image of the Mandelbrot set: username@learn $ goatbrot -i 1000 -o tile_000000_000000.ppm -c 0 ,0 -w 3 -s 1000 ,1000 The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Convert the image to the JPEG format (using a built-in program called convert ): username@learn $ convert tile_000000_000000.ppm mandel.jpg Dividing the Work into Smaller Pieces \u00b6 The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations (an HTC approach!) then stitched them together? Once we do that, we can run each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times: username@learn $ goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1 .5 -s 500 ,500 username@learn $ goatbrot -i 1000 -o tile_000000_000001.ppm -c 0 .75,0.75 -w 1 .5 -s 500 ,500 username@learn $ goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1 .5 -s 500 ,500 username@learn $ goatbrot -i 1000 -o tile_000001_000001.ppm -c 0 .75,-0.75 -w 1 .5 -s 500 ,500 Stitch the small images together into the complete image (in JPEG format): username@learn $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran goatbrot on each section of the grid. The built-in montage program stitches the files together and writes out the final image in JPEG format. View the Image! \u00b6 Run the commands above so that you have the Mandelbrot image. When you create the image, you might wonder how you can view it. If you're comfortable with scp or another method, you can copy it back to your computer to view it. Otherwise you can view it in your web browser in three easy steps: Make your web directory (you only need to do this once): username@learn $ cd ~ username@learn $ mkdir public_html username@learn $ chmod 0711 . username@learn $ chmod 0755 public_html Copy the image into your web directory (the below command assumes you're back in the directory where you created mandel.jpg): username@learn $ cp mandel.jpg ~/public_html/ Access http://learn.chtc.wisc.edu/~<USERNAME>/mandel.jpg in your web browser (change <USERNAME> to your username on learn.chtc.wisc.edu , keeping the ~ ).","title":"Exercise 1.2"},{"location":"materials/workflows/part1-ex2-mandelbrot/#workflows-exercise-12-a-brief-detour-through-the-mandelbrot-set","text":"Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures! We have a small program that draws pictures of the Mandelbrot set. You can read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal. We have a simple program that can draw the Mandelbrot set. It's called goatbrot . Before beginning, ensure that you are connected to learn.chtc.wisc.edu . Create a directory for this exercise and cd into it.","title":"Workflows Exercise 1.2: A Brief Detour Through the Mandelbrot Set"},{"location":"materials/workflows/part1-ex2-mandelbrot/#running-goatbrot-from-the-command-line","text":"You can generate the Mandelbrot set as a quick test with two simple commands. Generate a PPM image of the Mandelbrot set: username@learn $ goatbrot -i 1000 -o tile_000000_000000.ppm -c 0 ,0 -w 3 -s 1000 ,1000 The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Convert the image to the JPEG format (using a built-in program called convert ): username@learn $ convert tile_000000_000000.ppm mandel.jpg","title":"Running goatbrot From the Command Line"},{"location":"materials/workflows/part1-ex2-mandelbrot/#dividing-the-work-into-smaller-pieces","text":"The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations (an HTC approach!) then stitched them together? Once we do that, we can run each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times: username@learn $ goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1 .5 -s 500 ,500 username@learn $ goatbrot -i 1000 -o tile_000000_000001.ppm -c 0 .75,0.75 -w 1 .5 -s 500 ,500 username@learn $ goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1 .5 -s 500 ,500 username@learn $ goatbrot -i 1000 -o tile_000001_000001.ppm -c 0 .75,-0.75 -w 1 .5 -s 500 ,500 Stitch the small images together into the complete image (in JPEG format): username@learn $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran goatbrot on each section of the grid. The built-in montage program stitches the files together and writes out the final image in JPEG format.","title":"Dividing the Work into Smaller Pieces"},{"location":"materials/workflows/part1-ex2-mandelbrot/#view-the-image","text":"Run the commands above so that you have the Mandelbrot image. When you create the image, you might wonder how you can view it. If you're comfortable with scp or another method, you can copy it back to your computer to view it. Otherwise you can view it in your web browser in three easy steps: Make your web directory (you only need to do this once): username@learn $ cd ~ username@learn $ mkdir public_html username@learn $ chmod 0711 . username@learn $ chmod 0755 public_html Copy the image into your web directory (the below command assumes you're back in the directory where you created mandel.jpg): username@learn $ cp mandel.jpg ~/public_html/ Access http://learn.chtc.wisc.edu/~<USERNAME>/mandel.jpg in your web browser (change <USERNAME> to your username on learn.chtc.wisc.edu , keeping the ~ ).","title":"View the Image!"},{"location":"materials/workflows/part1-ex3-complex-dag/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Workflows Exercise 1.3: A More Complex DAG \u00b6 The objective of this exercise is to run a real set of jobs with DAGMan. Make Your Job Submission Files \u00b6 We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so each job will take longer to run. You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files. goatbrot1.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot2.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot3.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot4.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue montage.sub \u00b6 You should notice that the transfer_input_files statement refers to the files created by the other jobs. executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Make your DAG \u00b6 In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job? Running the DAG \u00b6 Submit your DAG: username@learn $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. ----------------------------------------------------------------------- Watch Your DAG \u00b6 Let\u2019s follow the progress of the whole DAG: Use the condor_watch_q command to keep an eye on the running jobs. See more information about this tool here . username@learn $ condor_watch_q If you're quick enough, you may have seen DAGMan running as the lone job, before it submitted additional job nodes: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 - 1 222059.0 [=============================================================================] Total: 1 jobs; 1 running Updated at 2021-07-28 13:52:57 DAGMan has submitted the goatbrot jobs, but they haven't started running yet BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 4 1 - 5 222059.0 ... 222063.0 [===============--------------------------------------------------------------] Total: 5 jobs; 4 idle, 1 running Updated at 2021-07-28 13:53:53 They're running BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 5 - 5 222059.0 ... 222063.0 [=============================================================================] Total: 5 jobs; 5 running Updated at 2021-07-28 13:54:33 They finished, but DAGMan hasn't noticed yet. It only checks periodically: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 4 - 5 222059.0 ... 222063.0 [##############################################################===============] Total: 5 jobs; 4 completed, 1 running Updated at 2021-07-28 13:55:13 Eventually, you'll see the montage job submitted, then running, then leave the queue, and then DAGMan will leave the queue. Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. username@learn $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? As you did earlier, copy the resulting mandel-from-dag.jpg to your public_html directory, then access it from your web browser. Does the image look correct? Clean up your results by removing all of the goatbrot.dag.* files if you like. Be careful to not delete the goatbrot.dag file. Bonus Challenge \u00b6 Re-run your DAG. When jobs are running, try condor_q -nobatch -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"Exercise 1.3"},{"location":"materials/workflows/part1-ex3-complex-dag/#workflows-exercise-13-a-more-complex-dag","text":"The objective of this exercise is to run a real set of jobs with DAGMan.","title":"Workflows Exercise 1.3: A More Complex DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#make-your-job-submission-files","text":"We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so each job will take longer to run. You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files.","title":"Make Your Job Submission Files"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot1sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot1.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot2sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot2.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot3sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot3.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot4sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot4.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#montagesub","text":"You should notice that the transfer_input_files statement refers to the files created by the other jobs. executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"montage.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#make-your-dag","text":"In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job?","title":"Make your DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#running-the-dag","text":"Submit your DAG: username@learn $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. -----------------------------------------------------------------------","title":"Running the DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#watch-your-dag","text":"Let\u2019s follow the progress of the whole DAG: Use the condor_watch_q command to keep an eye on the running jobs. See more information about this tool here . username@learn $ condor_watch_q If you're quick enough, you may have seen DAGMan running as the lone job, before it submitted additional job nodes: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 - 1 222059.0 [=============================================================================] Total: 1 jobs; 1 running Updated at 2021-07-28 13:52:57 DAGMan has submitted the goatbrot jobs, but they haven't started running yet BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 4 1 - 5 222059.0 ... 222063.0 [===============--------------------------------------------------------------] Total: 5 jobs; 4 idle, 1 running Updated at 2021-07-28 13:53:53 They're running BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 5 - 5 222059.0 ... 222063.0 [=============================================================================] Total: 5 jobs; 5 running Updated at 2021-07-28 13:54:33 They finished, but DAGMan hasn't noticed yet. It only checks periodically: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 4 - 5 222059.0 ... 222063.0 [##############################################################===============] Total: 5 jobs; 4 completed, 1 running Updated at 2021-07-28 13:55:13 Eventually, you'll see the montage job submitted, then running, then leave the queue, and then DAGMan will leave the queue. Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. username@learn $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? As you did earlier, copy the resulting mandel-from-dag.jpg to your public_html directory, then access it from your web browser. Does the image look correct? Clean up your results by removing all of the goatbrot.dag.* files if you like. Be careful to not delete the goatbrot.dag file.","title":"Watch Your DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#bonus-challenge","text":"Re-run your DAG. When jobs are running, try condor_q -nobatch -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"Bonus Challenge"},{"location":"materials/workflows/part1-ex4-failed-dag/","text":"Workflows Exercise 1.4: Handling a DAG That Fails \u00b6 The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. Background \u00b6 DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed. Breaking Things \u00b6 Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this, with the -h at the beginning of the highlighted line: executable = /usr/bin/montage arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Submit the DAG again: username@learn $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local Below is where DAGMan realizes that the montage node failed: 06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because its exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the rescue DAG? Look at the rescue DAG file. It's called a partial DAG because it indicates what part of the DAG has already been completed. username@learn $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6 /22/2012 23 :08:42 UTC # Rescue DAG version: 2 .0.1 ( partial ) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG. If you didn't fix the problem, DAGMan would generate another rescue DAG. username@learn $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- username@learn $ tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... 06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished. 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete. 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished. 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done. 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0 Success! Now go ahead and clean up. Bonus Challenge \u00b6 If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the HTCondor manual to see how to describe your post script.","title":"Exercise 1.4"},{"location":"materials/workflows/part1-ex4-failed-dag/#workflows-exercise-14-handling-a-dag-that-fails","text":"The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures.","title":"Workflows Exercise 1.4: Handling a DAG That Fails"},{"location":"materials/workflows/part1-ex4-failed-dag/#background","text":"DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed.","title":"Background"},{"location":"materials/workflows/part1-ex4-failed-dag/#breaking-things","text":"Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this, with the -h at the beginning of the highlighted line: executable = /usr/bin/montage arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Submit the DAG again: username@learn $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local Below is where DAGMan realizes that the montage node failed: 06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because its exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the rescue DAG? Look at the rescue DAG file. It's called a partial DAG because it indicates what part of the DAG has already been completed. username@learn $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6 /22/2012 23 :08:42 UTC # Rescue DAG version: 2 .0.1 ( partial ) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG. If you didn't fix the problem, DAGMan would generate another rescue DAG. username@learn $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- username@learn $ tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... 06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished. 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete. 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished. 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done. 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0 Success! Now go ahead and clean up.","title":"Breaking Things"},{"location":"materials/workflows/part1-ex4-failed-dag/#bonus-challenge","text":"If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the HTCondor manual to see how to describe your post script.","title":"Bonus Challenge"},{"location":"materials/workflows/part1-ex5-challenges/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus Workflows Exercise 1.5: YOUR Jobs and More on Workflows \u00b6 The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. Challenge 1 \u00b6 Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like distributed.net , SETI@home , Einstien@Home or others that you know. We prefer that you do your own work rather than one of these projects, but they are options. Challenge 2 \u00b6 Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat: goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000 goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000 goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625 -s 1000,1000 You can convert ppm files with convert , like so: convert ex1.ppm ex1.jpg Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie. Challenge 3 \u00b6 Try out Pegasus. Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation. Links to more information: Pegasus Website Pegasus Documentation Pegasus on OSG Connect If you have any questions or problems, please feel free to contact the Pegasus team by emailing pegasus-support@isi.edu Note: Be Nice \u00b6 Please be polite. Computers in our HTCondor pool will run multiple jobs at a time, and these computers are shared with your fellow students. If you have jobs that will use significant computational power or memory, limit your jobs to be kind to your neighbors, unless you run your jobs during off-hours.","title":"Bonus Exercises 1.5"},{"location":"materials/workflows/part1-ex5-challenges/#bonus-workflows-exercise-15-your-jobs-and-more-on-workflows","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Bonus Workflows Exercise 1.5: YOUR Jobs and More on Workflows"},{"location":"materials/workflows/part1-ex5-challenges/#challenge-1","text":"Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like distributed.net , SETI@home , Einstien@Home or others that you know. We prefer that you do your own work rather than one of these projects, but they are options.","title":"Challenge 1"},{"location":"materials/workflows/part1-ex5-challenges/#challenge-2","text":"Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat: goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000 goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000 goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625 -s 1000,1000 You can convert ppm files with convert , like so: convert ex1.ppm ex1.jpg Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie.","title":"Challenge 2"},{"location":"materials/workflows/part1-ex5-challenges/#challenge-3","text":"Try out Pegasus. Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation. Links to more information: Pegasus Website Pegasus Documentation Pegasus on OSG Connect If you have any questions or problems, please feel free to contact the Pegasus team by emailing pegasus-support@isi.edu","title":"Challenge 3"},{"location":"materials/workflows/part1-ex5-challenges/#note-be-nice","text":"Please be polite. Computers in our HTCondor pool will run multiple jobs at a time, and these computers are shared with your fellow students. If you have jobs that will use significant computational power or memory, limit your jobs to be kind to your neighbors, unless you run your jobs during off-hours.","title":"Note: Be Nice"}]}